{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNSy0B7hRxqMksnvqd/H0+6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/deep-learning-keras/blob/main/04_Advanced_Topics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Regularization techniques: Dropout, L1/L2 regularization\n"
      ],
      "metadata": {
        "id": "IimGMCNtwoE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's understand what these regularization techniques are:\n",
        "\n",
        "1. **Dropout:** It is a technique where randomly selected neurons are ignored or \"dropped-out\" during training. This means that their contribution to the activation of downstream neurons is temporarily removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. Dropout helps prevent overfitting.\n",
        "\n",
        "2. **L1/L2 regularization:** These are other regularization techniques that work by adding a penalty to the loss function. L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients. L2 adds a penalty equal to the square of the magnitude of coefficients. These techniques discourage learning a more complex or flexible model, so as to avoid the risk of overfitting.\n",
        "\n",
        "To apply these techniques using the Keras library with the Pima Indians Diabetes dataset, we first need to load the data:\n"
      ],
      "metadata": {
        "id": "0watZmIlwwEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "dataframe = pd.read_csv(url, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n"
      ],
      "metadata": {
        "id": "pwcKqccqw0_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to split the dataset into a training set and a testing set:\n"
      ],
      "metadata": {
        "id": "XeTzmufEw27z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "urqvw0fYw5me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's apply these regularization techniques in Keras:\n"
      ],
      "metadata": {
        "id": "boUPO3itw7XZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.regularizers import l1, l2\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(8, activation='relu', kernel_regularizer=l1(0.01)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "MPtaUyTAw-WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we fit the model to our data:\n"
      ],
      "metadata": {
        "id": "4Tz1sSq7xAiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train, epochs=150, batch_size=10, verbose=0)\n"
      ],
      "metadata": {
        "id": "NpMlIvEnxCVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `kernel_regularizer` argument in the `Dense` function allows us to add L1 or L2 regularization, while the `Dropout` function allows us to add dropout regularization.\n",
        "\n",
        "After training, we can evaluate the model performance on the testing dataset:\n"
      ],
      "metadata": {
        "id": "a3dpTDGXxFNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, accuracy = model.evaluate(X_test, Y_test)\n",
        "print('Accuracy: %.2f' % (accuracy*100))\n"
      ],
      "metadata": {
        "id": "LNjfsOY4xHrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that it's important to carefully tune the regularization parameters. If they're too high, they might cause underfitting. If they're too low, they might not effectively prevent overfitting. Also, the above example is a simple demonstration and there might be room for further improvement, such as data normalization or standardization, more sophisticated architecture, etc.\n"
      ],
      "metadata": {
        "id": "9vjJ2V89xKSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Advanced optimizer algorithms: RMSprop, Adam, etc.\n"
      ],
      "metadata": {
        "id": "4ggfrriTxNIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example of how to use the RMSprop and Adam optimizers in Keras using the Pima Indians Diabetes dataset. Note that the dataset needs to be preprocessed before feeding it to a neural network, such as by normalizing the features and splitting the data into training and testing sets.\n",
        "\n",
        "Then, you can load the Pima Indians Diabetes dataset:\n"
      ],
      "metadata": {
        "id": "uphlNGxrxSHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "data = pd.read_csv(url, names=names)\n",
        "\n",
        "X = data.iloc[:, 0:8]\n",
        "Y = data.iloc[:, 8]\n"
      ],
      "metadata": {
        "id": "Ze4HtKLsxZnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then create a function to build a simple Keras model:\n"
      ],
      "metadata": {
        "id": "m66aIUm5xbWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "def create_model(optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "xZ6ds8mCxeGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's fit the model using the RMSprop optimizer:\n"
      ],
      "metadata": {
        "id": "fmrMFkc5xgVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(optimizer='RMSprop')\n",
        "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\n",
        "_, accuracy = model.evaluate(X, Y, verbose=0)\n",
        "print('Accuracy: %.2f' % (accuracy*100))\n"
      ],
      "metadata": {
        "id": "tNc-JtWSxixR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, fit the model using the Adam optimizer:\n"
      ],
      "metadata": {
        "id": "8ROdV5Blxkv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(optimizer='adam')\n",
        "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\n",
        "_, accuracy = model.evaluate(X, Y, verbose=0)\n",
        "print('Accuracy: %.2f' % (accuracy*100))\n"
      ],
      "metadata": {
        "id": "_C4Utuypxm5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, you'll want to tune other parameters of the model as well, such as the learning rate and the number of layers in the model. You'll also want to use a separate validation set or cross-validation to get a better estimate of the model's performance on unseen data.\n",
        "\n",
        "Please note that both RMSprop and Adam are advanced optimization algorithms that adapt the learning rate during training. Adam combines the advantages of two other extensions of stochastic gradient descent: AdaGrad and RMSProp. RMSProp (Root Mean Square Propagation) is also a method in which the learning rate is adapted for each of the parameters. The idea is to divide the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight.\n"
      ],
      "metadata": {
        "id": "ob8IjNDLxpjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A quiz on Advanced topics on Deep Learning with Keras"
      ],
      "metadata": {
        "id": "dws5fIQBy80i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Question 1: What is the main purpose of using regularization techniques in deep learning models?\n",
        "<br>a) To speed up model training\n",
        "<br>b) To reduce the number of layers in the model\n",
        "<br>c) To prevent overfitting\n",
        "<br>d) To increase the model's complexity\n",
        "\n",
        "Question 2: Which of the following regularization techniques randomly drops a fraction of neurons during training?\n",
        "<br>a) L1 regularization\n",
        "<br>b) L2 regularization\n",
        "<br>c) Dropout\n",
        "<br>d) RMSprop\n",
        "\n",
        "Question 3: What is the benefit of using L1 regularization in a neural network?\n",
        "<br>a) It adds noise to the model, improving generalization.\n",
        "<br>b) It penalizes large weights, encouraging sparsity in the model.\n",
        "<br>c) It increases the learning rate, leading to faster convergence.\n",
        "<br>d) It helps the model escape local minima during training.\n",
        "\n",
        "Question 4: Which of the following statements about dropout is true?\n",
        "<br>a) Dropout only affects the output layer of the neural network.\n",
        "<br>b) Dropout randomly removes input features during training.\n",
        "<br>c) Dropout is applied during inference to improve prediction accuracy.\n",
        "<br>d) Dropout helps prevent overfitting by reducing co-adaptation of neurons.\n",
        "\n",
        "Question 5: What is the advantage of using advanced optimizer algorithms like RMSprop and Adam?\n",
        "<br>a) They guarantee a globally optimal solution.\n",
        "<br>b) They require fewer hyperparameter tunings compared to SGD.\n",
        "<br>c) They are computationally less expensive than other optimizers.\n",
        "<br>d) They can adapt the learning rate for each parameter, leading to faster convergence.\n",
        "\n",
        "---\n",
        "Answers\n",
        "<br>1: c) To prevent overfitting\n",
        "<br>2: c) Dropout\n",
        "<br>3: b) It penalizes large weights, encouraging sparsity in the model.\n",
        "<br>4: d) Dropout helps prevent overfitting by reducing co-adaptation of neurons.\n",
        "<br>5: d) They can adapt the learning rate for each parameter, leading to faster convergence.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Xy6YYV6fzE4P"
      }
    }
  ]
}