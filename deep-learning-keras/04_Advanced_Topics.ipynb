{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPF1um2DUm0NFUCqv8bWOH6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/data-science-programming/blob/main/deep-learning-keras/04_Advanced_Topics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularization techniques: Dropout, L1/L2 regularization\n"
      ],
      "metadata": {
        "id": "IimGMCNtwoE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's understand what these regularization techniques are:\n",
        "\n",
        "1. **Dropout:** It is a technique where randomly selected neurons are ignored or \"dropped-out\" during training. This means that their contribution to the activation of downstream neurons is temporarily removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. Dropout helps prevent overfitting.\n",
        "\n",
        "2. **L1/L2 regularization:** These are other regularization techniques that work by adding a penalty to the loss function. L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients. L2 adds a penalty equal to the square of the magnitude of coefficients. These techniques discourage learning a more complex or flexible model, so as to avoid the risk of overfitting.\n",
        "\n",
        "To apply these techniques using the Keras library with the Pima Indians Diabetes dataset, we first need to load the data:\n"
      ],
      "metadata": {
        "id": "0watZmIlwwEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "dataframe = pd.read_csv(url, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n"
      ],
      "metadata": {
        "id": "pwcKqccqw0_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to split the dataset into a training set and a testing set:\n"
      ],
      "metadata": {
        "id": "XeTzmufEw27z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "urqvw0fYw5me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's apply these regularization techniques in Keras:\n"
      ],
      "metadata": {
        "id": "boUPO3itw7XZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.regularizers import l1, l2\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(8, activation='relu', kernel_regularizer=l1(0.01)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "MPtaUyTAw-WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we fit the model to our data:\n"
      ],
      "metadata": {
        "id": "4Tz1sSq7xAiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train, epochs=150, batch_size=10, verbose=0)\n"
      ],
      "metadata": {
        "id": "NpMlIvEnxCVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `kernel_regularizer` argument in the `Dense` function allows us to add L1 or L2 regularization, while the `Dropout` function allows us to add dropout regularization.\n",
        "\n",
        "After training, we can evaluate the model performance on the testing dataset:\n"
      ],
      "metadata": {
        "id": "a3dpTDGXxFNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, accuracy = model.evaluate(X_test, Y_test)\n",
        "print('Accuracy: %.2f' % (accuracy*100))\n"
      ],
      "metadata": {
        "id": "LNjfsOY4xHrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that it's important to carefully tune the regularization parameters. If they're too high, they might cause underfitting. If they're too low, they might not effectively prevent overfitting. Also, the above example is a simple demonstration and there might be room for further improvement, such as data normalization or standardization, more sophisticated architecture, etc.\n"
      ],
      "metadata": {
        "id": "9vjJ2V89xKSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise"
      ],
      "metadata": {
        "id": "QFml0al58YYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Load the necessary libraries and the Pima Indian diabetes dataset.\n",
        "2. Preprocess the dataset by splitting it into features (X) and the target variable (y). Scale the features to have zero mean and unit variance.\n",
        "3. Create a function to build a neural network model with the specified regularization technique. The function should take the regularization parameter as input.\n",
        "4. Build three neural network models using the following regularization techniques:\n",
        "   a) Dropout: Add a dropout layer with a specified dropout rate.\n",
        "   b) L1 Regularization: Add L1 regularization to the model with a specified regularization parameter.\n",
        "   c) L2 Regularization: Add L2 regularization to the model with a specified regularization parameter.\n",
        "5. Compile and train the models using appropriate loss function, optimizer, and evaluation metric.\n",
        "6. Evaluate each model on the test set and print the accuracy score for comparison.\n"
      ],
      "metadata": {
        "id": "hAZCZpt_8aOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample Solution"
      ],
      "metadata": {
        "id": "4v3NXQrv89Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.regularizers import l1, l2\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Load the Pima Indian diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "columns = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "df = pd.read_csv(url, names=columns)\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df.drop('class', axis=1)\n",
        "y = df['class']\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to build a neural network model with regularization\n",
        "def build_model(regularization=None, reg_param=None, dropout_rate=None):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=8, activation='relu', kernel_regularizer=regularization(reg_param) if regularization else None))\n",
        "    if dropout_rate:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(32, activation='relu', kernel_regularizer=regularization(reg_param) if regularization else None))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Build and train models with different regularization techniques\n",
        "dropout_model = build_model(dropout_rate=0.2)\n",
        "dropout_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "dropout_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "\n",
        "l1_model = build_model(regularization=l1, reg_param=0.01)\n",
        "l1_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "l1_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "\n",
        "l2_model = build_model(regularization=l2, reg_param=0.01)\n",
        "l2_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "l2_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "\n",
        "# Evaluate models on the test set\n",
        "dropout_score = dropout_model.evaluate(X_test, y_test)[1]\n",
        "l1_score = l1_model.evaluate(X_test, y_test)[1]\n",
        "l2_score = l2_model.evaluate(X_test, y_test)[1]\n",
        "\n",
        "# Print the accuracy scores\n",
        "print(\"Dropout Model Accuracy: {:.2f}%\".format(dropout_score * 100))\n",
        "print(\"L1 Regularization Model Accuracy: {:.2f}%\".format(l1_score * 100))\n",
        "print(\"L2 Regularization Model Accuracy: {:.2f}%\".format(l2_score * 100))\n"
      ],
      "metadata": {
        "id": "0NYi_nks8b7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced optimizer algorithms: RMSprop, Adam, etc.\n"
      ],
      "metadata": {
        "id": "4ggfrriTxNIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example of how to use the RMSprop and Adam optimizers in Keras using the Pima Indians Diabetes dataset. Note that the dataset needs to be preprocessed before feeding it to a neural network, such as by normalizing the features and splitting the data into training and testing sets.\n",
        "\n",
        "Then, you can load the Pima Indians Diabetes dataset:\n"
      ],
      "metadata": {
        "id": "uphlNGxrxSHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "data = pd.read_csv(url, names=names)\n",
        "\n",
        "X = data.iloc[:, 0:8]\n",
        "Y = data.iloc[:, 8]\n"
      ],
      "metadata": {
        "id": "Ze4HtKLsxZnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then create a function to build a simple Keras model:\n"
      ],
      "metadata": {
        "id": "m66aIUm5xbWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "def create_model(optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "xZ6ds8mCxeGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's fit the model using the RMSprop optimizer:\n"
      ],
      "metadata": {
        "id": "fmrMFkc5xgVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(optimizer='RMSprop')\n",
        "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\n",
        "_, accuracy = model.evaluate(X, Y, verbose=0)\n",
        "print('Accuracy: %.2f' % (accuracy*100))\n"
      ],
      "metadata": {
        "id": "tNc-JtWSxixR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, fit the model using the Adam optimizer:\n"
      ],
      "metadata": {
        "id": "8ROdV5Blxkv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(optimizer='adam')\n",
        "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\n",
        "_, accuracy = model.evaluate(X, Y, verbose=0)\n",
        "print('Accuracy: %.2f' % (accuracy*100))\n"
      ],
      "metadata": {
        "id": "_C4Utuypxm5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, you'll want to tune other parameters of the model as well, such as the learning rate and the number of layers in the model. You'll also want to use a separate validation set or cross-validation to get a better estimate of the model's performance on unseen data.\n",
        "\n",
        "Please note that both RMSprop and Adam are advanced optimization algorithms that adapt the learning rate during training. Adam combines the advantages of two other extensions of stochastic gradient descent: AdaGrad and RMSProp. RMSProp (Root Mean Square Propagation) is also a method in which the learning rate is adapted for each of the parameters. The idea is to divide the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight.\n"
      ],
      "metadata": {
        "id": "ob8IjNDLxpjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise"
      ],
      "metadata": {
        "id": "Zfglrd9D-BvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Load the Pima Indian dataset from the given URL.\n",
        "2. Preprocess the data by splitting it into features (X) and labels (y), and then splitting them into training and testing sets.\n",
        "3. Build a neural network using Keras with the following architecture:\n",
        "   - Input layer with the appropriate input shape.\n",
        "   - Two hidden layers with 64 neurons each and ReLU activation function.\n",
        "   - Output layer with a single neuron and a sigmoid activation function.\n",
        "4. Compile the model with RMSprop optimizer and binary cross-entropy loss function. Train the model on the training data for 100 epochs.\n",
        "5. Evaluate the model on the testing data and record the accuracy.\n",
        "6. Repeat steps 4 and 5 but this time using the Adam optimizer.\n",
        "7. Compare the performance of RMSprop and Adam optimizers and analyze the results.\n"
      ],
      "metadata": {
        "id": "548AmwVb-Ds0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample Solution"
      ],
      "metadata": {
        "id": "XftpPEYh-os0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "\n",
        "# Step 1: Load the Pima Indian dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Step 2: Preprocess the data\n",
        "X = dataset.drop('Outcome', axis=1).values\n",
        "y = dataset['Outcome'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Build the neural network\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=8, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Step 4 and 5: Train and evaluate using RMSprop optimizer\n",
        "model_rmsprop = create_model()\n",
        "model_rmsprop.compile(optimizer=RMSprop(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_rmsprop.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
        "loss_rmsprop, accuracy_rmsprop = model_rmsprop.evaluate(X_test, y_test)\n",
        "\n",
        "print(\"RMSprop Optimizer:\")\n",
        "print(\"Test Accuracy:\", accuracy_rmsprop)\n",
        "\n",
        "# Step 6 and 7: Train and evaluate using Adam optimizer\n",
        "model_adam = create_model()\n",
        "model_adam.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_adam.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
        "loss_adam, accuracy_adam = model_adam.evaluate(X_test, y_test)\n",
        "\n",
        "print(\"\\nAdam Optimizer:\")\n",
        "print(\"Test Accuracy:\", accuracy_adam)\n",
        "\n",
        "# Step 7: Compare the performance of RMSprop and Adam optimizers\n",
        "if accuracy_rmsprop > accuracy_adam:\n",
        "    print(\"\\nRMSprop performed better than Adam.\")\n",
        "elif accuracy_rmsprop < accuracy_adam:\n",
        "    print(\"\\nAdam performed better than RMSprop.\")\n",
        "else:\n",
        "    print(\"\\nRMSprop and Adam had the same performance.\")\n"
      ],
      "metadata": {
        "id": "podHeoC9-GqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A quiz on Advanced topics on Deep Learning with Keras"
      ],
      "metadata": {
        "id": "dws5fIQBy80i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Question 1: What is the main purpose of using regularization techniques in deep learning models?\n",
        "<br>a) To speed up model training\n",
        "<br>b) To reduce the number of layers in the model\n",
        "<br>c) To prevent overfitting\n",
        "<br>d) To increase the model's complexity\n",
        "\n",
        "Question 2: Which of the following regularization techniques randomly drops a fraction of neurons during training?\n",
        "<br>a) L1 regularization\n",
        "<br>b) L2 regularization\n",
        "<br>c) Dropout\n",
        "<br>d) RMSprop\n",
        "\n",
        "Question 3: What is the benefit of using L1 regularization in a neural network?\n",
        "<br>a) It adds noise to the model, improving generalization.\n",
        "<br>b) It penalizes large weights, encouraging sparsity in the model.\n",
        "<br>c) It increases the learning rate, leading to faster convergence.\n",
        "<br>d) It helps the model escape local minima during training.\n",
        "\n",
        "Question 4: Which of the following statements about dropout is true?\n",
        "<br>a) Dropout only affects the output layer of the neural network.\n",
        "<br>b) Dropout randomly removes input features during training.\n",
        "<br>c) Dropout is applied during inference to improve prediction accuracy.\n",
        "<br>d) Dropout helps prevent overfitting by reducing co-adaptation of neurons.\n",
        "\n",
        "Question 5: What is the advantage of using advanced optimizer algorithms like RMSprop and Adam?\n",
        "<br>a) They guarantee a globally optimal solution.\n",
        "<br>b) They require fewer hyperparameter tunings compared to SGD.\n",
        "<br>c) They are computationally less expensive than other optimizers.\n",
        "<br>d) They can adapt the learning rate for each parameter, leading to faster convergence.\n",
        "\n",
        "---\n",
        "Answers\n",
        "<br>1: c) To prevent overfitting\n",
        "<br>2: c) Dropout\n",
        "<br>3: b) It penalizes large weights, encouraging sparsity in the model.\n",
        "<br>4: d) Dropout helps prevent overfitting by reducing co-adaptation of neurons.\n",
        "<br>5: d) They can adapt the learning rate for each parameter, leading to faster convergence.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Xy6YYV6fzE4P"
      }
    }
  ]
}