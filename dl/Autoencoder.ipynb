{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP1t3u3FU9K+DnPaXHEMxra",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/data-science-programming/blob/main/dl/Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background"
      ],
      "metadata": {
        "id": "c_UqcJ1O2ark"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An autoencoder is a type of neural network used for unsupervised learning, particularly in the field of representation learning and dimensionality reduction. It aims to learn a compressed representation (encoding) of the input data and then reconstruct the original data (decoding) from the compressed representation. The architecture is designed in a way that the output of the network closely resembles the input, forcing the autoencoder to learn meaningful features that capture the essential information of the data.\n",
        "\n",
        "Here's how an autoencoder works:\n",
        "\n",
        "1. Encoding: The input data is fed into the encoder, which consists of one or more hidden layers. These layers progressively reduce the dimensionality of the input and create a compressed representation (latent space) of the data.\n",
        "\n",
        "2. Decoding: The compressed representation from the encoder is then passed through the decoder, which consists of one or more hidden layers. The decoder's task is to reconstruct the original data from the compressed representation.\n",
        "\n",
        "The autoencoder is trained using a loss function that quantifies the difference between the input and the reconstructed output, such as Mean Squared Error (MSE) or Binary Cross Entropy (BCE). The network learns to minimize this loss function during training, which drives it to learn meaningful representations.\n",
        "\n",
        "**Pros of Autoencoder**:\n",
        "1. Unsupervised learning: Autoencoders do not require labeled data for training, making them useful when labeled data is scarce or expensive to obtain.\n",
        "2. Dimensionality reduction: Autoencoders can compress high-dimensional data into a lower-dimensional latent space, making them useful for feature extraction and representation learning.\n",
        "3. Feature learning: Autoencoders can learn to extract meaningful features from raw data, which can be valuable for subsequent tasks like classification or clustering.\n",
        "4. Data denoising: Autoencoders can be used for denoising data by training them to reconstruct clean data from noisy input.\n",
        "\n",
        "**Cons of Autoencoder**:\n",
        "1. Overfitting: Autoencoders can suffer from overfitting, especially if the encoder and decoder are too complex or if the training data is limited.\n",
        "2. Lack of interpretability: While autoencoders learn useful representations, these representations might not always be easily interpretable by humans.\n",
        "3. Hyperparameter tuning: Designing and tuning the architecture of an autoencoder can be challenging, requiring experimentation and domain knowledge.\n",
        "\n",
        "**When to use Autoencoders**:\n",
        "1. Dimensionality reduction: When dealing with high-dimensional data and you want to reduce the number of features while preserving the essential information.\n",
        "2. Preprocessing for other tasks: Autoencoders can be used to pretrain deep learning models, initializing them with meaningful representations before fine-tuning on specific tasks.\n",
        "3. Anomaly detection: Autoencoders can be used for anomaly detection by reconstructing normal data accurately, making it useful for spotting abnormal instances.\n",
        "4. Data denoising: If you have noisy data and want to clean it up, autoencoders can be trained to reconstruct clean data from the noisy input.\n",
        "\n",
        "In summary, autoencoders are powerful neural network architectures for unsupervised learning, especially when dealing with dimensionality reduction, feature learning, or data denoising tasks. However, their performance relies heavily on proper architecture design, tuning, and the availability of sufficient training data."
      ],
      "metadata": {
        "id": "jNaosO-q8jE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "14RB0FDcCE0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# Load the MNIST dataset (28x28 grayscale images of handwritten digits)\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "# Normalize and flatten the data\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "\n",
        "# Define the architecture of the autoencoder\n",
        "input_size = 784  # 28x28 = 784 (MNIST images)\n",
        "encoding_dim = 32  # Size of the bottleneck layer (latent space)\n",
        "\n",
        "input_img = Input(shape=(input_size,))\n",
        "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
        "decoded = Dense(input_size, activation='sigmoid')(encoded)\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "\n",
        "# Compile the autoencoder model\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the autoencoder\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))\n",
        "\n",
        "# Get the encoder part of the trained autoencoder\n",
        "encoder = Model(input_img, encoded)\n",
        "\n",
        "# Get encoded representations of the test data\n",
        "encoded_imgs = encoder.predict(x_test)\n",
        "\n",
        "# Get decoded images by passing encoded representations through the decoder\n",
        "decoded_imgs = autoencoder.predict(x_test)\n",
        "\n",
        "# Visualize some of the original and reconstructed images\n",
        "n = 10  # Number of digits to display\n",
        "plt.figure(figsize=(20, 4))\n",
        "\n",
        "for i in range(n):\n",
        "    # Original Images\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
        "    plt.title(\"Original\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Reconstructed Images\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28), cmap='gray')\n",
        "    plt.title(\"Reconstructed\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4wM1tTQaPqk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "yWMYpEq7Qgb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Import the required libraries:\n",
        "   - `numpy` for numerical operations.\n",
        "   - `matplotlib.pyplot` for visualization.\n",
        "   - `Input` and `Dense` from `keras.layers` for defining the layers of the neural network.\n",
        "   - `Model` from `keras.models` to create the autoencoder model.\n",
        "   - `mnist` from `keras.datasets` to load the MNIST dataset.\n",
        "\n",
        "2. Load the MNIST dataset:\n",
        "   - `mnist.load_data()`: This function loads the MNIST dataset and splits it into training and testing sets. The training set `x_train` contains images used to train the autoencoder, and the testing set `x_test` contains images for evaluation.\n",
        "\n",
        "3. Normalize and flatten the data:\n",
        "   - The pixel values of the images are normalized to a range of [0, 1] by dividing by 255.0 (since the original pixel values are in the range [0, 255]).\n",
        "   - The 2D images are flattened to 1D arrays of length 784 (28x28 = 784) to be used as input to the autoencoder.\n",
        "\n",
        "4. Define the architecture of the autoencoder:\n",
        "   - `input_size = 784`: The size of the input layer, which corresponds to the flattened image size.\n",
        "   - `encoding_dim = 32`: The size of the bottleneck layer (latent space), which will be the compressed representation of the input images.\n",
        "   - `input_img = Input(shape=(input_size,))`: Create an input layer of size `input_size`.\n",
        "   - `encoded = Dense(encoding_dim, activation='relu')(input_img)`: Add a dense (fully connected) layer with `encoding_dim` neurons and ReLU activation to the input layer. This is the encoder part of the autoencoder.\n",
        "   - `decoded = Dense(input_size, activation='sigmoid')(encoded)`: Add another dense layer with `input_size` neurons and a sigmoid activation function. This is the decoder part of the autoencoder.\n",
        "\n",
        "5. Compile the autoencoder model:\n",
        "   - `autoencoder.compile(optimizer='adam', loss='binary_crossentropy')`: Compile the autoencoder using the Adam optimizer and binary cross-entropy loss, which is commonly used for image reconstruction tasks.\n",
        "\n",
        "6. Train the autoencoder:\n",
        "   - `autoencoder.fit(...)`: Train the autoencoder using the training data `x_train` as both input and target data. The autoencoder tries to reconstruct the input images from the compressed representation in the bottleneck layer.\n",
        "\n",
        "7. Get the encoder part of the trained autoencoder:\n",
        "   - `encoder = Model(input_img, encoded)`: Create a new model that takes the input and outputs the encoded (compressed) representation. This will be used to obtain the compressed representations of the test data.\n",
        "\n",
        "8. Get encoded representations of the test data:\n",
        "   - `encoded_imgs = encoder.predict(x_test)`: Use the encoder model to obtain the compressed representations of the test images.\n",
        "\n",
        "9. Get decoded images by passing encoded representations through the decoder:\n",
        "   - `decoded_imgs = autoencoder.predict(x_test)`: Pass the encoded representations through the entire autoencoder model to obtain the reconstructed images.\n",
        "\n",
        "10. Visualize some of the original and reconstructed images:\n",
        "   - The code uses `matplotlib.pyplot` to display 10 original images and their corresponding reconstructed images side by side.\n",
        "\n",
        "11. Display the plot:\n",
        "   - `plt.tight_layout()` and `plt.show()`: Display the plot with original and reconstructed images.\n",
        "\n",
        "The autoencoder learns to compress the input images into a lower-dimensional latent space and then reconstruct them. The visualization shows how well the autoencoder can reconstruct the digits after compression and decompression."
      ],
      "metadata": {
        "id": "-T8wWeC0VF0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "KWKikTYjYGfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of using an autoencoder in a healthcare setting is medical image denoising.\n",
        "\n",
        "**Medical Image Denoising with Autoencoders:**\n",
        "\n",
        "Medical images, such as X-rays, CT scans, and MRI scans, are essential for diagnosing and monitoring various diseases. However, these images can be affected by noise during acquisition or transmission, which may reduce their quality and make it challenging for doctors to interpret the images accurately.\n",
        "\n",
        "Autoencoders are a type of unsupervised neural network that can be used for dimensionality reduction and feature learning. They are well-suited for tasks like image denoising.\n",
        "\n",
        "**Step-by-step process:**\n",
        "\n",
        "1. **Data Collection:** Gather a dataset of medical images (e.g., X-ray images) with noise introduced during acquisition or transmission.\n",
        "\n",
        "2. **Data Preprocessing:** Normalize the pixel values of the images and apply any necessary preprocessing steps.\n",
        "\n",
        "3. **Creating Noisy Data:** Add artificial noise to the images to simulate real-world noise scenarios.\n",
        "\n",
        "4. **Building the Autoencoder:**\n",
        "   - Design an autoencoder architecture with an encoder and decoder.\n",
        "   - The encoder reduces the dimensionality of the input image and captures the essential features.\n",
        "   - The decoder reconstructs the denoised image from the encoded representation.\n",
        "\n",
        "5. **Training the Autoencoder:**\n",
        "   - Train the autoencoder using the noisy images as both input and target output.\n",
        "   - The autoencoder learns to map noisy images to denoised images through the training process.\n",
        "\n",
        "6. **Evaluation:**\n",
        "   - Evaluate the performance of the autoencoder on a separate test dataset by comparing the denoised images to the corresponding ground truth (original clean images).\n",
        "   - Common evaluation metrics include mean squared error (MSE) and peak signal-to-noise ratio (PSNR).\n",
        "\n",
        "7. **Inference:**\n",
        "   - Deploy the trained autoencoder to denoise new medical images in real-time.\n",
        "   - When a new noisy medical image is input to the autoencoder, it will generate a denoised version of the image.\n",
        "\n",
        "**Benefits:**\n",
        "\n",
        "- Autoencoders can effectively denoise medical images, improving the accuracy of diagnosis and enhancing the quality of images used for treatment planning.\n",
        "- The denoised images can aid radiologists and medical professionals in making more reliable and accurate decisions.\n",
        "- With better-quality images, medical practitioners can detect abnormalities or diseases at an earlier stage, leading to improved patient outcomes.\n",
        "\n",
        "**Note:**\n",
        "Keep in mind that autoencoders in healthcare applications are not limited to image denoising. They can also be used for various other tasks, such as anomaly detection in medical images, feature extraction, and representation learning for downstream tasks like image segmentation or classification. Additionally, ensuring the privacy and security of sensitive medical data is crucial when deploying such models in real-world healthcare settings."
      ],
      "metadata": {
        "id": "BtDGxk0C7GHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "SWnRYukHZ43f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is an Autoencoder?\n",
        "   - An autoencoder is an unsupervised learning neural network architecture that aims to learn efficient representations of input data by encoding it into a lower-dimensional space and then decoding it back to the original data format.\n",
        "\n",
        "2. How does an Autoencoder work?\n",
        "   - An autoencoder consists of two main components: an encoder and a decoder. The encoder takes the input data and maps it to a latent space representation with lower dimensions. The decoder then takes this compressed representation and reconstructs the original data from it.\n",
        "\n",
        "3. What are the key applications of Autoencoders?\n",
        "   - Autoencoders find applications in various fields, including image compression, denoising, anomaly detection, feature extraction, and dimensionality reduction.\n",
        "\n",
        "4. How do Autoencoders perform data compression?\n",
        "   - By learning a compressed representation of the data in the latent space, autoencoders effectively perform data compression, as the dimensionality of the latent space is typically much smaller than the original input data.\n",
        "\n",
        "5. How can Autoencoders be used for denoising data?\n",
        "   - Autoencoders can be trained to map noisy data to its clean version. By adding noise to the input data during training and providing the clean data as the target output, the autoencoder learns to remove the noise from the input during the reconstruction process.\n",
        "\n",
        "6. How do Autoencoders detect anomalies in data?\n",
        "   - Anomalies can be detected using autoencoders by training the model on normal data and then evaluating how well the autoencoder reconstructs new unseen data. Data points that cannot be effectively reconstructed may indicate anomalies.\n",
        "\n",
        "7. What are Variational Autoencoders (VAEs)?\n",
        "   - VAEs are a type of autoencoder that is capable of generating new data samples from the learned latent space. Unlike traditional autoencoders, VAEs learn a probability distribution in the latent space, making them useful for generating new and meaningful data.\n",
        "\n",
        "8. Can Autoencoders be used for image generation?\n",
        "   - While traditional autoencoders focus on reconstructing the input data, Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) are more commonly used for image generation tasks.\n",
        "\n",
        "9. How do Autoencoders compare to PCA for dimensionality reduction?\n",
        "   - Autoencoders have the advantage over Principal Component Analysis (PCA) in that they can learn non-linear representations of data, whereas PCA is limited to linear transformations.\n",
        "\n",
        "10. Do Autoencoders suffer from overfitting?\n",
        "   - Like other neural network models, autoencoders can suffer from overfitting, especially when the latent space dimension is too large or the model capacity is too high. Regularization techniques and proper tuning can help mitigate this issue."
      ],
      "metadata": {
        "id": "Fy7p75YuUy1f"
      }
    }
  ]
}