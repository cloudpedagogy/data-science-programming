{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9QgzDlUwPUVzuoHlGG0Z1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/data-science-programming/blob/main/ml/k_Nearest_Neighbors_(k_NN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background"
      ],
      "metadata": {
        "id": "M-3bUGfb-nUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "k-Nearest Neighbors (k-NN) is a simple and popular machine learning algorithm used for classification and regression tasks. It operates on the principle of finding the \"k\" closest data points (neighbors) in the training dataset to a given query point and then makes predictions based on the majority class (for classification) or the average of the k neighbors' values (for regression). Here's how it works:\n",
        "\n",
        "1. Given a query point, the algorithm calculates the distance (e.g., Euclidean distance) between the query point and all the points in the training dataset.\n",
        "2. It selects the k closest data points (neighbors) based on the calculated distances.\n",
        "3. For classification tasks, k-NN makes predictions based on the majority class among the k neighbors.\n",
        "4. For regression tasks, k-NN predicts the output value as the average of the target values of the k neighbors.\n",
        "\n",
        "**Pros of k-Nearest Neighbors**:\n",
        "\n",
        "1. Simplicity: k-NN is easy to understand, implement, and interpret. It serves as a good baseline model for comparison with more complex algorithms.\n",
        "2. No Training Phase: Unlike many other algorithms, k-NN does not require an explicit training phase. It memorizes the training data and uses it for predictions directly.\n",
        "3. Versatile: k-NN can be used for both classification and regression tasks.\n",
        "4. Non-parametric: It is a non-parametric algorithm, which means it makes no assumptions about the underlying data distribution.\n",
        "\n",
        "**Cons of k-Nearest Neighbors**:\n",
        "\n",
        "1. Computational Cost: The algorithm's main drawback is its computational cost during the prediction phase, especially for large datasets. It needs to calculate distances for each query point against all training points.\n",
        "2. Memory Usage: Since k-NN memorizes the entire training dataset, it can consume a significant amount of memory, especially for large datasets.\n",
        "3. Sensitivity to Data Representation: The performance of k-NN can be sensitive to the data representation and feature scaling. Preprocessing the data may be necessary to achieve better results.\n",
        "4. Choosing Optimal k: Selecting an appropriate value for k is crucial. A small k may lead to noise sensitivity, while a large k may cause oversmoothing.\n",
        "\n",
        "**When to use k-Nearest Neighbors**:\n",
        "\n",
        "1. Small to Medium-sized Datasets: k-NN can be effective on datasets with a relatively small number of samples, as the computational cost is less of a concern.\n",
        "2. Non-linear Relationships: k-NN can capture non-linear relationships between features and target variables effectively.\n",
        "3. Simple Baseline: It serves as a quick and easy-to-implement baseline algorithm to compare against more sophisticated models. It can help you assess whether more complex methods are necessary for your problem.\n",
        "4. Data Exploration: k-NN can be used in the initial data exploration phase to gain insights into the data distribution and the potential separability of classes.\n",
        "\n",
        "However, for large datasets or problems where efficiency is a major concern, you may consider other algorithms like Support Vector Machines (SVM), Random Forests, or Neural Networks, as they can handle larger datasets more efficiently while offering competitive performance. Additionally, if the data has high dimensionality or is sparse, other dimensionality reduction techniques or specialized algorithms might be more appropriate."
      ],
      "metadata": {
        "id": "9-dX040S8teP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "Wxw02pH7Bslq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (optional but recommended)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create and train the k-NN model\n",
        "k_neighbors = 3  # You can change this value to choose the 'k' value\n",
        "knn_model = KNeighborsClassifier(n_neighbors=k_neighbors)\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = knn_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "HvFMNObyzS5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "_IvsVYRYG56V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Import necessary libraries:**\n",
        "   - `numpy` and `pandas` for data manipulation and processing.\n",
        "   - `load_iris` from `sklearn.datasets` to load the Iris dataset.\n",
        "   - `train_test_split` from `sklearn.model_selection` to split the data into training and testing sets.\n",
        "   - `StandardScaler` from `sklearn.preprocessing` to standardize the features (optional but recommended for some algorithms).\n",
        "   - `KNeighborsClassifier` from `sklearn.neighbors` to create and train the k-NN (k-Nearest Neighbors) model.\n",
        "   - `accuracy_score` from `sklearn.metrics` to evaluate the model's accuracy.\n",
        "\n",
        "2. **Load the Iris dataset:**\n",
        "   - The Iris dataset is one of the built-in datasets available in scikit-learn (`sklearn`). It is a popular dataset for classification tasks.\n",
        "   - The features (`X`) represent the measurements of sepal length, sepal width, petal length, and petal width for Iris flowers.\n",
        "   - The target (`y`) contains the corresponding class labels for each sample, representing three species of Iris flowers.\n",
        "\n",
        "3. **Split the data into training and testing sets:**\n",
        "   - `train_test_split` is used to split the dataset into training and testing subsets.\n",
        "   - `test_size=0.2` specifies that 20% of the data will be used for testing, while 80% will be used for training.\n",
        "   - `random_state=42` is a random seed to ensure reproducibility.\n",
        "\n",
        "4. **Standardize the features (optional but recommended):**\n",
        "   - `StandardScaler` is used to standardize the features, which scales the data to have zero mean and unit variance.\n",
        "   - Standardization is optional but can be beneficial, especially for distance-based algorithms like k-NN, as it helps ensure all features have the same weight.\n",
        "\n",
        "5. **Create and train the k-NN model:**\n",
        "   - `k_neighbors = 3`: We define the number of neighbors (`k`) to consider in the k-NN algorithm. You can change this value to experiment with different values of `k`.\n",
        "   - `KNeighborsClassifier(n_neighbors=k_neighbors)`: We create a k-NN classifier object with the specified number of neighbors.\n",
        "   - `knn_model.fit(X_train, y_train)`: We train the k-NN model using the training data (`X_train` and `y_train`).\n",
        "\n",
        "6. **Make predictions on the test set:**\n",
        "   - `knn_model.predict(X_test)`: We use the trained k-NN model to make predictions on the test set (`X_test`).\n",
        "   - The predicted class labels are stored in `y_pred`.\n",
        "\n",
        "7. **Evaluate the model:**\n",
        "   - `accuracy_score(y_test, y_pred)`: We calculate the accuracy of the model by comparing the predicted labels (`y_pred`) with the true labels from the test set (`y_test`).\n",
        "   - The accuracy is the proportion of correctly classified samples in the test set.\n",
        "   - The calculated accuracy is printed to the console using `print(f\"Accuracy: {accuracy:.2f}\")`.\n",
        "\n",
        "The code demonstrates a simple implementation of the k-NN algorithm for the Iris dataset, with the option to change the number of neighbors (`k`) to observe its effect on the model's accuracy."
      ],
      "metadata": {
        "id": "g6RDZDfrug1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "TWFIL-KbSK5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A real-world example of using the k-Nearest Neighbors (k-NN) model in a healthcare setting is for medical image classification. Medical imaging plays a crucial role in diagnosing various diseases and conditions, and the k-NN algorithm can be employed to classify medical images based on their features.\n",
        "\n",
        "Let's consider an example of using the k-NN model for classifying X-ray images of the chest into two categories: \"Normal\" and \"Abnormal\" (indicating potential lung diseases).\n",
        "\n",
        "**Data Collection and Preprocessing:**\n",
        "1. Gather a dataset of chest X-ray images, where each image is labeled as \"Normal\" or \"Abnormal.\"\n",
        "2. Preprocess the images by resizing them to a consistent resolution and converting them to grayscale to reduce computational complexity.\n",
        "3. Extract relevant features from the images to represent them numerically. For instance, you could use handcrafted features like texture, intensity, or shape descriptors from the images.\n",
        "\n",
        "**Building the k-NN Model:**\n",
        "1. Split the dataset into training and testing sets. The training set will be used to fit the k-NN model, and the testing set will be used to evaluate its performance.\n",
        "2. Train the k-NN model on the training data. During training, the model will store the features and corresponding labels of the training images.\n",
        "3. Define the value of 'k,' which determines the number of nearest neighbors to consider during classification. Choosing an appropriate 'k' is crucial; a small 'k' might result in overfitting, while a large 'k' may lead to underfitting.\n",
        "4. When classifying a new, unlabeled chest X-ray image, the k-NN model will:\n",
        "   - Extract features from the new image.\n",
        "   - Calculate the distance (e.g., Euclidean distance) between the new image's features and the features of all training images.\n",
        "   - Select the 'k' nearest neighbors with the smallest distances.\n",
        "   - Assign the class label to the new image based on the majority class among its 'k' nearest neighbors. For example, if the majority of the neighbors are \"Abnormal,\" the new image will be classified as \"Abnormal.\"\n",
        "\n",
        "**Evaluation and Deployment:**\n",
        "1. Evaluate the k-NN model's performance on the testing dataset using metrics such as accuracy, precision, recall, and F1-score to assess its ability to correctly classify normal and abnormal X-ray images.\n",
        "2. Fine-tune the model by experimenting with different feature extraction techniques or distance metrics to improve its performance.\n",
        "3. Once satisfied with the model's performance, deploy it in a healthcare setting to assist radiologists and clinicians in diagnosing chest X-ray images. The model can be integrated into a medical imaging system to provide preliminary assessments or as a second opinion to support human experts.\n",
        "\n",
        "It's essential to note that while k-NN is a straightforward and interpretable algorithm, its performance heavily depends on the choice of features and distance metric. In some cases, more advanced machine learning or deep learning models may be preferred for image classification tasks in healthcare due to their ability to learn complex patterns and representations from raw data."
      ],
      "metadata": {
        "id": "Uwc_9Zu9Cfaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "FNdjzFTmY83q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is the k-Nearest Neighbors (k-NN) algorithm?\n",
        "   - The k-Nearest Neighbors algorithm is a simple and intuitive machine learning algorithm used for classification and regression tasks. It makes predictions based on the majority class or average of the k-nearest data points to the input instance.\n",
        "\n",
        "2. How does the k-NN algorithm work?\n",
        "   - The k-NN algorithm works by finding the k-nearest data points (neighbors) to a given query point in the feature space. For classification, it assigns the majority class among the k-nearest neighbors to the query point. For regression, it calculates the average (or weighted average) of the target values of the k-nearest neighbors.\n",
        "\n",
        "3. How do you choose the value of k in k-NN?\n",
        "   - Selecting an appropriate value of k is crucial in k-NN. A small value of k may lead to overfitting, where the model captures noise in the data. On the other hand, a large value of k may lead to underfitting, where the model loses the ability to discriminate between different classes. Typically, cross-validation is used to find the optimal value of k for a specific dataset.\n",
        "\n",
        "4. Is k-NN sensitive to feature scaling?\n",
        "   - Yes, k-NN is sensitive to feature scaling. When features have different scales, some features may dominate the distance calculation, leading to biased results. Therefore, it is essential to normalize or scale the features before applying k-NN.\n",
        "\n",
        "5. Does k-NN have a high computational cost?\n",
        "   - Yes, k-NN can be computationally expensive, especially for large datasets. The algorithm needs to calculate the distances between the query point and all data points in the training set. To speed up the process, various data structures like KD-trees or Ball-trees can be used.\n",
        "\n",
        "6. Can k-NN handle multi-class classification?\n",
        "   - Yes, k-NN can handle multi-class classification. For a given query point, the algorithm calculates the distances to the k-nearest neighbors and assigns the majority class among those neighbors as the predicted class for the query point.\n",
        "\n",
        "7. Is k-NN a parametric or non-parametric model?\n",
        "   - k-NN is a non-parametric model, meaning it doesn't make any assumptions about the underlying data distribution. It directly uses the training data to make predictions.\n",
        "\n",
        "8. Does k-NN perform well with high-dimensional data?\n",
        "   - As the number of dimensions increases, the curse of dimensionality becomes a concern for k-NN. In high-dimensional spaces, the data points tend to be far apart, leading to less meaningful neighbor selection. k-NN may not perform well in such cases unless appropriate feature selection or dimensionality reduction techniques are applied.\n",
        "\n",
        "9. Can k-NN be used for imputation in missing data?\n",
        "   - Yes, k-NN can be used for imputing missing data in a dataset. The missing value is replaced with the average (or weighted average) of the k-nearest neighbors' corresponding feature values.\n",
        "\n",
        "10. Are there any variations of the k-NN algorithm?\n",
        "    - Yes, there are several variations of the k-NN algorithm, such as weighted k-NN, distance-weighted k-NN, and kernel density estimation-based k-NN, which apply different weights to the neighbors based on their distances or kernel functions. Additionally, there are instance-based learning techniques like Locally Weighted Learning (LWL) and Case-Based Reasoning (CBR), which are related to k-NN."
      ],
      "metadata": {
        "id": "_ILq7ihE4B_p"
      }
    }
  ]
}