{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBJ6pjN8I+R11VuLdIg5Nf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/data-science-programming/blob/main/ml/k_Means_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background"
      ],
      "metadata": {
        "id": "71fTtWxl-kTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "k-Means Clustering is a popular unsupervised machine learning algorithm used for clustering data into distinct groups based on similarities in the data points. It aims to partition the data into 'k' clusters, where each cluster is represented by its centroid (the mean of the data points within that cluster).\n",
        "\n",
        "Here's how the k-Means Clustering algorithm works:\n",
        "\n",
        "1. Initialization: Select 'k' initial centroids randomly from the data points or use a specific initialization method.\n",
        "2. Assignment: Assign each data point to the nearest centroid, forming 'k' clusters.\n",
        "3. Update: Recalculate the centroids for each cluster by taking the mean of the data points within that cluster.\n",
        "4. Repeat: Repeat steps 2 and 3 until the centroids stabilize or the maximum number of iterations is reached.\n",
        "\n",
        "**Pros of k-Means Clustering**:\n",
        "\n",
        "1. Simplicity: k-Means is relatively easy to understand and implement, making it a good starting point for clustering tasks.\n",
        "2. Efficiency: It is computationally efficient and can handle large datasets effectively.\n",
        "3. Scalability: Works well with a large number of variables or dimensions, making it suitable for high-dimensional data.\n",
        "4. Interpretability: The clusters formed by k-Means are easy to interpret and visualize.\n",
        "5. Works well with circular clusters: k-Means performs well when the clusters have a roughly circular shape.\n",
        "\n",
        "**Cons of k-Means Clustering**:\n",
        "\n",
        "1. Sensitive to initialization: The quality of the clusters depends on the initial choice of centroids, and different initializations can lead to different results.\n",
        "2. Fixed number of clusters: You need to specify the number of clusters 'k' beforehand, which may not always be known in advance or may be subjective.\n",
        "3. Sensitive to outliers: Outliers can significantly impact the centroid calculation and, therefore, the final clustering.\n",
        "4. Assumes spherical clusters: k-Means assumes that the clusters are spherical and of similar size, which might not hold for complex data distributions.\n",
        "\n",
        "**When to use k-Means Clustering**:\n",
        "\n",
        "k-Means Clustering is suitable for scenarios where:\n",
        "\n",
        "1. You have a large dataset and want a computationally efficient clustering algorithm.\n",
        "2. The number of clusters 'k' is known or can be reasonably estimated.\n",
        "3. The clusters are well-separated or roughly circular in shape.\n",
        "4. You need an interpretable and easy-to-understand clustering solution.\n",
        "5. You want to use clustering as a preprocessing step for other algorithms or tasks.\n",
        "\n",
        "Keep in mind that k-Means is just one of many clustering algorithms available, and its performance depends on the nature of your data and the specific clustering task at hand. Always consider exploring other clustering algorithms like hierarchical clustering, DBSCAN, or Gaussian Mixture Models, especially if your data has complex structures or varying cluster densities."
      ],
      "metadata": {
        "id": "gcoR-EF9BD-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "iq0ZVDnqBuZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Generate sample data\n",
        "data, _ = make_blobs(n_samples=300, centers=4, random_state=42, cluster_std=1.0)\n",
        "\n",
        "# Perform k-Means clustering\n",
        "num_clusters = 4\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "clusters = kmeans.fit_predict(data)\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "# Plot the data points and centroids\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(data[:, 0], data[:, 1], c=clusters, cmap='viridis', edgecolors='k')\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200)\n",
        "plt.title('k-Means Clustering')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6OYlnVl6H93b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "4n2plxTUG_Ps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **Import necessary libraries:**\n",
        "   - `numpy`: A library for numerical operations in Python.\n",
        "   - `matplotlib.pyplot`: A library for data visualization using plots in Python.\n",
        "   - `make_blobs`: A function from scikit-learn that generates synthetic data points with clusters.\n",
        "   - `KMeans`: A class from scikit-learn for performing k-Means clustering.\n",
        "\n",
        "2. **Generate sample data:**\n",
        "   - `make_blobs`: Creates a dataset with synthetic data points that are clustered around specified centers.\n",
        "   - `n_samples=300`: Generates 300 data points.\n",
        "   - `centers=4`: Specifies the number of clusters (centers) to create.\n",
        "   - `random_state=42`: Sets the random seed for reproducibility.\n",
        "   - `cluster_std=1.0`: Specifies the standard deviation of the clusters around their centers.\n",
        "\n",
        "3. **Perform k-Means clustering:**\n",
        "   - `num_clusters = 4`: Defines the number of clusters (same as the number of centers).\n",
        "   - `KMeans(n_clusters=num_clusters, random_state=42)`: Initializes the k-Means clustering algorithm with the specified number of clusters and random state for reproducibility.\n",
        "   - `kmeans.fit_predict(data)`: Fits the k-Means model to the data and predicts the cluster labels for each data point. The result is stored in the `clusters` variable.\n",
        "   - `kmeans.cluster_centers_`: Accesses the coordinates of the cluster centroids and stores them in the `centroids` variable.\n",
        "\n",
        "4. **Plot the data points and centroids:**\n",
        "   - `plt.figure(figsize=(8, 6))`: Creates a new plot with the specified figure size (8 inches wide and 6 inches tall).\n",
        "   - `plt.scatter(data[:, 0], data[:, 1], c=clusters, cmap='viridis', edgecolors='k')`: Plots the data points with `data[:, 0]` representing the x-coordinate and `data[:, 1]` representing the y-coordinate. The `c=clusters` parameter assigns a unique color to each cluster based on the cluster labels. The `cmap='viridis'` parameter sets the color map for the clusters, and `edgecolors='k'` adds black edges to the data points.\n",
        "   - `plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200)`: Plots the cluster centroids with `centroids[:, 0]` representing the x-coordinate and `centroids[:, 1]` representing the y-coordinate. The `c='red'` parameter sets the color of the centroids to red, `marker='X'` selects the marker style as 'X', and `s=200` sets the marker size to 200.\n",
        "   - `plt.title('k-Means Clustering')`: Sets the title of the plot to 'k-Means Clustering'.\n",
        "   - `plt.xlabel('Feature 1')`: Sets the label of the x-axis to 'Feature 1'.\n",
        "   - `plt.ylabel('Feature 2')`: Sets the label of the y-axis to 'Feature 2'.\n",
        "   - `plt.show()`: Displays the plot.\n",
        "\n",
        "In summary, this code generates sample data with four clusters, performs k-Means clustering on the data to identify the clusters, and then plots the data points and the cluster centroids using matplotlib. The plot helps visualize the effectiveness of k-Means clustering in grouping the data points into distinct clusters based on their features."
      ],
      "metadata": {
        "id": "ZxPLMaRUuUzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "nhx9PkyPSO_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One real-world example of applying k-Means clustering in a healthcare setting is patient segmentation based on health-related data. In this scenario, k-Means clustering can be used to group patients with similar characteristics and health conditions, allowing healthcare providers to tailor treatments and interventions more effectively.\n",
        "\n",
        "Let's consider a specific example to illustrate how k-Means clustering can be applied in healthcare:\n",
        "\n",
        "**Example: Patient Segmentation for Diabetes Management**\n",
        "\n",
        "Objective: To segment diabetic patients based on their health parameters and identify distinct subgroups with similar health profiles.\n",
        "\n",
        "Data Collection:\n",
        "- A dataset is collected from diabetic patients, including features such as age, gender, BMI (Body Mass Index), HbA1c levels (glycated hemoglobin), fasting blood glucose levels, postprandial blood glucose levels, and average number of daily insulin units.\n",
        "\n",
        "Data Preprocessing:\n",
        "- The data is preprocessed to handle missing values, normalize numerical features, and encode categorical features if present.\n",
        "\n",
        "Applying k-Means Clustering:\n",
        "- The k-Means clustering algorithm is applied to the preprocessed data.\n",
        "- The number of clusters, 'k,' is determined based on domain knowledge or using techniques like the elbow method or silhouette score to find the optimal number of clusters.\n",
        "\n",
        "Interpreting the Clusters:\n",
        "- Once the clustering is complete, each patient is assigned to one of the k clusters.\n",
        "- The clusters are analyzed to understand the characteristics of each group of patients.\n",
        "- Healthcare providers can interpret the clusters to identify meaningful patterns and correlations between health parameters and patient outcomes.\n",
        "\n",
        "Benefits and Applications:\n",
        "- Personalized Treatment Plans: Clustering helps in tailoring treatment plans based on the specific needs of each patient group. For example, patients in one cluster might respond better to certain medications, while patients in another cluster might benefit from lifestyle interventions like diet and exercise.\n",
        "\n",
        "- Early Detection of Risks: Identifying high-risk clusters can enable early detection of potential complications, allowing healthcare providers to intervene proactively and prevent adverse events.\n",
        "\n",
        "- Resource Allocation: Understanding patient subgroups can help allocate healthcare resources more efficiently. For instance, a cluster with higher insulin requirements may need specialized diabetes care and support.\n",
        "\n",
        "- Clinical Research: Researchers can use the insights gained from patient segmentation to conduct targeted studies and identify factors influencing disease progression and treatment responses.\n",
        "\n",
        "Overall, k-Means clustering in the healthcare setting allows for more personalized and effective patient care by identifying distinct patient subgroups and understanding the relationships between health parameters and outcomes. It enables healthcare providers to make informed decisions and optimize healthcare interventions for different patient populations."
      ],
      "metadata": {
        "id": "BMN3w6tm_jzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "buWM1VrSZAZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is k-Means Clustering?\n",
        "   - k-Means Clustering is an unsupervised machine learning algorithm used to partition data into k clusters based on their similarity.\n",
        "\n",
        "2. How does k-Means Clustering work?\n",
        "   - The k-Means algorithm iteratively assigns data points to the nearest cluster centroid and then updates the centroids based on the mean of the points in each cluster.\n",
        "\n",
        "3. What is the objective function in k-Means Clustering?\n",
        "   - The objective of k-Means is to minimize the sum of squared distances between data points and their assigned cluster centroids.\n",
        "\n",
        "4. How is the value of 'k' determined in k-Means Clustering?\n",
        "   - Selecting the right value of 'k' is essential. Common methods for determining 'k' include the elbow method, silhouette score, and gap statistics.\n",
        "\n",
        "5. What are the limitations of k-Means Clustering?\n",
        "   - k-Means is sensitive to the initial placement of centroids and can converge to local optima. It also assumes equal-sized and spherical clusters.\n",
        "\n",
        "6. Can k-Means handle non-spherical or overlapping clusters?\n",
        "   - No, k-Means assumes spherical clusters and struggles with non-linear and overlapping data distributions.\n",
        "\n",
        "7. How can you handle the sensitivity to initialization in k-Means?\n",
        "   - Multiple runs with different initializations and choosing the best result based on the lowest objective function value can help mitigate sensitivity to initialization.\n",
        "\n",
        "8. What are some popular distance metrics used in k-Means Clustering?\n",
        "   - Euclidean distance is the most commonly used metric, but other distance metrics like Manhattan distance and cosine similarity can also be employed.\n",
        "\n",
        "9. Is it necessary for k-Means to converge in every run?\n",
        "   - No, k-Means may not converge to the global optimum in every run, but it generally converges to a local optimum that minimizes the objective function.\n",
        "\n",
        "10. Can k-Means handle high-dimensional data effectively?\n",
        "    - k-Means can struggle with high-dimensional data due to the \"curse of dimensionality,\" where distances between points lose meaning in high-dimensional spaces.\n",
        "\n",
        "11. How can you evaluate the performance of k-Means Clustering?\n",
        "    - Internal evaluation metrics like silhouette score or external validation metrics like adjusted Rand index can be used to assess the quality of the clustering.\n",
        "\n",
        "12. Can k-Means be used for data with categorical features?\n",
        "    - No, k-Means is designed for numerical data and may not work well with categorical features. For such cases, other clustering algorithms like k-Modes or k-Prototypes are more appropriate.\n",
        "\n",
        "13. How is k-Means used in image compression?\n",
        "    - In image compression, k-Means can be used to cluster similar colors together and represent them with fewer colors, reducing the image size while preserving visual quality.\n",
        "\n",
        "14. Can k-Means be sensitive to outliers in the data?\n",
        "    - Yes, k-Means is sensitive to outliers as they can significantly impact the cluster centroids, leading to suboptimal clustering results.\n",
        "\n",
        "15. Is it possible for k-Means to converge to different solutions for different runs on the same data?\n",
        "    - Yes, k-Means may converge to different solutions for different runs due to the sensitivity to initialization, especially when the data contains overlapping clusters or when 'k' is not well-defined."
      ],
      "metadata": {
        "id": "nHt0uW653zVf"
      }
    }
  ]
}