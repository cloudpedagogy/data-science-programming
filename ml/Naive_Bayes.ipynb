{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAAovEiZ1hAEYKgLg7rYMu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/data-science-programming/blob/main/ml/Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background"
      ],
      "metadata": {
        "id": "xHIxFK9k_w4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes is a popular classification algorithm based on Bayes' theorem, which is used for supervised learning tasks, particularly in the field of machine learning and natural language processing. Despite its simplicity, Naive Bayes can be surprisingly effective in many real-world scenarios.\n",
        "\n",
        "**How Naive Bayes works:**\n",
        "Naive Bayes is a probabilistic model that makes predictions by calculating the probability of a given input belonging to a particular class. It assumes that all features are independent of each other, which is where the term \"naive\" comes from. This is a strong assumption and may not hold true in all cases, but in practice, Naive Bayes can still perform well, especially with discrete features or when there are many irrelevant features that cancel each other out.\n",
        "\n",
        "The algorithm is based on Bayes' theorem:\n",
        "\n",
        "```\n",
        "P(y|x) = (P(x|y) * P(y)) / P(x)\n",
        "```\n",
        "\n",
        "where:\n",
        "- `P(y|x)` is the posterior probability of class `y` given input `x`.\n",
        "- `P(x|y)` is the likelihood of input `x` given class `y`.\n",
        "- `P(y)` is the prior probability of class `y`.\n",
        "- `P(x)` is the evidence probability of input `x`.\n",
        "\n",
        "**Pros of Naive Bayes:**\n",
        "1. **Simplicity and Speed:** Naive Bayes is easy to implement and computationally efficient. It is particularly useful for large datasets.\n",
        "2. **Scalability:** Due to its simplicity and low computational overhead, Naive Bayes can handle high-dimensional data well.\n",
        "3. **Good with small datasets:** It requires fewer training data compared to more complex algorithms, making it suitable for cases with limited labeled data.\n",
        "4. **Real-time predictions:** Because of its speed, Naive Bayes is often used for real-time applications.\n",
        "5. **Interpretability:** The results are easily interpretable, providing insights into the importance of different features.\n",
        "\n",
        "**Cons of Naive Bayes:**\n",
        "1. **Naive Assumption:** The independence assumption may not hold in many real-world scenarios, which can lead to suboptimal predictions.\n",
        "2. **Limited Expressiveness:** Due to its simplicity, Naive Bayes may not capture complex relationships in the data as well as more sophisticated models.\n",
        "3. **Zero Probability Issue:** If a certain feature value in the test data wasn't present in the training data for a particular class, Naive Bayes assigns a zero probability, resulting in inaccurate predictions.\n",
        "4. **Sensitive to Feature Correlations:** Since it assumes independence among features, correlated features can negatively impact performance.\n",
        "5. **Continuous Features Handling:** Naive Bayes works well with discrete features but may require some data preprocessing for continuous features.\n",
        "\n",
        "**When to use Naive Bayes:**\n",
        "Naive Bayes is most suitable when:\n",
        "1. The independence assumption is reasonably valid for the given problem.\n",
        "2. You have a large dataset, and efficiency is a priority.\n",
        "3. The problem at hand is relatively simple, and complex interactions between features are not essential for accurate predictions.\n",
        "4. You have limited labeled data, and you want a quick and interpretable solution.\n",
        "5. You need to make real-time predictions.\n",
        "\n",
        "It's important to note that while Naive Bayes can be a good starting point, especially for text classification tasks, it's always a good idea to compare its performance with other more complex algorithms to ensure you are getting the best possible results for your specific problem."
      ],
      "metadata": {
        "id": "uWZ7ADCl9Sdr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "ieaa5DGFa4Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Iris dataset from scikit-learn\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Naive Bayes classifier\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_rep)\n"
      ],
      "metadata": {
        "id": "KrosLHeXz3Vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "EVzh1Jkx0Qsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Import the required libraries:\n",
        "   - `numpy` (aliased as `np`) for numerical operations.\n",
        "   - `pandas` (aliased as `pd`) for data manipulation and analysis.\n",
        "   - `train_test_split` from `sklearn.model_selection` to split the dataset into training and testing sets.\n",
        "   - `GaussianNB` from `sklearn.naive_bayes` to create and train the Naive Bayes classifier.\n",
        "   - `accuracy_score`, `classification_report`, and `confusion_matrix` from `sklearn.metrics` to evaluate the classifier's performance.\n",
        "\n",
        "2. Load the Iris dataset from scikit-learn:\n",
        "   - The Iris dataset is one of the standard datasets available in scikit-learn, representing iris flower species with four features (sepal length, sepal width, petal length, petal width) and one target variable (species class).\n",
        "\n",
        "3. Split the dataset into training and testing sets:\n",
        "   - `train_test_split(X, y, test_size=0.2, random_state=42)`: This function randomly splits the data into training and testing sets. It takes the features (X) and target labels (y) as input and returns four sets: `X_train`, `X_test`, `y_train`, and `y_test`. The test set size is set to 20% of the entire dataset, and `random_state=42` ensures reproducibility by fixing the random seed.\n",
        "\n",
        "4. Create and train the Naive Bayes classifier:\n",
        "   - `GaussianNB()`: Creates an instance of the Gaussian Naive Bayes classifier, which is appropriate for continuous features like the ones in the Iris dataset.\n",
        "   - `nb_classifier.fit(X_train, y_train)`: Trains the Naive Bayes classifier using the training data (`X_train` and `y_train`).\n",
        "\n",
        "5. Make predictions on the test set:\n",
        "   - `y_pred = nb_classifier.predict(X_test)`: The trained classifier is used to predict the target labels (`y_pred`) for the test set (`X_test`).\n",
        "\n",
        "6. Evaluate the classifier's performance:\n",
        "   - `accuracy_score(y_test, y_pred)`: Calculates the accuracy of the classifier by comparing the predicted labels (`y_pred`) with the true labels from the test set (`y_test`).\n",
        "   - `confusion_matrix(y_test, y_pred)`: Computes the confusion matrix, which is a table showing the true positive, true negative, false positive, and false negative counts for each class.\n",
        "   - `classification_report(y_test, y_pred)`: Generates a detailed report showing precision, recall, F1-score, and support for each class.\n",
        "\n",
        "7. Print the results:\n",
        "   - The script prints the accuracy, confusion matrix, and classification report to evaluate the Naive Bayes classifier's performance.\n",
        "\n",
        "That's it! The code loads the Iris dataset, splits it into training and testing sets, trains a Naive Bayes classifier, makes predictions on the test set, and evaluates the classifier's accuracy using various metrics."
      ],
      "metadata": {
        "id": "MwhHFeO5xUKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "1E8Ymy8pz03X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Example: Diagnosing a Respiratory Infection**\n",
        "\n",
        "Imagine a healthcare provider wants to diagnose whether a patient has a respiratory infection (e.g., common cold, flu, or pneumonia) based on the patient's symptoms. The symptoms to consider are:\n",
        "\n",
        "1. Cough (Yes or No)\n",
        "2. Fever (Yes or No)\n",
        "3. Runny Nose (Yes or No)\n",
        "4. Shortness of Breath (Yes or No)\n",
        "\n",
        "Additionally, the healthcare provider has historical data of patients who were diagnosed with respiratory infections and the corresponding symptoms they exhibited.\n",
        "\n",
        "**Building the Naive Bayes Model**\n",
        "\n",
        "1. **Data Collection:** The healthcare provider collects data from a group of patients who were diagnosed with respiratory infections and notes their symptoms (cough, fever, runny nose, and shortness of breath). This data forms the positive class.\n",
        "\n",
        "2. **Data Collection for Negative Class:** The healthcare provider also collects data from a group of patients who were not diagnosed with respiratory infections and notes their symptoms. This data forms the negative class.\n",
        "\n",
        "3. **Data Preprocessing:** The data is preprocessed to encode categorical features (Yes or No) as binary values (1 or 0).\n",
        "\n",
        "4. **Training the Naive Bayes Model:** The healthcare provider uses the preprocessed data to train a Naive Bayes classifier. The Naive Bayes model calculates the probability of each symptom given the presence or absence of a respiratory infection (positive or negative class). It assumes independence among the symptoms, which is a common assumption in Naive Bayes.\n",
        "\n",
        "5. **Making Predictions:** Once the model is trained, the healthcare provider can use it to make predictions for new patients. When a new patient visits with specific symptoms, the model calculates the probability of each class (respiratory infection or no respiratory infection) based on the symptoms exhibited by the patient. The model predicts the class with the higher probability as the diagnosis for the patient.\n",
        "\n",
        "**Real-World Application**\n",
        "\n",
        "Suppose a new patient visits the healthcare provider with symptoms: cough (Yes), fever (Yes), runny nose (Yes), and shortness of breath (No). The Naive Bayes model calculates the probabilities for both classes (respiratory infection and no respiratory infection) based on the patient's symptoms. If the probability of respiratory infection is higher, the model predicts that the patient has a respiratory infection and prescribes appropriate treatment or further diagnostic tests.\n",
        "\n",
        "It's important to note that while Naive Bayes can be useful in certain scenarios, it may not capture complex relationships between symptoms, and its performance heavily relies on the quality and representativeness of the training data. In real-world healthcare settings, more sophisticated models and extensive data may be required for accurate and reliable medical diagnosis. Naive Bayes serves as an elementary example, but actual medical decision-making typically involves a more comprehensive approach."
      ],
      "metadata": {
        "id": "PEDPG6A0FWQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "XT3ifKTh3I7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is the Naive Bayes model, and how does it work?\n",
        "   - The Naive Bayes model is a probabilistic machine learning algorithm used for classification tasks. It's based on Bayes' theorem and makes a \"naive\" assumption that features are conditionally independent given the class label.\n",
        "\n",
        "2. Why is it called \"Naive\" Bayes?\n",
        "   - It is called \"Naive\" because it makes a simplifying assumption that the features are independent, which is often not the case in real-world scenarios. Despite this assumption, Naive Bayes often performs surprisingly well in practice and is computationally efficient.\n",
        "\n",
        "3. In which applications is Naive Bayes commonly used?\n",
        "   - Naive Bayes is commonly used in text classification tasks like spam detection, sentiment analysis, document categorization, and language identification. It's also used in various other domains like medical diagnosis, recommendation systems, and fraud detection.\n",
        "\n",
        "4. How does Naive Bayes handle continuous and categorical features?\n",
        "   - Naive Bayes can handle both continuous and categorical features. For continuous features, it typically assumes a Gaussian (normal) distribution. For categorical features, it estimates probabilities using frequency counts.\n",
        "\n",
        "5. Is Naive Bayes suitable for high-dimensional data?\n",
        "   - Yes, Naive Bayes is particularly well-suited for high-dimensional data, like text data, where the number of features can be very large. Its performance often remains competitive even with a large number of features.\n",
        "\n",
        "6. What are the advantages of using Naive Bayes?\n",
        "   - Naive Bayes is simple, easy to implement, and computationally efficient. It works well with high-dimensional data and requires relatively little training data. It can also handle missing values gracefully.\n",
        "\n",
        "7. What are the limitations of the Naive Bayes model?\n",
        "   - The main limitation of Naive Bayes is its assumption of feature independence, which may not hold true in some cases. Due to this assumption, it may not perform well on datasets with strong interdependencies between features. However, it still performs reasonably well in practice.\n",
        "\n",
        "8. Can Naive Bayes handle multi-class classification?\n",
        "   - Yes, Naive Bayes can handle multi-class classification tasks. It assigns the class label with the highest probability among all classes.\n",
        "\n",
        "9. Is Naive Bayes sensitive to irrelevant features?\n",
        "   - Naive Bayes is generally considered to be somewhat robust to irrelevant features because it relies on probabilities rather than weights assigned to features. However, extremely irrelevant features may still have some impact on the model's performance.\n",
        "\n",
        "10. How does Laplace smoothing (additive smoothing) help in Naive Bayes?\n",
        "    - Laplace smoothing is used to address the problem of zero probabilities for unseen features. It adds a small value (usually 1) to all feature counts during probability estimation to ensure non-zero probabilities for all features, even those not seen in the training data. This prevents the model from assigning zero probabilities to unseen data during testing."
      ],
      "metadata": {
        "id": "JAVaha5r6o5w"
      }
    }
  ]
}