{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjxrC45BH4q2CiHBfoC65D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/data-science-programming/blob/main/ml/Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background"
      ],
      "metadata": {
        "id": "SemPjC4r_Bcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression is a popular statistical method used for predicting numerical values based on the relationship between one or more independent variables (also called features or predictors) and a dependent variable (also known as the target or outcome). It assumes a linear relationship between the independent variables and the dependent variable. The goal of linear regression is to find the best-fitting line that minimizes the error between the predicted values and the actual values in the dataset.\n",
        "\n",
        "The equation of a simple linear regression with one independent variable can be represented as:\n",
        "\n",
        "y = mx + b\n",
        "\n",
        "Where:\n",
        "- y is the dependent variable (target)\n",
        "- x is the independent variable (feature)\n",
        "- m is the slope of the line (representing the relationship between x and y)\n",
        "- b is the y-intercept (representing the value of y when x is 0)\n",
        "\n",
        "For multiple linear regression (with more than one independent variable), the equation becomes:\n",
        "\n",
        "y = b0 + b1*x1 + b2*x2 + ... + bn*xn\n",
        "\n",
        "Where:\n",
        "- b0 is the y-intercept\n",
        "- b1, b2, ..., bn are the coefficients for the independent variables x1, x2, ..., xn\n",
        "\n",
        "**Pros of Linear Regression**:\n",
        "1. Simplicity: Linear regression is straightforward and easy to understand, making it a good starting point for predictive modeling.\n",
        "2. Interpretability: The coefficients of the model provide insights into the relationship between the independent variables and the target variable.\n",
        "3. Fast computation: Training a linear regression model is computationally efficient, making it suitable for large datasets.\n",
        "4. Widely used: Linear regression is a fundamental technique that is widely applied in various fields, such as economics, finance, and social sciences.\n",
        "\n",
        "**Cons of Linear Regression**:\n",
        "1. Linearity assumption: Linear regression assumes a linear relationship between the independent variables and the target, which may not always be true in real-world scenarios.\n",
        "2. Sensitivity to outliers: Linear regression is sensitive to outliers in the data, and outliers can have a significant impact on the model's performance.\n",
        "3. Limited complexity: The model's simplicity can be a limitation when dealing with complex relationships in the data, where more advanced models might be required.\n",
        "4. Multicollinearity: If the independent variables are highly correlated, it can lead to multicollinearity issues, affecting the interpretability of the coefficients.\n",
        "\n",
        "**When to use Linear Regression**:\n",
        "Linear regression is appropriate when:\n",
        "1. You want to predict a numerical value (continuous target variable) based on one or more input features.\n",
        "2. There is a linear relationship between the input features and the target variable.\n",
        "3. You need a simple and interpretable model to gain insights into the relationships between variables.\n",
        "\n",
        "It is essential to evaluate your data and consider the assumptions of linear regression before applying it. If the relationship is nonlinear, you may need to consider using other regression models or machine learning algorithms. Also, make sure to preprocess your data, handle outliers, and check for multicollinearity before fitting the model."
      ],
      "metadata": {
        "id": "zqfCEnDAwiw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example"
      ],
      "metadata": {
        "id": "eVK45xq4Ancr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Generate some sample data (replace this with your own dataset)\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2 * X + 3 + np.random.randn(100, 1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print model performance metrics\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"R-squared:\", r2_score(y_test, y_pred))\n",
        "\n",
        "# Visualize the model's predictions\n",
        "plt.scatter(X_test, y_test, color='blue', label='Actual')\n",
        "plt.plot(X_test, y_pred, color='red', linewidth=2, label='Predicted')\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pT_ZoF4bvZ8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code breakdown"
      ],
      "metadata": {
        "id": "b94GmqURGpBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Import required libraries:\n",
        "   - `numpy` (as `np`): A library for numerical computing in Python.\n",
        "   - `pandas` (as `pd`): A library for data manipulation and analysis.\n",
        "   - `matplotlib.pyplot` (as `plt`): A library for data visualization.\n",
        "   - `sklearn.linear_model.LinearRegression`: A class representing the Linear Regression model from scikit-learn (a popular machine learning library).\n",
        "   - `sklearn.model_selection.train_test_split`: A function to split the data into training and testing sets.\n",
        "   - `sklearn.metrics.mean_squared_error`: A function to calculate the mean squared error, a common regression evaluation metric.\n",
        "   - `sklearn.metrics.r2_score`: A function to calculate the R-squared (coefficient of determination), another regression evaluation metric.\n",
        "\n",
        "2. Generate some sample data:\n",
        "   - `np.random.seed(42)`: Sets the random seed for reproducibility.\n",
        "   - `X`: A 1-dimensional NumPy array of shape (100, 1) containing random values between 0 and 10.\n",
        "   - `y`: A 1-dimensional NumPy array of shape (100, 1) representing the target variable `y`. It is created by applying the equation `y = 2 * X + 3 + random_noise`, where `random_noise` is drawn from a normal distribution (with mean 0 and standard deviation 1).\n",
        "\n",
        "3. Split the data into training and testing sets:\n",
        "   - `train_test_split(X, y, test_size=0.2, random_state=42)`: Splits the data into training and testing sets. 80% of the data is used for training, and 20% is used for testing. The `random_state` parameter ensures reproducibility of the split.\n",
        "\n",
        "4. Create and fit the Linear Regression model:\n",
        "   - `model = LinearRegression()`: Creates an instance of the LinearRegression class, representing a linear regression model.\n",
        "   - `model.fit(X_train, y_train)`: Fits the linear regression model to the training data (`X_train` and `y_train`).\n",
        "\n",
        "5. Make predictions on the test set:\n",
        "   - `y_pred = model.predict(X_test)`: Uses the trained model to make predictions on the test data (`X_test`) and stores the predicted values in `y_pred`.\n",
        "\n",
        "6. Print model performance metrics:\n",
        "   - `mean_squared_error(y_test, y_pred)`: Calculates the mean squared error between the actual test labels (`y_test`) and the predicted labels (`y_pred`).\n",
        "   - `r2_score(y_test, y_pred)`: Calculates the R-squared value, which represents the proportion of variance in the dependent variable (`y_test`) that is predictable from the independent variable (`y_pred`).\n",
        "\n",
        "7. Visualize the model's predictions:\n",
        "   - `plt.scatter(X_test, y_test, color='blue', label='Actual')`: Plots a scatter plot of the actual test data points (`X_test`, `y_test`) in blue.\n",
        "   - `plt.plot(X_test, y_pred, color='red', linewidth=2, label='Predicted')`: Plots a red line representing the model's predictions (`X_test`, `y_pred`) based on the fitted linear regression model.\n",
        "   - `plt.xlabel(\"X\")`: Adds a label to the x-axis of the plot.\n",
        "   - `plt.ylabel(\"y\")`: Adds a label to the y-axis of the plot.\n",
        "   - `plt.legend()`: Adds a legend to the plot.\n",
        "   - `plt.show()`: Displays the plot.\n",
        "\n",
        "This code demonstrates a simple linear regression example using scikit-learn. It generates sample data, splits it into training and testing sets, trains a linear regression model, makes predictions on the test set, evaluates the model's performance, and visualizes the actual and predicted data points."
      ],
      "metadata": {
        "id": "yi3UHl0Gvbj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world application"
      ],
      "metadata": {
        "id": "0tND6eRtR9WU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider a real-world example of using linear regression in a healthcare setting to predict patient hospital readmission rates based on various medical and demographic features. The goal is to identify factors that may contribute to higher readmission rates, allowing healthcare providers to take proactive measures to reduce readmissions and improve patient outcomes.\n",
        "\n",
        "**Example: Predicting Hospital Readmission Rates using Linear Regression**\n",
        "\n",
        "**Dataset:**\n",
        "Suppose we have a dataset containing the following features for a group of patients:\n",
        "\n",
        "1. Age: The age of the patient (numeric).\n",
        "2. BMI: Body Mass Index, a measure of body fat based on height and weight (numeric).\n",
        "3. Blood Pressure: Systolic and diastolic blood pressure readings (numeric).\n",
        "4. Diabetes: Binary variable indicating whether the patient has diabetes (0 = No, 1 = Yes).\n",
        "5. Heart Disease: Binary variable indicating whether the patient has a history of heart disease (0 = No, 1 = Yes).\n",
        "6. Length of Stay: The number of days the patient stayed in the hospital during their last admission (numeric).\n",
        "7. Number of Previous Admissions: The number of times the patient has been admitted to the hospital in the past (numeric).\n",
        "8. Readmission Rate: The target variable, representing the percentage of patients readmitted within 30 days of discharge (numeric).\n",
        "\n",
        "**Objective:**\n",
        "We want to build a linear regression model to predict the readmission rate of patients based on their demographic and medical features.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. **Data Preprocessing:**\n",
        "   - Handle missing values: Deal with any missing data in the dataset, either by imputing missing values or removing incomplete records.\n",
        "   - Feature Scaling: Scale the features, if necessary, to ensure all variables are on a similar scale. For example, normalize BMI, blood pressure, and length of stay.\n",
        "\n",
        "2. **Data Splitting:**\n",
        "   - Split the dataset into a training set and a test set. The training set will be used to train the linear regression model, while the test set will be used to evaluate its performance.\n",
        "\n",
        "3. **Linear Regression Model Training:**\n",
        "   - Train a linear regression model using the training data. The model will learn the coefficients (weights) for each feature, which represent the relationship between the features and the target variable (readmission rate).\n",
        "\n",
        "4. **Model Evaluation:**\n",
        "   - Evaluate the trained linear regression model using the test data. Calculate metrics such as Mean Squared Error (MSE), R-squared, or Mean Absolute Error (MAE) to assess the model's performance.\n",
        "\n",
        "5. **Feature Importance:**\n",
        "   - Analyze the coefficients of the linear regression model to identify the most influential features on readmission rates. This helps healthcare providers understand which factors have the most significant impact on readmission risk.\n",
        "\n",
        "6. **Prediction and Insights:**\n",
        "   - Use the trained linear regression model to predict readmission rates for new patients based on their medical and demographic information.\n",
        "   - Provide insights and recommendations to healthcare providers based on the model's findings. For example, if the length of stay is a significant predictor of readmission rates, healthcare providers may focus on optimizing patient care during hospital stays to reduce the risk of readmission.\n",
        "\n",
        "By employing a linear regression model in this healthcare setting, we can gain valuable insights into the factors affecting hospital readmission rates and develop strategies to improve patient care and outcomes."
      ],
      "metadata": {
        "id": "hGaNNWkaDqDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAQ"
      ],
      "metadata": {
        "id": "8wb9CG7PYnne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is Linear Regression, and how does it work?\n",
        "   Linear Regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data. It tries to find the best-fitting straight line through the data points, minimizing the sum of the squared differences between the predicted and actual values.\n",
        "\n",
        "2. When should I use Linear Regression?\n",
        "   Linear Regression is commonly used when there is a linear relationship between the dependent and independent variables. It is suitable for predicting continuous numeric values and understanding the strength and direction of the relationships between variables.\n",
        "\n",
        "3. How is the performance of a Linear Regression model measured?\n",
        "   The performance of a Linear Regression model is typically evaluated using metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R-squared (R2) value, which indicates the proportion of variance in the dependent variable that is predictable from the independent variables.\n",
        "\n",
        "4. Can Linear Regression handle categorical variables?\n",
        "   Linear Regression is designed for continuous numeric variables. However, categorical variables can be incorporated using techniques like one-hot encoding or dummy variable encoding to convert them into numerical representations.\n",
        "\n",
        "5. What are the assumptions of Linear Regression?\n",
        "   Linear Regression relies on certain assumptions, including linearity (the relationship between variables is linear), independence of errors (residuals), normality of errors, constant variance (homoscedasticity), and no multicollinearity (no strong correlation between independent variables).\n",
        "\n",
        "6. Is it possible to have negative R-squared values in Linear Regression?\n",
        "   Yes, it is possible to have negative R-squared values in Linear Regression. This occurs when the model's fit is worse than the average of the dependent variable's sample mean. It indicates that the model is not capturing the variation in the data and might not be suitable for the given dataset.\n",
        "\n",
        "7. What is the difference between Simple Linear Regression and Multiple Linear Regression?\n",
        "   Simple Linear Regression involves one independent variable and one dependent variable, fitting a straight line to the data. On the other hand, Multiple Linear Regression deals with multiple independent variables to predict a single dependent variable using a multi-dimensional hyperplane.\n",
        "\n",
        "8. Can outliers affect the performance of a Linear Regression model?\n",
        "   Yes, outliers can significantly influence the performance of a Linear Regression model. Outliers can pull the regression line away from the majority of data points, affecting the model's accuracy and making it less reliable for making predictions.\n",
        "\n",
        "9. Is it possible to use Linear Regression for non-linear relationships?\n",
        "   Although Linear Regression assumes a linear relationship between variables, it is possible to model certain non-linear relationships using techniques like polynomial regression, log transformation, or introducing interaction terms to capture more complex patterns.\n",
        "\n",
        "10. How can multicollinearity impact the interpretation of a Linear Regression model?\n",
        "    Multicollinearity occurs when two or more independent variables are highly correlated. This can lead to issues in the model interpretation, as it becomes challenging to distinguish the individual effects of these variables on the dependent variable. It can also cause instability in the model's coefficients and predictions."
      ],
      "metadata": {
        "id": "r6odQcqJ5W2u"
      }
    }
  ]
}