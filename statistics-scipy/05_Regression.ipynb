{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regression analysis\n"
      ],
      "metadata": {
        "id": "-yqngcUyfgec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Load the dataset from the provided URL.\n",
        "2. Explore and preprocess the data, splitting it into features (X) and the target variable (y).\n",
        "3. Choose one feature from the dataset to be your independent variable (X) and the target variable as your dependent variable (y).\n",
        "4. Split the data into training and testing sets.\n",
        "5. Perform simple linear regression on the training data using the selected feature.\n",
        "6. Make predictions on the test data using the trained model.\n",
        "7. Evaluate the model's performance using appropriate metrics (e.g., mean squared error, R-squared).\n"
      ],
      "metadata": {
        "id": "4NP4EXfrSNlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import linregress\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\n",
        "    \"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\",\n",
        "    \"BMI\", \"DiabetesPedigree\", \"Age\", \"Outcome\"\n",
        "]\n",
        "data = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Step 2: Preprocess the data\n",
        "X = data[[\"Glucose\"]]  # Choose the feature 'Glucose' as the independent variable\n",
        "y = data[\"Outcome\"]\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Perform simple linear regression\n",
        "slope, intercept, r_value, _, _ = linregress(X_train.values.flatten(), y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test data\n",
        "y_pred = slope * X_test.values.flatten() + intercept\n",
        "\n",
        "# Step 6: Evaluate the model's performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Regression Coefficients:\")\n",
        "print(\"Slope (Coefficient):\", slope)\n",
        "print(\"Intercept:\", intercept)\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r_squared)\n"
      ],
      "metadata": {
        "id": "DEG4nwEQSCFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview"
      ],
      "metadata": {
        "id": "mXOD8SqOftGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression analysis is a statistical technique widely used in data science to model the relationship between a dependent variable (or target variable) and one or more independent variables (or predictor variables). The goal of regression analysis is to understand how changes in the independent variables are associated with changes in the dependent variable, enabling us to make predictions and draw insights from the data.\n",
        "\n",
        "In data science, regression analysis serves various purposes, including:\n",
        "\n",
        "1. Prediction: By fitting a regression model to historical data, we can predict the values of the dependent variable for new data points, making it valuable for forecasting future trends and outcomes.\n",
        "\n",
        "2. Understanding Relationships: Regression analysis helps us quantify the strength, direction, and significance of the relationships between the independent and dependent variables. It allows us to identify which factors have the most significant impact on the target variable.\n",
        "\n",
        "3. Hypothesis Testing: Regression analysis can be used to test hypotheses about the relationships between variables. By comparing the coefficients of the independent variables to their standard errors, we can determine whether the relationships are statistically significant.\n",
        "\n",
        "4. Feature Importance: In predictive modeling, regression can help determine the importance of different features or predictors in explaining the variability of the target variable.\n",
        "\n",
        "There are several types of regression techniques, and the choice of the appropriate method depends on the nature of the data and the research question. Some common types of regression analysis include:\n",
        "\n",
        "1. Linear Regression: The simplest form of regression, used when the relationship between the dependent and independent variables is linear. The model assumes that the relationship can be represented by a straight line.\n",
        "\n",
        "2. Multiple Regression: An extension of linear regression, used when there are multiple independent variables. It helps identify the combined effects of all predictors on the dependent variable.\n",
        "\n",
        "3. Polynomial Regression: When the relationship between the variables is nonlinear, polynomial regression fits a polynomial equation to the data, allowing for more flexible curves.\n",
        "\n",
        "4. Logistic Regression: While the name contains \"regression,\" logistic regression is a classification technique used for predicting binary outcomes. It models the probability of a binary response based on predictor variables.\n",
        "\n",
        "5. Ridge Regression and Lasso Regression: Techniques used for regularization to prevent overfitting in multiple regression models by adding penalty terms to the coefficients.\n",
        "\n",
        "Regression analysis involves estimating the parameters of the regression equation, such as coefficients and intercept, using statistical techniques like Ordinary Least Squares (OLS) for linear regression. These parameters represent the best-fit line or curve that minimizes the differences between predicted values and actual values in the training data.\n",
        "\n",
        "In summary, regression analysis is a fundamental tool in data science that allows us to model and understand relationships between variables, make predictions, and gain valuable insights from data."
      ],
      "metadata": {
        "id": "fTJ8Ox74Vh4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Linear Regression\n",
        "To perform simple linear regression using Python's SciPy library on the Pima Indians Diabetes dataset, we can follow these steps:\n",
        "\n",
        "**Step 1: Import the required libraries and load the data**\n"
      ],
      "metadata": {
        "id": "o-pLxVMkfzXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "data = pd.read_csv(url, names=names)\n",
        "\n",
        "# Select the predictor (X) and target (y) variables\n",
        "X = data['BMI']\n",
        "y = data['Glucose']\n"
      ],
      "metadata": {
        "id": "o4mBCneKf3Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Perform linear regression using SciPy**\n"
      ],
      "metadata": {
        "id": "s8wGM5Esf6JL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform linear regression using the linregress function from scipy.stats\n",
        "slope, intercept, r_value, p_value, std_err = stats.linregress(X, y)\n",
        "\n",
        "# Print the slope and intercept of the linear regression line\n",
        "print(\"Slope:\", slope)\n",
        "print(\"Intercept:\", intercept)\n"
      ],
      "metadata": {
        "id": "thNepJSGf9CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Visualize the linear regression line**\n"
      ],
      "metadata": {
        "id": "F8AUDJuJgApg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the original data points\n",
        "plt.scatter(X, y, alpha=0.5)\n",
        "\n",
        "# Add the linear regression line to the plot\n",
        "plt.plot(X, slope * X + intercept, color='red')\n",
        "\n",
        "# Add labels and title to the plot\n",
        "plt.xlabel('BMI')\n",
        "plt.ylabel('Glucose')\n",
        "plt.title('Linear Regression: BMI vs. Glucose')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "u4tTj2DNgDV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we use the SciPy library's `stats.linregress()` function to perform simple linear regression. It calculates the slope, intercept, correlation coefficient (r-value), p-value, and standard error of the regression line. We then plot the original data points and add the linear regression line to visualize the relationship between BMI and Glucose levels.\n",
        "\n",
        "Please note that this is a simple example, and in practice, you might need to handle missing values, perform data preprocessing, validate the model, and consider additional factors such as multicollinearity or model assumptions.\n"
      ],
      "metadata": {
        "id": "XNyoERN1gGsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "peHdgpf3VofI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, for logistic regression analysis on the Pima Indian dataset, it is recommended to use specialized libraries like Scikit-learn, which provide robust implementations of logistic regression and other machine learning algorithms. Scikit-learn is widely used, well-documented, and provides a user-friendly API for machine learning tasks.\n",
        "\n",
        "This highlights an issue where another library ie Scikit-learn can provide a better algorithm to what is provided in SciPy.\n",
        "\n",
        "Here's how you can perform logistic regression on the Pima Indian dataset using Scikit-learn:"
      ],
      "metadata": {
        "id": "ZzMBiM9vVsmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the dataset from the provided URL\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "data = pd.read_csv(url, header=None)\n",
        "\n",
        "# Assigning column names to the dataset\n",
        "data.columns = [\n",
        "    \"pregnancies\",\n",
        "    \"glucose\",\n",
        "    \"blood_pressure\",\n",
        "    \"skin_thickness\",\n",
        "    \"insulin\",\n",
        "    \"bmi\",\n",
        "    \"diabetes_pedigree\",\n",
        "    \"age\",\n",
        "    \"outcome\",\n",
        "]\n",
        "\n",
        "# Preprocessing the data\n",
        "X = data.drop(\"outcome\", axis=1)  # Features\n",
        "y = data[\"outcome\"]  # Target variable\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "logreg_model = LogisticRegression()\n",
        "\n",
        "# Fitting the model to the training data\n",
        "logreg_model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the test set\n",
        "y_pred = logreg_model.predict(X_test)\n",
        "\n",
        "# Evaluating the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "YFZZoffRWcUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we use Scikit-learn to perform logistic regression. The dataset is loaded and preprocessed similarly to the previous example. We then split the data into training and testing sets, fit a logistic regression model to the training data, make predictions on the test set, and evaluate the model's performance using accuracy, classification report, and confusion matrix.\n",
        "\n",
        "Scikit-learn makes it much easier to work with machine learning algorithms, including logistic regression, and is the recommended approach for this type of analysis.\n",
        "\n",
        "Now lets compare this with SciPy's approach:"
      ],
      "metadata": {
        "id": "9vQocTVbWfXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.optimize as optimize\n",
        "\n",
        "# Load the dataset from the provided URL\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "data = pd.read_csv(url, header=None)\n",
        "\n",
        "# Assigning column names to the dataset\n",
        "data.columns = [\n",
        "    \"pregnancies\",\n",
        "    \"glucose\",\n",
        "    \"blood_pressure\",\n",
        "    \"skin_thickness\",\n",
        "    \"insulin\",\n",
        "    \"bmi\",\n",
        "    \"diabetes_pedigree\",\n",
        "    \"age\",\n",
        "    \"outcome\",\n",
        "]\n",
        "\n",
        "# Preprocessing the data\n",
        "X = data.drop(\"outcome\", axis=1)  # Features\n",
        "y = data[\"outcome\"]  # Target variable\n",
        "\n",
        "# Implementing Logistic Regression using Scipy\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def cost_function(theta, X, y):\n",
        "    z = np.dot(X, theta)\n",
        "    h = sigmoid(z)\n",
        "    epsilon = 1e-5  # to avoid division by zero\n",
        "    return -np.mean(y * np.log(h + epsilon) + (1 - y) * np.log(1 - h + epsilon))\n",
        "\n",
        "def gradient(theta, X, y):\n",
        "    z = np.dot(X, theta)\n",
        "    h = sigmoid(z)\n",
        "    return np.dot(X.T, (h - y)) / len(y)\n",
        "\n",
        "def logistic_regression(X, y):\n",
        "    # Adding an intercept term (bias)\n",
        "    X = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "\n",
        "    # Initialize theta (parameters) with zeros\n",
        "    theta = np.zeros(X.shape[1])\n",
        "\n",
        "    # Using Scipy's optimize.minimize function to find the optimal parameters\n",
        "    result = optimize.minimize(\n",
        "        cost_function, theta, args=(X, y), jac=gradient, method=\"BFGS\"\n",
        "    )\n",
        "    optimal_params = result.x\n",
        "    return optimal_params\n",
        "\n",
        "# Fit the logistic regression model\n",
        "optimal_params = logistic_regression(X, y)\n",
        "\n",
        "# Display the coefficients for each feature\n",
        "feature_names = [\"Intercept\"] + list(X.columns)\n",
        "coefficients = pd.Series(optimal_params, index=feature_names)\n",
        "print(\"Logistic Regression Coefficients:\")\n",
        "print(coefficients)\n",
        "\n",
        "# Note: The coefficients represent the log-odds of having diabetes.\n",
        "# Positive coefficients indicate a higher probability of diabetes,\n",
        "# while negative coefficients indicate a lower probability.\n"
      ],
      "metadata": {
        "id": "3HNd2ah-Wi1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see the SciPy approach is much more detailed in how it executes  a logistic regression.\n",
        "\n",
        "So which do you choose?\n",
        "\n",
        "Both Scipy and Scikit-learn (`scikit-learn`) are powerful Python libraries, but they serve different purposes and have distinct strengths when it comes to logistic regression.\n",
        "\n",
        "1. **Scipy:**\n",
        "   - Scipy is a comprehensive library for scientific computing in Python and includes a wide range of functionality for numerical computations, optimization, integration, interpolation, and more.\n",
        "   - It provides a basic optimization function, `scipy.optimize.minimize()`, which can be used to find the optimal parameters for logistic regression. However, it requires more manual implementation for the logistic regression model itself, including the cost function and gradient calculation.\n",
        "   - Scipy is suitable for users who prefer a more hands-on approach to implementation and customization, or those who need to perform various scientific computations beyond machine learning tasks.\n",
        "\n",
        "2. **Scikit-learn:**\n",
        "   - Scikit-learn is a dedicated machine learning library in Python and is widely used for building and evaluating machine learning models.\n",
        "   - It provides a high-level, user-friendly interface to implement various machine learning algorithms, including logistic regression. The `LogisticRegression` class in Scikit-learn handles the entire logistic regression process, including optimization, regularization, and predictions.\n",
        "   - Scikit-learn offers a wide range of utilities for data preprocessing, model evaluation, and model selection, making it an excellent choice for streamlined machine learning workflows.\n",
        "   - It also provides a consistent API for other machine learning algorithms, allowing users to switch between different models easily.\n",
        "\n",
        "In summary, if you are primarily focused on logistic regression and other machine learning tasks, and you prefer a more user-friendly and efficient implementation, Scikit-learn is the go-to choice. On the other hand, if you have more general scientific computing needs and want more control over the optimization process, Scipy might be more suitable. In many cases, users may even leverage both libraries, combining the strengths of Scipy for general scientific computations and Scikit-learn for machine learning-specific tasks."
      ],
      "metadata": {
        "id": "r5XS5inEWmDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reflection points"
      ],
      "metadata": {
        "id": "3CB3SuWzgU-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Linear Regression:\n",
        "1. Interpretability: Linear regression provides straightforward and easily interpretable results. The coefficients of the regression equation indicate the direction and magnitude of the relationship between the predictor variables and the target variable.\n",
        "\n",
        "2. Assumptions: Linear regression assumes a linear relationship between the predictor variables and the target variable, as well as independence and homoscedasticity of errors. It is essential to check these assumptions before relying on the model's predictions.\n",
        "\n",
        "3. Continuous Dependent Variable: Linear regression is suitable for predicting continuous numeric values. It is commonly used for tasks such as predicting sales, housing prices, or temperature.\n",
        "\n",
        "4. Outliers: Linear regression is sensitive to outliers, which can significantly impact the model's performance. It is important to identify and handle outliers appropriately during data preprocessing.\n",
        "\n",
        "5. Overfitting: Linear regression can suffer from overfitting if there are too many predictor variables relative to the number of observations. Regularization techniques like Ridge or Lasso regression can be applied to mitigate overfitting.\n",
        "\n",
        "Logistic Regression:\n",
        "1. Binary Classification: Logistic regression is widely used for binary classification problems where the target variable has two classes, such as \"yes/no,\" \"spam/not spam,\" or \"1/0.\"\n",
        "\n",
        "2. Probability Interpretation: Unlike linear regression, logistic regression models the probability of an observation belonging to a particular class. The predicted probabilities are then thresholded to make binary predictions.\n",
        "\n",
        "3. Log Odds: Logistic regression works on the log-odds scale, transforming the linear combination of predictor variables into probabilities using the logistic (sigmoid) function.\n",
        "\n",
        "4. Assumptions: Logistic regression assumes that the relationship between the predictor variables and the log-odds of the target variable is linear. Additionally, it assumes that there is little to no multicollinearity among the predictor variables.\n",
        "\n",
        "5. Multi-class Classification: While logistic regression is inherently binary, it can be extended to handle multi-class classification problems using techniques like One-vs-Rest (OvR) or Multinomial logistic regression.\n",
        "\n",
        "6. Decision Boundary: The decision boundary in logistic regression is a hyperplane that separates the two classes. It's important to visualize and understand how this boundary influences the model's predictions.\n",
        "\n",
        "7. Imbalanced Data: Logistic regression can be affected by imbalanced datasets, where one class dominates the other in terms of the number of observations. Techniques like class weighting or resampling can be used to address this issue.\n",
        "\n",
        "Both linear regression and logistic regression are fundamental and widely used techniques in statistical modeling and machine learning. Understanding their strengths, limitations, and assumptions is crucial for selecting the appropriate model for a given problem and interpreting the results accurately."
      ],
      "metadata": {
        "id": "GvL_ZsiygZ-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise"
      ],
      "metadata": {
        "id": "uoxAqf13R43I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Load the dataset using Pandas.\n",
        "2. Preprocess the data by splitting it into features and target variables.\n",
        "3. Divide the data into training and testing sets.\n",
        "4. Build a logistic regression model using scipy's `optimize.minimize` function.\n",
        "5. Evaluate the model's performance on the testing set.\n"
      ],
      "metadata": {
        "id": "bchr16Y8TbOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.optimize as opt\n",
        "\n",
        "# Step 1: Load the dataset using Pandas\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "data = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Step 2: Preprocess the data\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "# Step 3: Divide the data into training and testing sets\n",
        "# You can choose an appropriate split ratio for training and testing data.\n",
        "# For example, you can use 80% for training and 20% for testing.\n",
        "# Make sure to set a random seed to ensure reproducibility.\n",
        "\n",
        "# Step 4: Build a logistic regression model using scipy's optimize.minimize function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def cost_function(theta, X, y):\n",
        "    m = len(y)\n",
        "    h = sigmoid(X.dot(theta))\n",
        "    J = -(1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
        "    return J\n",
        "\n",
        "def gradient(theta, X, y):\n",
        "    m = len(y)\n",
        "    h = sigmoid(X.dot(theta))\n",
        "    grad = (1/m) * X.T.dot(h - y)\n",
        "    return grad\n",
        "\n",
        "# Initialize theta with zeros and use optimize.minimize to find the optimal parameters\n",
        "initial_theta = np.zeros(X.shape[1])\n",
        "result = opt.minimize(cost_function, initial_theta, args=(X_train, y_train), method='BFGS', jac=gradient)\n",
        "optimal_params = result.x\n",
        "\n",
        "# Step 5: Evaluate the model's performance on the testing set\n",
        "def predict(theta, X):\n",
        "    return (sigmoid(X.dot(theta)) >= 0.5).astype(int)\n",
        "\n",
        "y_pred = predict(optimal_params, X_test)\n",
        "\n",
        "# Calculate accuracy on the testing set\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "v_RcKWBGSktY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A quiz on Regression analysis"
      ],
      "metadata": {
        "id": "UsceH9BCYHSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is the primary objective of Simple Linear Regression?\n",
        "   <br>a) To predict a continuous dependent variable based on one or more independent variables.\n",
        "   <br>b) To classify data points into different categories.\n",
        "   <br>c) To find the correlation between two dependent variables.\n",
        "   <br>d) To find the area under the curve of a given dataset.\n",
        "\n",
        "2. In Simple Linear Regression, the relationship between the independent variable (X) and dependent variable (Y) is modeled as:\n",
        "   <br>a) A straight line: Y = mx + c, where m is the slope and c is the intercept.\n",
        "   <br>b) A quadratic function: Y = ax^2 + bx + c.\n",
        "   <br>c) An exponential function: Y = a * e^(bx), where 'e' is the base of natural logarithm.\n",
        "   <br>d) A sine function: Y = a * sin(bx).\n",
        "\n",
        "3. Which of the following statements is true regarding the coefficient of determination (R-squared) in Simple Linear Regression?\n",
        "   <br>a) It has a range of [0, 1], where 0 indicates no correlation and 1 indicates a perfect fit.\n",
        "   <br>b) It can have negative values, indicating a poor model fit.\n",
        "   <br>c) It is calculated as R-squared = 1 - (sum of squared residuals / total sum of squares).\n",
        "   <br>d) R-squared value can exceed 1 if the model is overfit.\n",
        "\n",
        "4. What is the main purpose of Logistic Regression?\n",
        "   <br>a) To predict a continuous dependent variable based on one or more independent variables.\n",
        "   <br>b) To classify data points into different categories (binary or multiclass) using probability scores.\n",
        "   <br>c) To find the correlation between two dependent variables.\n",
        "   <br>d) To estimate the mean and variance of a given dataset.\n",
        "\n",
        "5. In Logistic Regression, the S-shaped curve used to model the relationship between the independent variable (X) and the probability of belonging to a specific class (Y=1) is known as:\n",
        "   <br>a) Step function.\n",
        "   <br>b) Logistic function (sigmoid function).\n",
        "   <br>c) Exponential function.\n",
        "   <br>d) Hyperbolic tangent function.\n",
        "\n",
        "6. Which of the following statements is true regarding the cost function in Logistic Regression?\n",
        "   <br>a) The cost function for Logistic Regression is the Mean Squared Error (MSE).\n",
        "   <br>b) The goal is to minimize the cost function using optimization algorithms like Gradient Descent.\n",
        "   <br>c) The cost function is only used in binary classification problems, not in multiclass problems.\n",
        "   <br>d) The cost function is not necessary for Logistic Regression.\n",
        "\n",
        "7. Scipy is a Python library that provides tools for scientific computing, including functions to perform:\n",
        "   <br>a) Linear algebra operations and optimization routines, but not regression.\n",
        "   <br>b) Statistical analysis, such as hypothesis testing and ANOVA, but not regression.\n",
        "   <br>c) Regression analysis, interpolation, and optimization, among other functionalities.\n",
        "   <br>d) Visualization and plotting, but not regression-related tasks.\n",
        "\n",
        "8. Which Scipy function can be used to perform Simple Linear Regression?\n",
        "   <br>a) scipy.regression.linregress()\n",
        "   <br>b) scipy.stats.linregress()\n",
        "   <br>c) scipy.linear_model.regression()\n",
        "   <br>d) scipy.linear_model.LinearRegression()\n",
        "\n",
        "9. To evaluate a Logistic Regression model, we typically split the data into training and testing sets. What is the purpose of this step?\n",
        "   <br>a) To use a larger dataset for training to get more accurate results.\n",
        "   <br>b) To use the testing set to fit the model and the training set to evaluate its performance.\n",
        "   <br>c) To avoid overfitting and assess the model's generalization to new, unseen data.\n",
        "   <br>d) To increase the computational efficiency of the Logistic Regression algorithm.\n",
        "\n",
        "10. Which Scipy function can be used to perform Logistic Regression?\n",
        "    <br>a) scipy.linear_model.LogisticRegression()\n",
        "    <br>b) scipy.stats.logistic_regression()\n",
        "    <br>c) scipy.regression.logistic_reg()\n",
        "    <br>d) scipy.logistic_model.LogisticRegression()\n",
        "\n",
        "---\n",
        "Answers:\n",
        "1. a) To predict a continuous dependent variable based on one or more independent variables.\n",
        "2. a) A straight line: Y = mx + c, where m is the slope and c is the intercept.\n",
        "3. a) It has a range of [0, 1], where 0 indicates no correlation and 1 indicates a perfect fit.\n",
        "4. b) To classify data points into different categories (binary or multiclass) using probability scores.\n",
        "5. b) Logistic function (sigmoid function).\n",
        "6. b) The goal is to minimize the cost function using optimization algorithms like Gradient Descent.\n",
        "7. c) Regression analysis, interpolation, and optimization, among other functionalities.\n",
        "8. d) scipy.linear_model.LinearRegression()\n",
        "9. c) To avoid overfitting and assess the model's generalization to new, unseen data.\n",
        "10. a) scipy.linear_model.LogisticRegression()\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ohzAYOBNYNRA"
      }
    }
  ]
}