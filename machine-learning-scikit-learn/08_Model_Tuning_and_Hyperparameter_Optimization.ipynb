{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/machine-learning-scikit-learn/blob/main/08_Model_Tuning_and_Hyperparameter_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RONBCglk34lq"
      },
      "source": [
        "#Model Tuning and Hyperparameter Optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Overview"
      ],
      "metadata": {
        "id": "-khs1T5Kz9hy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the world of machine learning, building accurate and robust models is essential for solving complex real-world problems. However, merely choosing a machine learning algorithm is not enough to ensure optimal performance. Model tuning and hyperparameter optimization are crucial steps in the model development process, aimed at fine-tuning the model to achieve the best possible results.\n",
        "\n",
        "**Model Tuning:**\n",
        "Model tuning involves the process of systematically adjusting the parameters and configurations of a machine learning model to improve its performance on a given task. It goes beyond simply selecting the right algorithm and involves tweaking various settings specific to that algorithm. Every machine learning model has one or more hyperparameters, which are values set before training the model and cannot be learned from the data. Tuning these hyperparameters can significantly impact the model's predictive power and generalization capability.\n",
        "\n",
        "**Hyperparameter Optimization:**\n",
        "Hyperparameter optimization, also known as hyperparameter tuning, is the systematic search for the best combination of hyperparameters that results in the optimal model performance. Since the optimal set of hyperparameters can vary from one dataset to another, it is essential to explore different values and configurations. The goal of hyperparameter optimization is to find the hyperparameters that maximize the model's performance metric, such as accuracy, precision, recall, or F1-score.\n",
        "\n",
        "**Challenges in Model Tuning and Hyperparameter Optimization:**\n",
        "Model tuning and hyperparameter optimization can be a challenging task due to several reasons. Firstly, the search space for hyperparameters can be vast and high-dimensional, making a brute-force search impractical. Secondly, the impact of each hyperparameter on the model's performance can be non-linear and complex, requiring a systematic and efficient approach to exploration. Lastly, finding the right balance between underfitting and overfitting the model is critical, as tuning hyperparameters too aggressively may lead to poor generalization.\n",
        "\n",
        "**Python Libraries for Model Tuning and Hyperparameter Optimization:**\n",
        "Python provides several powerful libraries that facilitate model tuning and hyperparameter optimization. One of the most widely used libraries is scikit-learn, which offers various built-in tools for hyperparameter search using techniques like Grid Search, Random Search, and Bayesian Optimization. Additionally, libraries like Hyperopt and Optuna provide more advanced and sophisticated algorithms for hyperparameter optimization, making the process more efficient and effective.\n",
        "\n"
      ],
      "metadata": {
        "id": "fXbQxtCmz_ku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Grid search and random search\n"
      ],
      "metadata": {
        "id": "C9gwckeA0IIL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVNtjXyp37ux"
      },
      "source": [
        "\n",
        "##Grid search\n",
        "Grid search is a technique used in machine learning to search for the optimal hyperparameters of a model. It involves defining a grid of hyperparameter values and exhaustively evaluating the model performance for each combination of hyperparameters. Scikit-Learn provides a built-in `GridSearchCV` class that simplifies the process of grid search.\n",
        "\n",
        "Here's an example of how to use grid search with the Pima Indian Diabetes dataset using Scikit-Learn:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7P8vzqv33_3C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Split the dataset into features and target variable\n",
        "X = dataset.drop('Outcome', axis=1)\n",
        "y = dataset['Outcome']\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Create the model\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMbXjPCw4FgM"
      },
      "source": [
        "In this example, we use the `GridSearchCV` class from Scikit-Learn to perform grid search on a Random Forest Classifier model. We define a parameter grid that includes different values for the number of estimators, maximum depth, minimum samples split, and minimum samples leaf. The `cv` parameter specifies the number of folds for cross-validation.\n",
        "\n",
        "We create an instance of the Random Forest Classifier model and then instantiate the `GridSearchCV` class with the model and parameter grid. The `fit` method is called to perform the grid search and find the best combination of hyperparameters.\n",
        "\n",
        "Finally, we print the best parameters and the best score obtained from the grid search. The best parameters represent the optimal combination of hyperparameters, and the best score indicates the performance of the model using those hyperparameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIpSlRcO4KGm"
      },
      "source": [
        "##Random Search\n",
        "\n",
        "Random search is a technique used in hyperparameter optimization to search for the best combination of hyperparameters for a machine learning algorithm. The Scikit-Learn library provides a `RandomizedSearchCV` class that performs random search efficiently.\n",
        "\n",
        "Here's an example of using random search with the Pima Indian Diabetes dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5ayb7g84QBA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Prepare the feature matrix X and target vector y\n",
        "X = dataset.drop(\"Outcome\", axis=1)\n",
        "y = dataset[\"Outcome\"]\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 200],\n",
        "    \"max_depth\": [None, 5, 10],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 4],\n",
        "}\n",
        "\n",
        "# Create the random forest classifier\n",
        "rf_classifier = RandomForestClassifier()\n",
        "\n",
        "# Perform random search\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf_classifier,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=10,  # Number of parameter settings that are sampled\n",
        "    scoring=\"accuracy\",  # Scoring metric to evaluate the models\n",
        "    cv=5,  # Cross-validation folds\n",
        "    random_state=42  # Random seed for reproducibility\n",
        ")\n",
        "\n",
        "# Fit the random search to the data\n",
        "random_search.fit(X, y)\n",
        "\n",
        "# Print the best hyperparameters and corresponding accuracy score\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "print(\"Best Accuracy Score:\", random_search.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we use the Pima Indian Diabetes dataset and the Random Forest classifier from Scikit-Learn. We define a hyperparameter grid containing different values for `n_estimators`, `max_depth`, `min_samples_split`, and `min_samples_leaf`. We then create an instance of `RandomizedSearchCV` with the random forest classifier, the hyperparameter grid, and other parameters such as the number of iterations (`n_iter`), scoring metric (`scoring`), cross-validation folds (`cv`), and random seed (`random_state`).\n",
        "\n",
        "Next, we fit the random search to the data by calling the `fit` method with the feature matrix `X` and target vector `y`. The random search will perform a specified number of iterations, evaluating different combinations of hyperparameters and selecting the best set based on the specified scoring metric and cross-validation.\n",
        "\n",
        "Finally, we print the best hyperparameters found by the random search (`random_search.best_params_`) and the corresponding accuracy score (`random_search.best_score_`). These results represent the optimal combination of hyperparameters for the random forest classifier on the Pima Indian Diabetes dataset, as determined by the random search.\n"
      ],
      "metadata": {
        "id": "r_0JxawV_6k5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model performance improvement techniques\n",
        "\n",
        "There are several model performance improvement techniques available in the Scikit-Learn library. Here are a few commonly used techniques:\n",
        "\n",
        "1. Cross-validation: Cross-validation is a technique used to assess the performance of a model on unseen data. It helps in estimating the model's generalization capability. Scikit-Learn provides the `cross_val_score` function to perform cross-validation. Here's an example using the Pima Indian Diabetes dataset:\n"
      ],
      "metadata": {
        "id": "dGG6bS6T__A7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = dataset.drop(\"Outcome\", axis=1)\n",
        "y = dataset[\"Outcome\"]\n",
        "\n",
        "# Create a logistic regression model\n",
        "model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "\n",
        "# Perform cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=5)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-Validation Scores:\", scores)\n"
      ],
      "metadata": {
        "id": "7oHZ5Pp-ADhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we load the Pima Indian Diabetes dataset using Pandas. We then separate the features (`X`) and the target variable (`y`). Next, we create a logistic regression model using the `LogisticRegression` class from Scikit-Learn. Finally, we perform cross-validation using the `cross_val_score` function and specify `cv=5` to perform 5-fold cross-validation. The resulting scores represent the model's performance on each fold.\n",
        "\n",
        "2. Hyperparameter Tuning: Hyperparameters are parameters that are not learned by the model itself but are set before training. Tuning hyperparameters can significantly impact the model's performance. Scikit-Learn provides various techniques like Grid Search and Randomized Search for hyperparameter tuning. Here's an example using the Pima Indian Diabetes dataset and Grid Search:\n"
      ],
      "metadata": {
        "id": "LuvEebT8AJtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = dataset.drop(\"Outcome\", axis=1)\n",
        "y = dataset[\"Outcome\"]\n",
        "\n",
        "# Create a random forest classifier\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "param_grid = {'n_estimators': [100, 200, 300], 'max_depth': [None, 5, 10]}\n",
        "\n",
        "# Perform grid search for hyperparameter tuning\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n"
      ],
      "metadata": {
        "id": "2Uqb6QRkBK-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we load the Pima Indian Diabetes dataset using Pandas and separate the features (`X`) and the target variable (`y`). Next, we create a random forest classifier using the `RandomForestClassifier` class from Scikit-Learn. We define a grid of hyperparameters to tune, including the number of estimators (`n_estimators`) and the maximum depth of the trees (`max_depth`). Then, we perform a grid search using `GridSearchCV`, specifying `cv=5` for 5-fold cross-validation. The grid search will try all combinations of hyperparameters and select the best ones based on the model's performance.\n",
        "\n",
        "These are just a few examples of model performance improvement techniques in Scikit-Learn. Other techniques include feature selection, feature scaling, ensemble methods, and more. The choice of technique depends on the specific problem and the characteristics of the dataset.\n"
      ],
      "metadata": {
        "id": "keUgjtOJBRwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reflection points"
      ],
      "metadata": {
        "id": "hxfD6iFMBUvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **What is grid search, and how does it work in machine learning?**\n",
        "   - Grid search is a technique used to tune hyperparameters of a machine learning model by exhaustively searching through a specified parameter grid. It involves defining a set of hyperparameter values to evaluate and systematically testing each combination to find the best configuration.\n",
        "\n",
        "2. **What are the advantages of grid search over manual tuning?**\n",
        "   - Grid search automates the process of hyperparameter tuning, saving time and effort.\n",
        "   - It ensures that all possible combinations of hyperparameters are considered, reducing the risk of missing the optimal configuration.\n",
        "   - It provides a systematic and organized approach to finding the best hyperparameter values, enhancing reproducibility.\n",
        "\n",
        "3. **What are the limitations of grid search?**\n",
        "   - Grid search can be computationally expensive, especially when dealing with a large number of hyperparameters and their potential values.\n",
        "   - It may not be suitable for datasets with limited computational resources or strict time constraints.\n",
        "   - Grid search assumes that hyperparameters are independent of each other, which may not always be the case.\n",
        "\n",
        "4. **What is random search, and how does it differ from grid search?**\n",
        "   - Random search is an alternative hyperparameter optimization technique that randomly samples combinations of hyperparameter values within a defined search space. It explores a more diverse set of configurations compared to grid search, which systematically evaluates all combinations.\n",
        "\n",
        "5. **What are the advantages of random search over grid search?**\n",
        "   - Random search can be more efficient when the number of hyperparameters and their potential values is large.\n",
        "   - It allows for a more extensive exploration of the hyperparameter space, potentially discovering better configurations in fewer iterations.\n",
        "   - Random search can handle cases where some hyperparameters are less influential than others, as it randomly samples across the search space.\n",
        "\n",
        "6. **How do you evaluate model performance during grid search or random search?**\n",
        "   - Model performance is typically evaluated using a chosen evaluation metric, such as accuracy, precision, recall, or F1 score, depending on the problem at hand. Cross-validation is often employed to obtain robust estimates of the model's performance for each hyperparameter configuration.\n",
        "\n",
        "7. **What are some model performance improvement techniques?**\n",
        "   - Feature engineering: Creating new features or transforming existing ones to enhance the model's ability to capture patterns and make accurate predictions.\n",
        "   - Ensemble methods: Combining multiple models to leverage their strengths and improve overall performance, such as bagging, boosting, or stacking.\n",
        "   - Regularization techniques: Applying penalties to the model's coefficients to reduce overfitting and improve generalization.\n",
        "   - Data augmentation: Generating additional training data by applying transformations or introducing noise to enhance the model's ability to generalize.\n",
        "   - Model selection: Exploring different algorithms or architectures to identify the most suitable model for a given task.\n",
        "\n",
        "8. **How can you determine if a model is overfitting or underfitting?**\n",
        "   - Overfitting occurs when a model performs well on the training data but fails to generalize to unseen data. It may be identified by a significant gap between training and validation/test performance or high variance in cross-validation results.\n",
        "   - Underfitting, on the other hand, occurs when a model fails to capture the underlying patterns in the data, resulting in poor performance on both training and validation/test sets.\n",
        "\n",
        "9. **What techniques can be used to address overfitting or underfitting?**\n",
        "   - Regularization techniques (e.g., L1/L2 regularization, dropout) can help combat overfitting by reducing the model's complexity and discouraging excessive reliance on individual features.\n",
        "   - Increasing model complexity (e.g., adding more layers or nodes) may help address underfitting by allowing the model to capture more intricate patterns in the data.\n",
        "   - Collecting more data or performing data augmentation can also help mitigate underfitting by providing the model with more diverse examples to learn from.\n"
      ],
      "metadata": {
        "id": "cqNWs-_OBZIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A quiz on Model Tuning and Hyperparameter Optimization"
      ],
      "metadata": {
        "id": "msleMtq50WET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is hyperparameter optimization?\n",
        "   <br>a) The process of selecting the best machine learning model.\n",
        "   <br>b) The process of fine-tuning the hyperbolic tangent activation function.\n",
        "   <br>c) The process of finding the best hyperparameters for a machine learning model.\n",
        "   <br>d) The process of preprocessing data for machine learning.\n",
        "\n",
        "2. Which of the following methods is NOT used for hyperparameter optimization?\n",
        "   <br>a) Grid Search\n",
        "   <br>b) Random Search\n",
        "   <br>c) Bayesian Optimization\n",
        "   <br>d) Linear Regression\n",
        "\n",
        "3. True or False: Hyperparameters are learned from the data during the training process.\n",
        "   \n",
        "4. What is overfitting in the context of hyperparameter tuning?\n",
        "   <br>a) When a model performs well on the test data but poorly on the training data.\n",
        "   <br>b) When a model performs well on the training data but poorly on unseen data.\n",
        "   <br>c) When a model has too many hyperparameters to tune effectively.\n",
        "   <br>d) When a model's hyperparameters are set to their default values.\n",
        "\n",
        "5. Which library in Python is commonly used for hyperparameter optimization?\n",
        "   <br>a) TensorFlow\n",
        "   <br>b) PyTorch\n",
        "   <br>c) Scikit-learn\n",
        "   <br>d) Pandas\n",
        "\n",
        "6. What is the purpose of cross-validation in hyperparameter tuning?\n",
        "   <br>a) To ensure that the model generalizes well on unseen data.\n",
        "   <br>b) To reduce the number of hyperparameters in the model.\n",
        "   <br>c) To speed up the hyperparameter optimization process.\n",
        "   <br>d) To make the model more complex.\n",
        "\n",
        "7. Which hyperparameter optimization method uses probability models to model the search space?\n",
        "   <br>a) Grid Search\n",
        "   <br>b) Random Search\n",
        "   <br>c) Bayesian Optimization\n",
        "   <br>d) Linear Regression\n",
        "\n",
        "8. True or False: Hyperparameter tuning always guarantees a significant improvement in model performance.\n",
        "\n",
        "9. Which of the following techniques can help prevent overfitting during hyperparameter optimization?\n",
        "   <br>a) Increasing the number of hyperparameters.\n",
        "   <br>b) Using a larger dataset for training.\n",
        "   <br>c) Using a smaller number of cross-validation folds.\n",
        "   <br>d) Regularization techniques.\n",
        "\n",
        "10. What is the primary drawback of using Grid Search for hyperparameter optimization?\n",
        "   <br>a) It can be computationally expensive.\n",
        "   <br>b) It only works for a single hyperparameter.\n",
        "   <br>c) It always finds the best hyperparameter values.\n",
        "   <br>d) It requires specialized hardware.\n",
        "\n",
        "---\n",
        "**Answers:**\n",
        "\n",
        "1. c) The process of finding the best hyperparameters for a machine learning model.\n",
        "2. d) Linear Regression\n",
        "3. False\n",
        "4. b) When a model performs well on the training data but poorly on unseen data.\n",
        "5. c) Scikit-learn\n",
        "6. a) To ensure that the model generalizes well on unseen data.\n",
        "7. c) Bayesian Optimization\n",
        "8. False\n",
        "9. d) Regularization techniques.\n",
        "10. a) It can be computationally expensive.\n",
        "---"
      ],
      "metadata": {
        "id": "OqVVS2Dh0XiT"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM0/8CJHPVBmJzAi4MUP66y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}