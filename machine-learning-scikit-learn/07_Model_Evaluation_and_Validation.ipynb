{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation and Validation\n"
      ],
      "metadata": {
        "id": "T7exouHr2clU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview"
      ],
      "metadata": {
        "id": "HlNtoNiVx7h3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model evaluation and validation are essential steps in the machine learning workflow to ensure that the trained model performs well on unseen data and generalizes effectively. Python provides a variety of libraries and tools to perform these tasks efficiently. In this paragraph, we will discuss the key concepts and techniques involved in model evaluation and validation in Python.\n",
        "\n",
        "One of the primary goals of model evaluation is to assess the performance of a trained machine learning model. A common practice is to split the dataset into two subsets: the training set and the testing set. The training set is used to train the model, while the testing set is used to evaluate its performance. Scikit-learn, a popular Python library for machine learning, provides the `train_test_split` function to easily perform this data splitting.\n",
        "\n",
        "To evaluate the model's performance, various metrics can be employed, depending on the type of machine learning problem. For classification tasks, metrics such as accuracy, precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC-ROC) are commonly used. For regression tasks, metrics like mean squared error (MSE), mean absolute error (MAE), and R-squared are frequently employed.\n",
        "\n",
        "Cross-validation is another critical technique in model evaluation, especially when the dataset is limited. It involves dividing the data into multiple subsets (folds) and iteratively training and testing the model on different combinations of these folds. This process helps in obtaining a more robust estimate of the model's performance. Scikit-learn provides the `cross_val_score` function to perform cross-validation easily.\n",
        "\n",
        "Model validation is the process of tuning hyperparameters to find the best configuration for the model. Grid search and randomized search are popular techniques used in Python, and they can be implemented using the `GridSearchCV` and `RandomizedSearchCV` classes from Scikit-learn. These methods automatically explore various hyperparameter combinations to find the optimal configuration that yields the best performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "cY_rz8zqx-NU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and testing split\n",
        "\n"
      ],
      "metadata": {
        "id": "ediz_MZe2gJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview"
      ],
      "metadata": {
        "id": "agloWGSR61EZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Training and testing split is a fundamental concept in machine learning that plays a crucial role in developing reliable and accurate predictive models. When building a machine learning model, it is essential to evaluate its performance on unseen data to assess its generalization ability. The process of dividing the available dataset into two distinct subsets - the training set and the testing set - enables us to achieve this objective.\n",
        "\n",
        "**1. Purpose of Training and Testing Split:**\n",
        "The main purpose of the training and testing split is to assess how well a machine learning model can generalize to new, unseen data. During the training phase, the model learns the underlying patterns and relationships present in the training data, adjusting its parameters to minimize the prediction errors. However, it is critical to ensure that the model does not simply memorize the training data but rather captures the underlying patterns that apply to unseen data as well. The testing set serves as a proxy for the real-world data that the model will encounter in production, allowing us to evaluate its performance on data it has never seen before.\n",
        "\n",
        "**2. Dividing the Dataset:**\n",
        "The process of creating a training and testing split involves dividing the original dataset into two distinct and non-overlapping subsets. Typically, the majority of the data (around 70-80%) is allocated to the training set, while the remaining portion (around 20-30%) becomes the testing set. This division ensures that the model is exposed to a sufficient amount of data during training while leaving enough unseen data for evaluation.\n",
        "\n",
        "**3. Importance of Unbiased Split:**\n",
        "It is crucial to ensure that the training and testing sets are representative of the overall dataset. The split should be unbiased, meaning that both subsets should contain a diverse and random sampling of data points from different classes or categories. An unbiased split helps avoid introducing biases into the model's training process, leading to more accurate and reliable performance evaluations.\n",
        "\n",
        "**4. Cross-Validation:**\n",
        "In some cases, a single train-test split may not provide a robust evaluation of the model's performance. To overcome this limitation and obtain more reliable results, techniques like k-fold cross-validation can be employed. Cross-validation involves dividing the data into multiple subsets or \"folds,\" using each fold alternately as the testing set while the remaining folds are used for training. This process is repeated several times, and the performance metrics are averaged to obtain a more comprehensive assessment of the model's generalization performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "d3Bw3NQ-62nF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = dataset.drop('Outcome', axis=1)\n",
        "y = dataset['Outcome']\n",
        "\n",
        "# Perform the training and testing split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting splits\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "21OIEug-2kCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first load the Pima Indian Diabetes dataset using Pandas. We then split the dataset into features (X) and the target variable (y). The features are all the columns except for the 'Outcome' column, which represents the presence or absence of diabetes. The target variable is the 'Outcome' column itself.\n",
        "\n",
        "Next, we use the `train_test_split` function from scikit-learn to split the data into training and testing sets. We pass the features (X) and the target variable (y) to the function along with the `test_size` parameter, which specifies the proportion of the dataset that should be allocated to the testing set (in this case, 20%). The `random_state` parameter ensures reproducibility of the split.\n",
        "\n",
        "Finally, we print the shapes of the resulting splits (`X_train`, `X_test`, `y_train`, `y_test`) to verify the sizes of the training and testing sets.\n",
        "\n",
        "Note: Make sure you have the scikit-learn library installed to run this example (`pip install scikit-learn`).\n"
      ],
      "metadata": {
        "id": "yfmlUuUF2nyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-validation techniques\n"
      ],
      "metadata": {
        "id": "mOe_mGLbQ8BW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation is a technique used in machine learning to evaluate the performance and generalization ability of a model. It involves dividing the dataset into multiple subsets, training the model on a portion of the data, and evaluating its performance on the remaining part. Scikit-Learn provides several cross-validation techniques through its `model_selection` module, such as K-fold cross-validation and stratified K-fold cross-validation.\n",
        "\n",
        "Here's an example of using K-fold cross-validation and stratified K-fold cross-validation on the Pima Indian Diabetes dataset:\n"
      ],
      "metadata": {
        "id": "PP2lxtkp2skk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = dataset.drop('Outcome', axis=1)\n",
        "y = dataset['Outcome']\n",
        "\n",
        "# Perform K-fold cross-validation with Logistic Regression\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "model = LogisticRegression()\n",
        "scores = cross_val_score(model, X, y, cv=kfold)\n",
        "\n",
        "print(\"K-fold cross-validation scores:\")\n",
        "print(scores)\n",
        "print(\"Mean accuracy:\", scores.mean())\n",
        "print(\"Standard deviation:\", scores.std())\n",
        "\n",
        "# Perform stratified K-fold cross-validation with Logistic Regression\n",
        "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "model = LogisticRegression()\n",
        "stratified_scores = cross_val_score(model, X, y, cv=stratified_kfold)\n",
        "\n",
        "print(\"Stratified K-fold cross-validation scores:\")\n",
        "print(stratified_scores)\n",
        "print(\"Mean accuracy:\", stratified_scores.mean())\n",
        "print(\"Standard deviation:\", stratified_scores.std())\n"
      ],
      "metadata": {
        "id": "dM9pH8Hp2w3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first load the Pima Indian Diabetes dataset using Pandas. Then, we separate the features (X) and the target variable (y).\n",
        "\n",
        "For K-fold cross-validation, we create a `StratifiedKFold` object with 5 splits, shuffle the data, and set a random seed for reproducibility. We initialize a logistic regression model and use the `cross_val_score` function to perform K-fold cross-validation on the model using the features (X) and target variable (y). The resulting scores are printed along with the mean accuracy and standard deviation.\n",
        "\n",
        "For stratified K-fold cross-validation, we follow a similar process as K-fold but using the `StratifiedKFold` object. This technique ensures that each fold contains approximately the same proportion of target classes as the whole dataset, which is important when dealing with imbalanced datasets. The scores, mean accuracy, and standard deviation are printed for stratified K-fold cross-validation as well.\n",
        "\n",
        "Note that in the example above, we use logistic regression as a simple example model, but you can replace it with any other model or algorithm from Scikit-Learn according to your requirements.\n"
      ],
      "metadata": {
        "id": "HDKkbcpr24Jh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation metrics (accuracy, precision, recall, F1-score)\n"
      ],
      "metadata": {
        "id": "nq1-VqBUQ-HU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Scikit-Learn provides various evaluation metrics to assess the performance of classification models. Here are the commonly used evaluation metrics in Scikit-Learn:\n",
        "\n",
        "1. Accuracy: Accuracy measures the proportion of correctly classified instances out of the total instances. It is computed as the ratio of the number of correct predictions to the total number of predictions.\n",
        "\n",
        "2. Precision: Precision measures the proportion of correctly predicted positive instances out of the total instances predicted as positive. It is computed as the ratio of true positives to the sum of true positives and false positives.\n",
        "\n",
        "3. Recall: Recall, also known as sensitivity or true positive rate, measures the proportion of correctly predicted positive instances out of the total actual positive instances. It is computed as the ratio of true positives to the sum of true positives and false negatives.\n",
        "\n",
        "4. F1-score: The F1-score is the harmonic mean of precision and recall. It provides a balanced measure that considers both precision and recall. F1-score is useful when you want to find a balance between precision and recall.\n",
        "\n",
        "Here's an example using the Pima Indian Diabetes dataset to demonstrate the calculation of these evaluation metrics:\n"
      ],
      "metadata": {
        "id": "P4VdZ_LQ3FtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = dataset.drop('Outcome', axis=1)\n",
        "y = dataset['Outcome']\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n"
      ],
      "metadata": {
        "id": "D2Scg8au3KH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we load the Pima Indian Diabetes dataset using Pandas library. We split the dataset into training and test sets using `train_test_split()` function from Scikit-Learn. Then, we train a logistic regression model on the training set and make predictions on the test set.\n",
        "\n",
        "After that, we calculate the evaluation metrics by comparing the predicted values (`y_pred`) with the actual values (`y_test`). We use the functions `accuracy_score()`, `precision_score()`, `recall_score()`, and `f1_score()` from Scikit-Learn to compute these metrics.\n",
        "\n",
        "Finally, we print the evaluation metrics to see the output.\n"
      ],
      "metadata": {
        "id": "mjSb0D2r3OgB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overfitting and underfitting\n"
      ],
      "metadata": {
        "id": "7tCARNTrQ_-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Overfitting and underfitting are two common problems in machine learning models, including those implemented with the Scikit-Learn library. Let's understand each concept and provide examples using the Pima Indian Diabetes dataset.\n",
        "\n",
        "1. Overfitting:\n",
        "Overfitting occurs when a model learns the training data too well and becomes too specialized, failing to generalize well to new, unseen data. It often happens when the model is too complex and captures noise or random fluctuations in the training data. Signs of overfitting include high accuracy on the training set but poor performance on the test/validation set.\n",
        "\n",
        "Example:\n"
      ],
      "metadata": {
        "id": "nGzVXd5v3Rxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Split the dataset into features and target variable\n",
        "X = dataset.drop('Outcome', axis=1)\n",
        "y = dataset['Outcome']\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an overly complex Decision Tree classifier\n",
        "model = DecisionTreeClassifier(max_depth=10)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the training set\n",
        "train_predictions = model.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, train_predictions)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_predictions = model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "\n",
        "# Print the accuracies\n",
        "print(\"Training accuracy:\", train_accuracy)\n",
        "print(\"Test accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "w997uPZI3WgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we use a Decision Tree classifier with a max_depth of 10. The model is trained on the Pima Indian Diabetes dataset, and the accuracies are calculated for both the training and test sets. If the training accuracy is significantly higher than the test accuracy, it indicates overfitting."
      ],
      "metadata": {
        "id": "NlwF6VhA3aWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Underfitting:\n",
        "Underfitting occurs when a model is too simple to capture the underlying patterns in the data. It often happens when the model has insufficient complexity or features to learn from the data. Signs of underfitting include low accuracy on both the training and test/validation sets.\n",
        "\n",
        "Example:\n"
      ],
      "metadata": {
        "id": "A18Q-9x13dGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Split the dataset into features and target variable\n",
        "X = dataset.drop('Outcome', axis=1)\n",
        "y = dataset['Outcome']\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an overly simple Logistic Regression classifier\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the training set\n",
        "train_predictions = model.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, train_predictions)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_predictions = model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "\n",
        "# Print the accuracies\n",
        "print(\"Training accuracy:\", train_accuracy)\n",
        "print(\"Test accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "YA3DG74X3heM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we use a simple Logistic Regression classifier. The model is trained on the Pima Indian Diabetes dataset, and the accuracies are calculated for both the training and test sets. If both the training and test accuracies are low, it suggests underfitting, indicating that the model is too simple to capture the patterns in the data effectively.\n",
        "\n",
        "To mitigate overfitting, you can reduce the model complexity (e.g., by reducing the number of features or decreasing the model's hyperparameters) or use regularization techniques. To address underfitting, you can try increasing the model complexity (e.g., by adding more features, using a more complex algorithm, or adjusting hyperparameters) or using more advanced algorithms.\n"
      ],
      "metadata": {
        "id": "6xx0gpyg3lfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reflection points"
      ],
      "metadata": {
        "id": "GaLH7XX73oNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Training and Testing Split:**\n",
        "   - What is the purpose of splitting data into training and testing sets?\n",
        "     - The purpose is to evaluate the performance of a machine learning model on unseen data. Training set is used for model training, while testing set helps assess how well the model generalizes.\n",
        "\n",
        "   - What is the recommended ratio for splitting data into training and testing sets?\n",
        "     - A common practice is to use a 70-30 or 80-20 ratio, where 70% or 80% of the data is used for training and the remaining portion for testing. However, the ratio can vary depending on the dataset size and specific requirements.\n",
        "\n",
        "2. **Cross-Validation Techniques:**\n",
        "   - What is cross-validation, and why is it useful?\n",
        "     - Cross-validation is a technique to assess the model's performance by partitioning the data into multiple subsets and performing training and testing iteratively. It provides a more reliable estimate of the model's generalization ability and helps mitigate issues like overfitting.\n",
        "\n",
        "   - What are some common cross-validation methods?\n",
        "     - Common cross-validation methods include k-fold cross-validation, stratified k-fold cross-validation, and leave-one-out cross-validation. Each method has its own advantages and suits different scenarios.\n",
        "\n",
        "3. **Evaluation Metrics (Accuracy, Precision, Recall, F1-Score):**\n",
        "   - What is accuracy, and when is it suitable to use?\n",
        "     - Accuracy measures the overall correctness of the model's predictions. It is suitable when the class distribution is balanced and false positives and false negatives have similar consequences.\n",
        "\n",
        "   - What is precision, and when is it important to consider?\n",
        "     - Precision measures the proportion of true positive predictions among positive predictions. It is important to consider when false positives have significant consequences, such as in medical diagnoses or spam detection.\n",
        "\n",
        "   - What is recall, and when is it important to consider?\n",
        "     - Recall (also known as sensitivity or true positive rate) measures the proportion of true positives captured among all actual positives. It is important to consider when false negatives have severe consequences, such as identifying diseases or detecting fraudulent transactions.\n",
        "\n",
        "   - What is the F1-score, and why is it useful?\n",
        "     - The F1-score is the harmonic mean of precision and recall, providing a balanced measure of the model's performance. It is particularly useful when there is an imbalance between the positive and negative classes in the dataset.\n",
        "\n",
        "4. **Overfitting and Underfitting:**\n",
        "   - What is overfitting, and how does it occur?\n",
        "     - Overfitting happens when a model learns the training data too well, resulting in poor performance on unseen data. It occurs when the model becomes too complex or when the training data is insufficient.\n",
        "\n",
        "   - What is underfitting, and how does it occur?\n",
        "     - Underfitting occurs when a model is too simple or lacks the capacity to capture the underlying patterns in the data. It leads to poor performance both on the training set and unseen data.\n",
        "\n",
        "   - What are some methods to address overfitting and underfitting?\n",
        "     - Regularization techniques (e.g., L1 and L2 regularization), collecting more data, reducing model complexity, feature selection, and early stopping are common approaches to address overfitting and underfitting.\n"
      ],
      "metadata": {
        "id": "gIStUsyy3tX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise"
      ],
      "metadata": {
        "id": "9-Ju8ariFv8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Download the Pima Indian Diabetes dataset from the provided URL.\n",
        "2. Load the dataset into a Pandas DataFrame and examine the data.\n",
        "3. Split the dataset into features (X) and the target variable (y), where 'Outcome' will be our target variable.\n",
        "\n",
        "Step 2: Model Building and Evaluation\n",
        "1. Split the data into training and testing sets using a 70-30 split.\n",
        "2. Implement the following classifiers from scikit-learn:\n",
        "   - Logistic Regression\n",
        "   - Decision Tree\n",
        "   - Random Forest\n",
        "3. Train each classifier on the training data and make predictions on the testing data.\n",
        "4. Evaluate the performance of each classifier using the following metrics:\n",
        "   - Accuracy\n",
        "   - Precision\n",
        "   - Recall\n",
        "   - F1-score\n",
        "   - Confusion Matrix\n",
        "\n",
        "Step 3: Cross-Validation\n",
        "1. Perform 5-fold cross-validation on each classifier and compute the mean and standard deviation of the accuracy.\n"
      ],
      "metadata": {
        "id": "OYVW-6o4GRPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Step 1: Data Preparation\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
        "columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "data = pd.read_csv(url, header=None, names=columns)\n",
        "X = data.drop('Outcome', axis=1)\n",
        "y = data['Outcome']\n",
        "\n",
        "# Step 2: Model Building and Evaluation\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "classifiers = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier()\n",
        "}\n",
        "\n",
        "for clf_name, clf in classifiers.items():\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print(f\"{clf_name} Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print()\n",
        "\n",
        "# Step 3: Cross-Validation\n",
        "for clf_name, clf in classifiers.items():\n",
        "    scores = cross_val_score(clf, X, y, cv=5)\n",
        "    print(f\"{clf_name} Cross-Validation Accuracy: {scores.mean():.4f} (Std: {scores.std():.4f})\")\n"
      ],
      "metadata": {
        "id": "XdhjYs3NGAm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A quiz on Model Evaluation and Validation"
      ],
      "metadata": {
        "id": "Of2BCPOJuxQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. What is the purpose of model evaluation in machine learning?\n",
        "   <br>a) To optimize the training time\n",
        "   <br>b) To measure the model's performance on unseen data\n",
        "   <br>c) To reduce the complexity of the model\n",
        "   <br>d) To increase the number of features in the model\n",
        "\n",
        "2. Which of the following metrics is commonly used for evaluating classification models?\n",
        "   <br>a) R-squared\n",
        "   <br>b) Mean Absolute Error (MAE)\n",
        "   <br>c) Area Under the Receiver Operating Characteristic (ROC-AUC)\n",
        "   <br>d) Mean Squared Error (MSE)\n",
        "\n",
        "3. True or False: In regression, the closer the R-squared value is to 1, the better the model's fit to the data.\n",
        "\n",
        "4. What is overfitting in the context of machine learning?\n",
        "   <br>a) When the model performs poorly on the training data\n",
        "   <br>b) When the model generalizes well to new, unseen data\n",
        "   <br>c) When the model performs exceptionally well on the training data but poorly on new data\n",
        "   <br>d) When the model has too few features\n",
        "\n",
        "5. How can you mitigate overfitting in a machine learning model?\n",
        "   <br>a) Adding more complex features to the model\n",
        "   <br>b) Increasing the number of training samples\n",
        "   <br>c) Decreasing the regularization strength\n",
        "   <br>d) Ignoring the validation data\n",
        "---\n",
        "**Answers**\n",
        "\n",
        "1. b) To measure the model's performance on unseen data\n",
        "\n",
        "2. c) Area Under the Receiver Operating Characteristic (ROC-AUC)\n",
        "\n",
        "3. True\n",
        "\n",
        "4. c) When the model performs exceptionally well on the training data but poorly on new data\n",
        "\n",
        "5. c) Decreasing the regularization strength\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "UNJr2vy8u23j"
      }
    }
  ]
}