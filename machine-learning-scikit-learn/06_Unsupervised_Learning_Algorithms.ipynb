{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMW2YDTJNB2327HaAY7hmBX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/data-science-programming/blob/main/machine-learning-scikit-learn/06_Unsupervised_Learning_Algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Learning Algorithms\n"
      ],
      "metadata": {
        "id": "R1TFyqhv1JM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview"
      ],
      "metadata": {
        "id": "YIrvHqrYy2QD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsupervised learning is a category of machine learning algorithms where the model is trained on a dataset without explicit supervision or labeled output. In other words, the algorithm is left to find patterns, structures, or relationships within the data on its own. Unlike supervised learning, where the algorithm is guided by labeled examples to make predictions, unsupervised learning deals with raw, unlabeled data.\n",
        "\n",
        "One of the most popular unsupervised learning algorithms in Python is K-means clustering. K-means is a partition-based clustering algorithm that aims to divide the data into K clusters, where each data point belongs to the cluster with the nearest mean. The algorithm iteratively assigns data points to the nearest cluster centroid and updates the centroids until convergence. K-means is widely used for various applications, such as customer segmentation, image compression, and anomaly detection.\n",
        "\n",
        "Another essential unsupervised learning technique is hierarchical clustering. Hierarchical clustering builds a tree-like structure called a dendrogram to represent data points' hierarchy. It recursively merges data points or clusters based on their similarity until all data points belong to a single cluster. The hierarchical nature of this algorithm allows users to visualize the data's natural grouping and determine the number of clusters based on the dendrogram's structure.\n",
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in unsupervised learning. PCA aims to transform the original high-dimensional data into a lower-dimensional space while preserving the most important information. It identifies the principal components, which are orthogonal vectors representing the directions of maximum variance in the data. By projecting the data onto a reduced set of principal components, PCA can simplify the data representation, making it easier to analyze and visualize complex datasets.\n",
        "\n",
        "Anomaly detection is another crucial aspect of unsupervised learning, where the goal is to identify rare and abnormal data points or events in a dataset. Various unsupervised anomaly detection algorithms exist, such as K-nearest neighbors (KNN) and clustering-based approaches. In KNN-based anomaly detection, data points are identified as anomalies based on their distance to their K-nearest neighbors. In clustering-based approaches, anomalies are detected by considering data points that do not fit well into any cluster or have significantly different properties compared to other clusters.\n",
        "\n",
        "Python provides a rich ecosystem of libraries for implementing unsupervised learning algorithms. The scikit-learn library is particularly popular and offers a wide range of tools and functions for K-means clustering, hierarchical clustering, PCA, and various anomaly detection techniques. By leveraging these unsupervised learning algorithms, data scientists and researchers can gain valuable insights from their data, discover patterns, and make data-driven decisions without the need for labeled data."
      ],
      "metadata": {
        "id": "Ouoa7ZFry4Xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-means clustering\n"
      ],
      "metadata": {
        "id": "UdltRsOhQu8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "K-means clustering is a popular unsupervised machine learning algorithm used to partition data into distinct groups or clusters based on their similarities. Scikit-Learn is a powerful machine learning library in Python that provides an implementation of the K-means clustering algorithm.\n",
        "\n",
        "Here's an example of how to perform K-means clustering on the Pima Indian Diabetes dataset using Scikit-Learn:\n"
      ],
      "metadata": {
        "id": "YD7EU_0V1NPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Prepare the data for clustering\n",
        "X = dataset.drop('Outcome', axis=1)  # Features\n",
        "y = dataset['Outcome']  # Target variable\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Perform K-means clustering\n",
        "k = 2  # Number of clusters\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "# Get the cluster labels for each data point\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Visualize the clusters (using the first two features)\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap='viridis')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('K-means Clustering')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UCP3PjU-1RIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we start by loading the Pima Indian Diabetes dataset using Pandas. We then prepare the data for clustering by separating the features (X) and the target variable (y).\n",
        "\n",
        "Next, we standardize the features using the `StandardScaler` from Scikit-Learn to ensure that all features are on the same scale. Standardizing the features is important for K-means clustering since it is based on the Euclidean distance between data points.\n",
        "\n",
        "Afterward, we create an instance of the `KMeans` class with the desired number of clusters (k=2 in this case) and fit it to the standardized data. The `fit()` method performs the actual clustering.\n",
        "\n",
        "We then obtain the cluster labels for each data point using the `labels_` attribute of the fitted KMeans object.\n",
        "\n",
        "Finally, we visualize the clusters by plotting the first two features of the standardized data and coloring the points based on their assigned cluster labels.\n",
        "\n",
        "Note that in this example, we are performing K-means clustering for illustrative purposes, but the Pima Indian Diabetes dataset is not particularly suitable for clustering as it is typically used for binary classification tasks.\n"
      ],
      "metadata": {
        "id": "UOhvxn8X1ZSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hierarchical clustering\n"
      ],
      "metadata": {
        "id": "SxzCAvfgQxM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In Scikit-Learn, hierarchical clustering can be performed using the AgglomerativeClustering class from the cluster module. Hierarchical clustering is a method of clustering data points based on their similarity and forms a hierarchy of clusters.\n",
        "\n",
        "Here's an example of performing hierarchical clustering on the Pima Indian Diabetes dataset:\n"
      ],
      "metadata": {
        "id": "xGVhnPOt1eHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Select the features for clustering\n",
        "features = dataset[['Glucose', 'BMI']]\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Perform hierarchical clustering\n",
        "cluster = AgglomerativeClustering(n_clusters=2)\n",
        "cluster_labels = cluster.fit_predict(features_scaled)\n",
        "\n",
        "# Plot the clusters\n",
        "plt.scatter(features_scaled[:, 0], features_scaled[:, 1], c=cluster_labels, cmap='viridis')\n",
        "plt.xlabel('Glucose')\n",
        "plt.ylabel('BMI')\n",
        "plt.title('Hierarchical Clustering on Pima Indian Diabetes Dataset')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BG_jkJlH1iGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we load the Pima Indian Diabetes dataset using Pandas library. We select the 'Glucose' and 'BMI' features for clustering. Before clustering, we standardize the features using the StandardScaler from Scikit-Learn to ensure that they have a similar scale.\n",
        "\n",
        "Next, we create an instance of the AgglomerativeClustering class with the parameter `n_clusters=2` to specify that we want to form 2 clusters. We then fit and predict the cluster labels using the `fit_predict` method.\n",
        "\n",
        "Finally, we plot the clusters using a scatter plot, where the x-axis represents the standardized 'Glucose' values and the y-axis represents the standardized 'BMI' values. Each point is colored according to its assigned cluster label.\n"
      ],
      "metadata": {
        "id": "fVtotx401tWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principal Component Analysis (PCA)\n"
      ],
      "metadata": {
        "id": "MCrHgwYOQzW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique used to identify patterns and reduce the number of variables in a dataset while preserving the most important information. It transforms the original variables into a new set of uncorrelated variables called principal components.\n",
        "\n",
        "Scikit-Learn is a popular machine learning library in Python that provides a straightforward implementation of PCA through its `PCA` class. Here's an example of how to use PCA with the Pima Indian Diabetes dataset:\n"
      ],
      "metadata": {
        "id": "U2yEmrZb1022"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Separate the features and target variable\n",
        "X = dataset.drop('Outcome', axis=1)\n",
        "y = dataset['Outcome']\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Create a new DataFrame with the principal components\n",
        "principal_df = pd.DataFrame(data=X_pca, columns=['Principal Component 1', 'Principal Component 2'])\n",
        "principal_df['Outcome'] = y\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(principal_df.head())\n"
      ],
      "metadata": {
        "id": "hgFbmNa516oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first load the Pima Indian Diabetes dataset using Pandas. Then, we separate the features (`X`) and the target variable (`y`).\n",
        "\n",
        "Next, we standardize the features using `StandardScaler` to ensure that all variables have a mean of 0 and a standard deviation of 1. Standardization is important before applying PCA to avoid variables with larger scales dominating the principal components.\n",
        "\n",
        "We then create an instance of the `PCA` class and specify the number of components we want to retain (in this case, 2). The `fit_transform` method is used to apply PCA and obtain the transformed principal components (`X_pca`).\n",
        "\n",
        "Finally, we create a new DataFrame (`principal_df`) with the principal components and the original target variable. We print the first few rows of the resulting DataFrame to see the output.\n",
        "\n",
        "Note: It's common to visualize the results of PCA, but the example above focuses on the implementation. You can explore further by visualizing the principal components using scatter plots or other techniques.\n"
      ],
      "metadata": {
        "id": "RYNr-rWJ1-z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Anomaly detection\n"
      ],
      "metadata": {
        "id": "mx9xDLrHQ1Yd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Scikit-Learn is a popular machine learning library in Python that provides various algorithms and tools for anomaly detection. One commonly used algorithm for anomaly detection in Scikit-Learn is the Isolation Forest algorithm. It is an efficient and effective algorithm for identifying anomalies in datasets.\n",
        "\n",
        "Here's an example of anomaly detection using the Isolation Forest algorithm with the Pima Indian Diabetes dataset:\n"
      ],
      "metadata": {
        "id": "u4qnVQyI2B_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Prepare the data\n",
        "X = dataset.drop('Outcome', axis=1)\n",
        "\n",
        "# Train the Isolation Forest model\n",
        "model = IsolationForest(contamination=0.1)  # Contamination represents the expected proportion of anomalies in the data\n",
        "model.fit(X)\n",
        "\n",
        "# Predict anomalies\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Add the anomaly predictions to the dataset\n",
        "dataset['Anomaly'] = predictions\n",
        "\n",
        "# Print the dataset with anomaly predictions\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "id": "4ypFsn1O2FkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we load the Pima Indian Diabetes dataset using Pandas library. We then prepare the data by dropping the 'Outcome' column, as it represents the class labels and we want to perform unsupervised anomaly detection. Next, we train the Isolation Forest model by initializing an instance of the `IsolationForest` class with a contamination parameter of 0.1, indicating that we expect 10% of the data to be anomalies.\n",
        "\n",
        "After training the model, we use it to predict anomalies in the dataset by calling the `predict()` method on the data. The resulting predictions are assigned to the `predictions` variable. We then add the anomaly predictions as a new column named 'Anomaly' to the dataset.\n",
        "\n",
        "Finally, we print the dataset with the added 'Anomaly' column to see the output, where the values of -1 indicate anomalies and 1 indicate normal data points.\n"
      ],
      "metadata": {
        "id": "CnMQHlLR2JOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reflection points"
      ],
      "metadata": {
        "id": "94iCdKUz2MNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **K-means Clustering**:\n",
        "   - Explain the concept of K-means clustering and its primary objective.\n",
        "     - Sample answer: K-means clustering is an unsupervised machine learning technique used to partition a dataset into K clusters, where each data point belongs to the cluster with the nearest mean. The objective is to minimize the within-cluster sum of squares, effectively grouping similar data points together.\n",
        "   - Discuss the role of the K parameter in K-means clustering.\n",
        "     - Sample answer: The K parameter represents the number of clusters to create. It determines the granularity of the clustering. Choosing the optimal value of K is important and typically requires evaluating various metrics, such as the elbow method or silhouette score, to find the best balance between complexity and performance.\n",
        "   - Explain the steps involved in the K-means clustering algorithm.\n",
        "     - Sample answer: The steps in the K-means clustering algorithm include initializing K centroids, assigning data points to the nearest centroid, updating the centroids based on the assigned data points, and repeating the assignment and update steps until convergence is achieved or a predefined stopping criterion is met.\n",
        "\n",
        "2. **Hierarchical Clustering**:\n",
        "   - Define hierarchical clustering and its key characteristics.\n",
        "     - Sample answer: Hierarchical clustering is an unsupervised clustering algorithm that builds a hierarchy of clusters. It does not require a pre-defined number of clusters. Instead, it generates a tree-like structure called a dendrogram, which represents the merging and splitting of clusters based on the similarity of data points.\n",
        "   - Discuss the differences between agglomerative and divisive hierarchical clustering.\n",
        "     - Sample answer: Agglomerative hierarchical clustering starts with each data point as a separate cluster and progressively merges similar clusters until a single cluster remains. Divisive hierarchical clustering starts with all data points in a single cluster and recursively splits it into smaller clusters until each data point forms its own cluster.\n",
        "   - Explain the linkage criteria used in hierarchical clustering.\n",
        "     - Sample answer: Linkage criteria determine the distance between clusters and guide the merging process. Common linkage methods include single-linkage (minimum distance), complete-linkage (maximum distance), and average-linkage (average distance).\n",
        "\n",
        "3. **Principal Component Analysis (PCA)**:\n",
        "   - Describe the purpose and benefits of PCA.\n",
        "     - Sample answer: Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while retaining most of the relevant information. It helps uncover the underlying structure and patterns in the data, simplifies data visualization, and can improve model performance by reducing noise and multicollinearity.\n",
        "   - Explain the concept of principal components and their interpretation.\n",
        "     - Sample answer: Principal components are new variables obtained by linearly combining the original variables in a way that captures the maximum variance in the data. Each principal component is orthogonal to the others and represents a unique direction in the data space. The first principal component explains the most significant variation, followed by subsequent components in decreasing order of importance.\n",
        "   - Discuss the steps involved in performing PCA.\n",
        "     - Sample answer: The steps in PCA include standardizing the data, computing the covariance or correlation matrix, calculating the eigenvectors and eigenvalues of the covariance matrix, selecting the desired number of principal components based on explained variance or a cumulative variance threshold, and transforming the data into the new lower-dimensional space.\n",
        "\n",
        "4. **Anomaly Detection**:\n",
        "   - Define anomaly detection and its applications.\n",
        "     - Sample answer: Anomaly detection is the identification of rare or abnormal instances that deviate from the expected behavior within a dataset. It finds applications in fraud detection, network intrusion detection, equipment failure prediction, and outlier detection in various domains.\n",
        "   - Discuss the types of anomalies and their detection techniques.\n",
        "     - Sample answer: Anomalies can be classified as point anomalies (individual instances), contextual anomalies (anomalies within a specific context), or collective anomalies (anomalies that occur as a group). Detection techniques include statistical methods (e.g., z-score, Mahalanobis distance), clustering-based approaches, and machine learning algorithms (e.g., isolation forest, one-class SVM).\n",
        "   - Explain the evaluation metrics for anomaly detection.\n",
        "     - Sample answer: Evaluation metrics for anomaly detection include precision, recall, F1-score, and the area under the receiver operating characteristic curve (AUC-ROC). The choice of the appropriate metric depends on the specific problem and the desired balance between false positives and false negatives.\n"
      ],
      "metadata": {
        "id": "-Vh7Cn9k2R1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise"
      ],
      "metadata": {
        "id": "7Bp5YJ7MFReT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Load the dataset from the given URL.\n",
        "2. Preprocess the data by handling any missing values or irrelevant columns.\n",
        "3. Standardize the data so that all features have a mean of 0 and a standard deviation of 1.\n",
        "4. Perform clustering using two different algorithms: K-Means and Agglomerative Clustering.\n",
        "5. Visualize the clusters in a 2D plot.\n",
        "6. Analyze the results and draw insights from the clustering.\n"
      ],
      "metadata": {
        "id": "hfoav2rhFgw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample Solution"
      ],
      "metadata": {
        "id": "DZxDtydJFm0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = ['pregnancies', 'glucose', 'blood_pressure', 'skin_thickness', 'insulin', 'bmi', 'diabetes_pedigree', 'age', 'diabetes']\n",
        "data = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Preprocess the data\n",
        "# (If there are any specific preprocessing steps needed, you can add them here.)\n",
        "\n",
        "# Extract features and target\n",
        "X = data.drop('diabetes', axis=1)\n",
        "y = data['diabetes']\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_std = scaler.fit_transform(X)\n",
        "\n",
        "# Perform clustering with K-Means algorithm\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X_std)\n",
        "\n",
        "# Perform clustering with Agglomerative Clustering algorithm\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=2)\n",
        "y_agg_clustering = agg_clustering.fit_predict(X_std)\n",
        "\n",
        "# Reduce dimensionality for 2D visualization using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_std)\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# K-Means clusters\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_pca[y_kmeans == 0, 0], X_pca[y_kmeans == 0, 1], c='blue', label='Cluster 1')\n",
        "plt.scatter(X_pca[y_kmeans == 1, 0], X_pca[y_kmeans == 1, 1], c='red', label='Cluster 2')\n",
        "plt.title('K-Means Clustering')\n",
        "plt.legend()\n",
        "\n",
        "# Agglomerative Clustering clusters\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_pca[y_agg_clustering == 0, 0], X_pca[y_agg_clustering == 0, 1], c='green', label='Cluster 1')\n",
        "plt.scatter(X_pca[y_agg_clustering == 1, 0], X_pca[y_agg_clustering == 1, 1], c='purple', label='Cluster 2')\n",
        "plt.title('Agglomerative Clustering')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Analyze the results and draw insights\n",
        "# (You can provide your insights here based on the visualization and clustering results.)\n"
      ],
      "metadata": {
        "id": "EBg31h2cFXUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A quiz on Unsupervised Learning Algorithms"
      ],
      "metadata": {
        "id": "6eWfU-KhtPsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Quiz on Clustering and Dimensionality Reduction Techniques in Python**\n",
        "\n",
        "**1. K-means clustering is a method used for:**\n",
        "<br>a) Classification\n",
        "<br>b) Regression\n",
        "<br>c) Clustering\n",
        "<br>d) Dimensionality reduction\n",
        "\n",
        "**2. In K-means clustering, what is the objective of the algorithm?**\n",
        "<br>a) Minimize the variance within each cluster\n",
        "<br>b) Maximize the variance within each cluster\n",
        "<br>c) Minimize the distance between clusters\n",
        "<br>d) Maximize the distance between clusters\n",
        "\n",
        "**3. The number of clusters in K-means is determined by:**\n",
        "<br>a) The user-specified value\n",
        "<br>b) Random initialization\n",
        "<br>c) The algorithm itself\n",
        "<br>d) None of the above\n",
        "\n",
        "**4. Hierarchical clustering can be classified into two main types, which are:**\n",
        "<br>a) Single-linkage and complete-linkage\n",
        "<br>b) K-means and Ward's linkage\n",
        "<br>c) Mean-shift and DBSCAN\n",
        "<br>d) Linear regression and logistic regression\n",
        "\n",
        "**5. In hierarchical clustering, the dendrogram is used to:**\n",
        "<br>a) Visualize the data points in clusters\n",
        "<br>b) Determine the number of clusters\n",
        "<br>c) Evaluate the quality of clustering\n",
        "<br>d) Store the clusters\n",
        "\n",
        "**6. Principal Component Analysis (PCA) is used for:**\n",
        "<br>a) Clustering similar data points together\n",
        "<br>b) Reducing the number of dimensions in the data\n",
        "<br>c) Adding more features to the dataset\n",
        "<br>d) Performing regression tasks\n",
        "\n",
        "**7. In PCA, the principal components are orthogonal to each other, meaning:**\n",
        "<br>a) They have equal magnitudes\n",
        "<br>b) They are parallel to each other\n",
        "<br>c) They are perpendicular to each other\n",
        "<br>d) They are not correlated\n",
        "\n",
        "**8. Anomaly detection is used for:**\n",
        "<br>a) Identifying outliers in the data\n",
        "<br>b) Creating clusters of similar data points\n",
        "<br>c) Dimensionality reduction\n",
        "<br>d) Fitting a line to the data\n",
        "\n",
        "**9. Which of the following is a commonly used unsupervised anomaly detection algorithm?**\n",
        "<br>a) K-nearest neighbors (KNN)\n",
        "<br>b) Decision trees\n",
        "<br>c) Support Vector Machines (SVM)\n",
        "<br>d) Linear regression\n",
        "\n",
        "**10. In Python, which library can be used for various clustering and anomaly detection techniques?**\n",
        "<br>a) Matplotlib\n",
        "<br>b) Pandas\n",
        "<br>c) Scikit-learn\n",
        "<br>d) NumPy\n",
        "\n",
        "---\n",
        "**Answers:**\n",
        "1. c) Clustering\n",
        "2. a) Minimize the variance within each cluster\n",
        "3. a) The user-specified value\n",
        "4. a) Single-linkage and complete-linkage\n",
        "5. b) Determine the number of clusters\n",
        "6. b) Reducing the number of dimensions in the data\n",
        "7. c) They are perpendicular to each other\n",
        "8. a) Identifying outliers in the data\n",
        "9. a) K-nearest neighbors (KNN)\n",
        "10. c) Scikit-learn\n",
        "---"
      ],
      "metadata": {
        "id": "yZw7UGKZtRyI"
      }
    }
  ]
}