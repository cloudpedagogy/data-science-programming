{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMlB8ViYNsmnArm7ceOxTRx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/data-science-programming/blob/main/machine-learning-scikit-learn/04_Exploratory_Data_Analysis_(EDA).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (EDA)\n"
      ],
      "metadata": {
        "id": "zCTBVBcwxmkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview"
      ],
      "metadata": {
        "id": "eeN3m2i6xq_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Exploratory Data Analysis (EDA) is a crucial step in the data analysis process that empowers data scientists and analysts to gain valuable insights, understand the underlying patterns, and detect potential anomalies within a dataset. It serves as the foundation for making informed decisions and forming hypotheses before diving into more complex modeling and statistical techniques. EDA is an essential practice in the data science workflow and is typically performed as a preliminary step before any formal modeling or hypothesis testing.\n",
        "\n",
        "In Python, EDA can be efficiently performed using a wide range of libraries and tools, making it a popular choice among data professionals. Some of the widely used Python libraries for EDA include Pandas, NumPy, Matplotlib, Seaborn, and Plotly. These libraries offer robust data manipulation, visualization, and statistical capabilities, enabling data scientists to explore and interact with their data effectively.\n",
        "\n",
        "During the EDA process, data is thoroughly examined to understand its structure, distribution, and relationships between variables. Data cleaning and preprocessing steps are also taken to handle missing values, outliers, and inconsistencies that might affect the analysis. Exploring summary statistics, histograms, scatter plots, and correlation matrices are common techniques employed during this phase.\n",
        "\n",
        "EDA plays a crucial role in identifying potential challenges and limitations in the dataset. It helps in uncovering data quality issues and can often reveal the need for further data collection or refinement. By visualizing and summarizing the data, analysts can also generate hypotheses about the relationships between variables, which can later be tested through more advanced statistical methods or machine learning algorithms.\n",
        "\n",
        "Moreover, EDA facilitates the identification of patterns and trends that might lead to actionable insights and provide a deeper understanding of the data's characteristics. It also helps in selecting appropriate features for predictive models and deciding on the most suitable data transformation techniques.\n",
        "\n"
      ],
      "metadata": {
        "id": "-8uklZ8EyYr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Select a subset of features for descriptive statistics\n",
        "features = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]\n",
        "\n",
        "# Standardize the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "dataset_scaled = scaler.fit_transform(dataset[features])\n",
        "\n",
        "# Calculate the descriptive statistics\n",
        "mean_values = dataset_scaled.mean(axis=0)\n",
        "std_values = dataset_scaled.std(axis=0)\n",
        "min_values = dataset_scaled.min(axis=0)\n",
        "max_values = dataset_scaled.max(axis=0)\n",
        "median_values = np.median(dataset_scaled, axis=0)\n",
        "quartiles = np.percentile(dataset_scaled, [25, 50, 75], axis=0)\n",
        "\n",
        "# Print the descriptive statistics\n",
        "print(\"Mean values:\", mean_values)\n",
        "print(\"Standard deviation values:\", std_values)\n",
        "print(\"Minimum values:\", min_values)\n",
        "print(\"Maximum values:\", max_values)\n",
        "print(\"Median values:\", median_values)\n",
        "print(\"Quartiles:\", quartiles)\n"
      ],
      "metadata": {
        "id": "YhGrFpnKyBq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first load the Pima Indian Diabetes dataset using Pandas library. We then select a subset of features on which we want to calculate the descriptive statistics. To ensure meaningful comparisons across different features, we standardize the features using `StandardScaler` from scikit-learn. We fit the scaler on the selected features and transform the dataset to obtain the standardized values in `dataset_scaled`.\n",
        "\n",
        "After that, we use NumPy functions to calculate the descriptive statistics. The `mean` function calculates the mean values along the 0th axis (column-wise), `std` function calculates the standard deviation, `min` and `max` functions find the minimum and maximum values, `median` function calculates the median values, and `percentile` calculates the quartiles (25th, 50th, and 75th percentiles).\n",
        "\n",
        "Finally, we print the calculated descriptive statistics for each feature.\n"
      ],
      "metadata": {
        "id": "-s2nRbCByFRB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Visualization\n",
        "\n",
        "Scikit-Learn (sklearn) is primarily a machine learning library in Python, and it doesn't provide built-in data visualization capabilities. However, it does offer various preprocessing and modeling techniques. To perform data visualization, you can use other libraries such as Matplotlib or Seaborn in conjunction with Scikit-Learn. Let's look at an example using the Pima Indian Diabetes dataset:\n"
      ],
      "metadata": {
        "id": "KmNXDnuJySl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Plot a histogram of the Glucose levels\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(data=dataset, x='Glucose', kde=True, hue='Outcome')\n",
        "plt.title(\"Histogram of Glucose levels\")\n",
        "plt.xlabel(\"Glucose\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# Plot a correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "correlation_matrix = dataset.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"YlGnBu\")\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MqO6MDaQyWTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we load the Pima Indian Diabetes dataset using Pandas library. We then use Matplotlib and Seaborn libraries for data visualization.\n",
        "\n",
        "The first visualization is a histogram of the Glucose levels, where we use `sns.histplot` to create a histogram. We set `kde=True` to overlay a kernel density estimate plot and use the `hue='Outcome'` parameter to differentiate the distribution of Glucose levels for different outcomes (diabetic or non-diabetic).\n",
        "\n",
        "The second visualization is a correlation matrix, where we calculate the correlation between each pair of variables in the dataset using the `corr` method. We use `sns.heatmap` to create a heatmap of the correlation matrix, setting `annot=True` to display the correlation values, and using the colormap \"YlGnBu\" for visual representation.\n",
        "\n",
        "These examples demonstrate how to perform data visualization using Matplotlib and Seaborn alongside Scikit-Learn for data analysis and modeling tasks.\n"
      ],
      "metadata": {
        "id": "EK7qox6lyd5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature selection and dimensionality reduction\n",
        "\n",
        "\n",
        "Feature selection is an important step in machine learning to identify the most relevant features or variables that contribute significantly to the prediction or target variable. Scikit-Learn, a popular machine learning library in Python, provides several techniques for feature selection. Here are a few examples using the Pima Indian Diabetes dataset:\n",
        "\n",
        "1. **Univariate Selection**:\n",
        "Univariate selection evaluates each feature individually to determine its relationship with the target variable. Scikit-Learn provides the `SelectKBest` class along with various scoring functions to select a specific number (K) of features based on their scores. For example, let's use the chi-square test to select the top 5 features from the Pima Indian Diabetes dataset:\n"
      ],
      "metadata": {
        "id": "VNMUPL7IyhoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Separate the features and target variable\n",
        "X = dataset.iloc[:, :-1]\n",
        "y = dataset.iloc[:, -1]\n",
        "\n",
        "# Apply SelectKBest and chi2 scoring to select top 5 features\n",
        "selector = SelectKBest(score_func=chi2, k=5)\n",
        "X_new = selector.fit_transform(X, y)\n",
        "\n",
        "# Get the selected feature indices\n",
        "selected_indices = selector.get_support(indices=True)\n",
        "selected_features = X.columns[selected_indices]\n",
        "\n",
        "# Print the selected features\n",
        "print(\"Selected Features:\")\n",
        "print(selected_features)\n"
      ],
      "metadata": {
        "id": "ArOFGFxIymLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we use the `SelectKBest` class with the `chi2` scoring function to select the top 5 features from the Pima Indian Diabetes dataset. We separate the features (X) and the target variable (y) from the dataset. Then, we apply the `fit_transform()` method to select the top 5 features based on their chi-square scores. Finally, we retrieve the selected feature indices and print the corresponding feature names.\n",
        "\n",
        "2. **Feature Importance with Random Forest**:\n",
        "Random Forest is an ensemble algorithm that can provide a measure of the importance of each feature in predicting the target variable. Scikit-Learn's `RandomForestClassifier` or `RandomForestRegressor` can be used to estimate feature importance. Here's an example using a Random Forest classifier:\n"
      ],
      "metadata": {
        "id": "XDq0yRWTyq--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Separate the features and target variable\n",
        "X = dataset.iloc[:, :-1]\n",
        "y = dataset.iloc[:, -1]\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Fit the classifier to the data\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf.feature_importances_\n",
        "\n",
        "# Create a dataframe with feature names and their importances\n",
        "feature_importances_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
        "\n",
        "# Sort the features by importance in descending order\n",
        "sorted_features = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the sorted features\n",
        "print(\"Sorted Features:\")\n",
        "print(sorted_features)\n"
      ],
      "metadata": {
        "id": "Wcgy_38mywXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we use a Random Forest classifier to estimate feature importance. We separate the features (X) and the target variable (y) from the dataset. Then, we create a `RandomForestClassifier` and fit it to the data. We retrieve the feature importances using the `feature_importances_` attribute. Next, we create a dataframe with feature names and their importances and sort them in descending order. Finally, we print the sorted features based on their importance.\n",
        "\n",
        "These are just a couple of examples of feature selection techniques in Scikit-Learn. The library provides many other methods like Recursive Feature Elimination (RFE), L1-based feature selection, and more. The choice of the technique depends on the specific problem and the characteristics of the dataset.\n"
      ],
      "metadata": {
        "id": "TM6EVslly1FN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality Reduction\n",
        "Dimensionality reduction is a technique used to reduce the number of features (dimensions) in a dataset while retaining the most important information. Scikit-Learn, a popular machine learning library in Python, provides several methods for dimensionality reduction.\n",
        "\n",
        "Two commonly used techniques for dimensionality reduction in Scikit-Learn are Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE). Here's an example of how you can use these techniques with the Pima Indian Diabetes dataset:\n",
        "\n",
        "1. Principal Component Analysis (PCA):\n"
      ],
      "metadata": {
        "id": "ClTdiFjPy5db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = dataset.drop(\"Outcome\", axis=1)\n",
        "y = dataset[\"Outcome\"]\n",
        "\n",
        "# Perform PCA with 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Print the transformed data\n",
        "print(X_pca)\n"
      ],
      "metadata": {
        "id": "Bicjn6wmy_7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first load the Pima Indian Diabetes dataset using Pandas. Then, we separate the features (X) and the target variable (y). We initialize a PCA object and set the number of components to 2. We fit the PCA model to the features data (X) and transform it to obtain the reduced dimensionality representation (X_pca) with 2 components. Finally, we print the transformed data.\n",
        "\n",
        "2. t-Distributed Stochastic Neighbor Embedding (t-SNE):\n"
      ],
      "metadata": {
        "id": "-_arHJOPzDnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = dataset.drop(\"Outcome\", axis=1)\n",
        "y = dataset[\"Outcome\"]\n",
        "\n",
        "# Perform t-SNE with 2 components\n",
        "tsne = TSNE(n_components=2)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# Print the transformed data\n",
        "print(X_tsne)\n"
      ],
      "metadata": {
        "id": "segbS0q9zGR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we again load the Pima Indian Diabetes dataset and separate the features (X) and the target variable (y). We create a t-SNE object and set the number of components to 2. We fit the t-SNE model to the features data (X) and transform it to obtain the reduced dimensionality representation (X_tsne) with 2 components. Finally, we print the transformed data.\n",
        "\n",
        "Note: Both PCA and t-SNE are unsupervised techniques and do not take the target variable into account during the dimensionality reduction process.\n"
      ],
      "metadata": {
        "id": "nQ8Z35JqzN2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reflection points"
      ],
      "metadata": {
        "id": "7q3XuXJazQk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Descriptive Statistics:**\n",
        "- What are the key measures used in descriptive statistics to summarize and describe data?\n",
        "  - Sample answer: Key measures include measures of central tendency (mean, median, mode) and measures of variability (range, variance, standard deviation).\n",
        "\n",
        "- How can descriptive statistics be used to gain insights into a dataset?\n",
        "  - Sample answer: Descriptive statistics help in understanding the distribution, spread, and central values of data, enabling us to identify patterns, outliers, and key characteristics.\n",
        "\n",
        "- What Python libraries can be used to perform descriptive statistics?\n",
        "  - Sample answer: Python libraries like NumPy, Pandas, and SciPy provide functions and methods to compute descriptive statistics on datasets.\n",
        "\n",
        "**2. Data Visualization:**\n",
        "- Why is data visualization important in data analysis?\n",
        "  - Sample answer: Data visualization helps in effectively communicating patterns, trends, and insights hidden within the data. It aids in understanding complex information and making informed decisions.\n",
        "\n",
        "- What are some popular Python libraries for data visualization?\n",
        "  - Sample answer: Matplotlib, Seaborn, and Plotly are commonly used Python libraries for creating various types of visualizations, including bar plots, scatter plots, histograms, and more.\n",
        "\n",
        "- How can data visualization techniques be used to explore and present data effectively?\n",
        "  - Sample answer: Data visualization techniques help in understanding relationships between variables, identifying outliers, detecting trends, and conveying findings in a visually appealing manner.\n",
        "\n",
        "**3. Feature Selection:**\n",
        "- Why is feature selection important in machine learning and data analysis?\n",
        "  - Sample answer: Feature selection helps in improving model performance, reducing complexity, and enhancing interpretability by selecting the most relevant and informative features for a given task.\n",
        "\n",
        "- What are some commonly used feature selection techniques?\n",
        "  - Sample answer: Techniques like filter methods (e.g., correlation, information gain), wrapper methods (e.g., recursive feature elimination), and embedded methods (e.g., LASSO regression) are commonly used for feature selection.\n",
        "\n",
        "- How can feature selection be implemented using Python?\n",
        "  - Sample answer: Python libraries like Scikit-learn provide functions and classes for feature selection, including methods like SelectKBest, SelectFromModel, and Recursive Feature Elimination.\n",
        "\n",
        "**4. Dimensionality Reduction:**\n",
        "- What is the purpose of dimensionality reduction in data analysis?\n",
        "  - Sample answer: Dimensionality reduction aims to reduce the number of features while preserving important information, thereby addressing the curse of dimensionality, improving efficiency, and aiding visualization.\n",
        "\n",
        "- What are the main techniques for dimensionality reduction?\n",
        "  - Sample answer: Principal Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE) are widely used techniques for dimensionality reduction.\n",
        "\n",
        "- How can dimensionality reduction be implemented in Python?\n",
        "  - Sample answer: Python libraries like Scikit-learn provide classes for dimensionality reduction, including PCA and t-SNE, which can be easily applied to datasets.\n"
      ],
      "metadata": {
        "id": "FtRU3MIyzWZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A quiz on Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "CsblISvnp_QE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Which type of data visualization is best suited for showing the distribution of a single continuous variable?\n",
        "   <br>a) Scatter plot\n",
        "   <br>b) Bar chart\n",
        "   <br>c) Line plot\n",
        "   <br>d) Histogram\n",
        "\n",
        "2. Which data visualization technique is used to show the relationship between two continuous variables?\n",
        "   <br>a) Bar chart\n",
        "   <br>b) Scatter plot\n",
        "   <br>c) Pie chart\n",
        "   <br>d) Box plot\n",
        "\n",
        "**Feature Selection and Dimensionality Reduction:**\n",
        "\n",
        "3. What is the main purpose of feature selection in machine learning?\n",
        "   <br>a) To increase the number of features in the dataset\n",
        "   <br>b) To reduce the number of features in the dataset\n",
        "   <br>c) To transform features into a higher-dimensional space\n",
        "   <br>d) To remove outliers from the dataset\n",
        "\n",
        "4. Principal Component Analysis (PCA) is used for:\n",
        "   <br>a) Feature scaling\n",
        "   <br>b) Dimensionality reduction\n",
        "   <br>c) Feature extraction\n",
        "   <br>d) Outlier detection\n",
        "\n",
        "**Dimensionality Reduction in Python:**\n",
        "\n",
        "5. Which Python library provides functions for Principal Component Analysis (PCA) and other dimensionality reduction techniques?\n",
        "   <br>a) Scikit-learn\n",
        "   <br>b) TensorFlow\n",
        "   <br>c) PyTorch\n",
        "   <br>d) Pandas\n",
        "\n",
        "6. What does the explained variance in PCA represent?\n",
        "   <br>a) The percentage of the original information retained in the data after dimensionality reduction\n",
        "   <br>b) The number of features in the original dataset\n",
        "   <br>c) The ratio of the training set size to the testing set size\n",
        "   <br>d) The number of iterations required for PCA convergence\n",
        "---\n",
        "**Answers:**\n",
        "\n",
        "1. d) Histogram\n",
        "2. b) Scatter plot\n",
        "3. b) To reduce the number of features in the dataset\n",
        "4. b) Dimensionality reduction\n",
        "5. a) Scikit-learn\n",
        "6. a) The percentage of the original information retained in the data after dimensionality reduction\n",
        "---"
      ],
      "metadata": {
        "id": "oPpXqwQhqDO5"
      }
    }
  ]
}