{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPptAguDgkOcORa3YW1RBwH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/data-science-programming/blob/main/machine-learning-scikit-learn/05_Supervised_Learning_Algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Learning Algorithms\n"
      ],
      "metadata": {
        "id": "KIqy39OQzqGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview"
      ],
      "metadata": {
        "id": "iSqOQYKA36U1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction to Supervised Learning Algorithms**\n",
        "\n",
        "Supervised learning is one of the fundamental branches of machine learning, focusing on the task of training algorithms to learn patterns and relationships between input data and their corresponding output labels. In this paradigm, the learning process is \"supervised\" because the algorithm is provided with a labeled dataset during training. The labels act as ground truth, guiding the algorithm to make accurate predictions on unseen data.\n",
        "\n",
        "The goal of supervised learning is to create a mapping function that can generalize well to new, previously unseen examples. This function, also known as a model, is capable of capturing the underlying patterns and correlations within the data, allowing it to predict the correct output for new input instances.\n",
        "\n",
        "The process of supervised learning involves two main phases:\n",
        "\n",
        "1. **Training Phase**: During the training phase, the algorithm is exposed to a labeled dataset consisting of input-output pairs. The algorithm uses this data to adjust its internal parameters or model structure, learning from the relationships present in the training examples. The quality and representativeness of the training data significantly impact the model's ability to generalize to new, unseen data.\n",
        "\n",
        "2. **Inference Phase**: Once the model is trained, it can be used to make predictions on new, unlabeled data. The model takes the input data and applies the learned relationships to generate the corresponding output or prediction. The model's performance is evaluated based on its ability to accurately predict the correct labels for the new data.\n",
        "\n",
        "There are various types of supervised learning algorithms, each designed to address different types of problems. Some common types of supervised learning algorithms include:\n",
        "\n",
        "- **Regression Algorithms**: Regression algorithms are used for predicting continuous numeric values. They establish a relationship between the input features and the target variable, enabling the algorithm to make precise predictions within a numerical range.\n",
        "\n",
        "- **Classification Algorithms**: Classification algorithms are employed when the task involves predicting discrete class labels or categories. These algorithms learn to classify data into predefined classes based on patterns observed in the training data.\n",
        "\n"
      ],
      "metadata": {
        "id": "Txz07Su137zu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression analysis"
      ],
      "metadata": {
        "id": "rLu5Y_th5Mch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview"
      ],
      "metadata": {
        "id": "woIHfQF75RiJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction to Regression Analysis**\n",
        "\n",
        "Regression analysis is a fundamental statistical technique used to model and examine the relationships between variables. It is widely employed in various fields, including economics, finance, social sciences, engineering, and data science. The primary goal of regression analysis is to understand how one or more independent variables (predictors) influence a dependent variable (response) and make predictions based on this relationship.\n",
        "\n",
        "In simple terms, regression analysis allows us to identify and quantify the association between variables, enabling us to predict or estimate the value of the dependent variable based on the values of one or more independent variables. This predictive ability makes regression analysis a powerful tool for making data-driven decisions and understanding the underlying patterns within datasets.\n",
        "\n",
        "The most common form of regression analysis is linear regression, where the relationship between the dependent variable and the independent variable(s) is modeled as a straight line. However, regression analysis is not limited to linear relationships; it can handle more complex relationships by using techniques such as polynomial regression, multiple regression, and nonlinear regression.\n",
        "\n",
        "Regression analysis involves two key components:\n",
        "\n",
        "1. **Dependent Variable (Response)**: This is the variable we are trying to predict or explain based on the independent variables. It is also referred to as the target variable.\n",
        "\n",
        "2. **Independent Variable(s) (Predictor)**: These are the variables used to predict or explain the variation in the dependent variable. They are also known as explanatory or input variables.\n",
        "\n",
        "During regression analysis, the model fits a line or curve that best represents the relationship between the independent and dependent variables. The process involves finding the optimal parameters (coefficients) for the model, which minimize the difference between the predicted values and the actual values of the dependent variable.\n",
        "\n",
        "Before performing regression analysis, it is essential to explore the data, check for assumptions, and preprocess the variables to ensure accurate and meaningful results. Additionally, various evaluation metrics are used to assess the performance of the regression model and validate its predictive capabilities.\n",
        "\n"
      ],
      "metadata": {
        "id": "MEeMPu6P5S69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear regression\n",
        "\n",
        "Linear regression is a widely used statistical technique for modeling the relationship between a dependent variable and one or more independent variables. In Python, the Scikit-Learn library provides a powerful implementation of linear regression.\n",
        "\n",
        "Here's an example of how to perform linear regression using the Scikit-Learn library with the Pima Indian Diabetes dataset:\n"
      ],
      "metadata": {
        "id": "y5fV3iqszwHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Split the dataset into input features (X) and target variable (y)\n",
        "X = dataset.drop('Outcome', axis=1)\n",
        "y = dataset['Outcome']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n"
      ],
      "metadata": {
        "id": "TQwXrV-nz0oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we start by loading the Pima Indian Diabetes dataset using Pandas. We then split the dataset into input features (X) and the target variable (y), where X contains all the columns except the 'Outcome' column and y contains only the 'Outcome' column.\n",
        "\n",
        "Next, we split the dataset into training and testing sets using the `train_test_split` function from Scikit-Learn. We specify a test size of 0.2, meaning that 20% of the data will be used for testing, while 80% will be used for training.\n",
        "\n",
        "We create a linear regression model using the `LinearRegression` class from Scikit-Learn. We then fit the model to the training data using the `fit` method.\n",
        "\n",
        "After training the model, we make predictions on the testing data using the `predict` method. Finally, we calculate the mean squared error (MSE) between the actual target values (y_test) and the predicted values (y_pred) using the `mean_squared_error` function from Scikit-Learn. The MSE is a commonly used metric to evaluate the performance of regression models.\n"
      ],
      "metadata": {
        "id": "D-F2ReIAz4wn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic regression\n",
        "Logistic regression is a popular algorithm used for binary classification problems. It is available in the Scikit-Learn library, which provides a wide range of machine learning algorithms and tools. Here's an example of how to use logistic regression from Scikit-Learn with the Pima Indian Diabetes dataset:\n"
      ],
      "metadata": {
        "id": "W83AS8Mjz-S7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = dataset.drop(\"Outcome\", axis=1)\n",
        "y = dataset[\"Outcome\"]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a logistic regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Generate a classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "vLOzOXt_0CeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we start by loading the Pima Indian Diabetes dataset using Pandas. Then, we split the dataset into features (X) and the target variable (y). The features contain all columns except the \"Outcome\" column, which represents the presence or absence of diabetes. We split the data into training and testing sets using the `train_test_split` function from Scikit-Learn.\n",
        "\n",
        "Next, we create an instance of the logistic regression model using `LogisticRegression()`. We then train the model on the training data using the `fit()` method. After training, we use the model to make predictions on the testing data with the `predict()` method.\n",
        "\n",
        "To evaluate the performance of the model, we calculate the accuracy by comparing the predicted labels (`y_pred`) with the actual labels (`y_test`). Additionally, we generate a classification report using the `classification_report()` function to get detailed metrics such as precision, recall, and F1-score for each class.\n",
        "\n",
        "By running this code, you can train a logistic regression model on the Pima Indian Diabetes dataset and evaluate its performance on the testing set.\n"
      ],
      "metadata": {
        "id": "EoSgUO0Y0Gp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision trees and random forests\n"
      ],
      "metadata": {
        "id": "ZphCZ1marxm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview"
      ],
      "metadata": {
        "id": "ez8o2iMk5mNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Decision trees and random forests are powerful machine learning algorithms used for both classification and regression tasks. They are part of the ensemble learning methods, which combine multiple individual models to create a more robust and accurate prediction model.\n",
        "\n",
        "**Decision Trees:**\n",
        "A decision tree is a flowchart-like structure where each internal node represents a feature or attribute, each branch represents a decision or a rule based on that feature, and each leaf node represents the outcome or the final decision. The process of constructing a decision tree involves recursively partitioning the data into subsets based on the values of the features until a stopping criterion is met, resulting in a tree that can be used for making predictions.\n",
        "\n",
        "Decision trees are easy to interpret and visualize, making them useful for gaining insights into the decision-making process. They are capable of handling both numerical and categorical data and can handle missing values effectively. However, decision trees can suffer from overfitting, especially when they become too complex or deep. This limitation led to the development of ensemble methods like random forests.\n",
        "\n",
        "**Random Forests:**\n",
        "A random forest is an ensemble learning technique that combines multiple decision trees to improve the overall performance and generalization of the model. It creates a collection of decision trees, each trained on different subsets of the data and using random subsets of features. During the prediction phase, each tree in the forest independently predicts the outcome, and the final prediction is determined by a majority vote (in the case of classification) or averaging (in the case of regression) of the individual tree predictions.\n",
        "\n",
        "Random forests offer several advantages over single decision trees. They are less prone to overfitting due to the aggregation of multiple trees, which helps to reduce variance and improve accuracy. Additionally, random forests can handle large datasets, high-dimensional feature spaces, and are relatively robust to outliers and noisy data. Their ability to provide feature importance rankings is also valuable for feature selection and understanding the impact of different features on the model's predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "4P8Am4lq5n53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Decision trees\n",
        "Decision trees are a popular machine learning algorithm used for both classification and regression tasks. The Scikit-Learn library in Python provides an implementation of decision trees through the `DecisionTreeClassifier` and `DecisionTreeRegressor` classes.\n",
        "\n",
        "Here's an example of using the `DecisionTreeClassifier` class from Scikit-Learn with the Pima Indian Diabetes dataset:\n"
      ],
      "metadata": {
        "id": "gGQh_aY-0JdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Split the dataset into features and target variable\n",
        "X = dataset.drop('Outcome', axis=1)\n",
        "y = dataset['Outcome']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a decision tree classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "I27KAvjA0OvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first load the Pima Indian Diabetes dataset using Pandas. We split the dataset into the features (X) and the target variable (y). Then, we split the data into training and test sets using the `train_test_split` function from Scikit-Learn.\n",
        "\n",
        "Next, we create an instance of the `DecisionTreeClassifier` class and train the classifier on the training data using the `fit` method. Once the classifier is trained, we use it to make predictions on the test data using the `predict` method.\n",
        "\n",
        "Finally, we evaluate the accuracy of the classifier by comparing the predicted labels (`y_pred`) with the true labels (`y_test`) and calculating the accuracy score. The accuracy score is printed as the output.\n",
        "\n",
        "You can adjust various parameters of the `DecisionTreeClassifier`, such as the maximum depth, minimum samples split, and criterion, to customize the behavior of the decision tree algorithm and improve its performance based on your specific requirements.\n"
      ],
      "metadata": {
        "id": "mUF0AGYy0SZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forests\n",
        "\n",
        "Random Forests is a popular machine learning algorithm used for both classification and regression tasks. It is implemented in the Scikit-Learn library in Python. Random Forests combine the predictions of multiple decision trees to make accurate and robust predictions. Here's an example of using Random Forests with the Pima Indian Diabetes dataset:\n"
      ],
      "metadata": {
        "id": "ptcBXC990XG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Split the dataset into features and target variable\n",
        "X = dataset.drop('Outcome', axis=1)\n",
        "y = dataset['Outcome']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier with 100 trees\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "# Train the classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "G5yv_K-P0auL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first load the Pima Indian Diabetes dataset using Pandas. Then, we split the dataset into features (`X`) and the target variable (`y`).\n",
        "\n",
        "Next, we split the dataset into training and testing sets using the `train_test_split` function from Scikit-Learn. We allocate 80% of the data for training and 20% for testing.\n",
        "\n",
        "We create a Random Forest Classifier using the `RandomForestClassifier` class from Scikit-Learn. We set the number of trees in the forest to 100 by passing `n_estimators=100`.\n",
        "\n",
        "We then train the classifier using the training set by calling the `fit` method.\n",
        "\n",
        "After training, we make predictions on the test set using the `predict` method. The predicted labels are stored in `y_pred`.\n",
        "\n",
        "Finally, we calculate the accuracy of the classifier by comparing the predicted labels (`y_pred`) with the true labels (`y_test`) using the `accuracy_score` function from Scikit-Learn. The accuracy is printed as the output.\n",
        "\n",
        "Note: Before running the example, make sure you have the Scikit-Learn library installed (`pip install scikit-learn`).\n"
      ],
      "metadata": {
        "id": "cnNXyveA0fFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support vector machines (SVM)\n",
        "\n"
      ],
      "metadata": {
        "id": "OCHCvDDu0ilD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview"
      ],
      "metadata": {
        "id": "bhm7mSWi5-uw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction to Support Vector Machines (SVM)**\n",
        "\n",
        "Support Vector Machines (SVM) are powerful and versatile machine learning algorithms used for classification and regression tasks. Introduced in the early 1990s by Vladimir Vapnik and his colleagues, SVM quickly gained popularity due to its effectiveness in handling complex, high-dimensional data and its ability to generalize well to unseen examples.\n",
        "\n",
        "At its core, SVM is a supervised learning algorithm that aims to find an optimal hyperplane in a high-dimensional feature space that best separates different classes of data points. In a binary classification scenario, the hyperplane acts as a decision boundary, effectively dividing the data into two classes. The SVM algorithm searches for the hyperplane with the largest margin, known as the \"maximum-margin hyperplane,\" which maximizes the distance between the two closest data points from each class, called \"support vectors.\" This design choice not only improves the classifier's accuracy on the training data but also enhances its generalization ability on unseen data, making SVM particularly well-suited for dealing with overfitting.\n",
        "\n",
        "One of the key strengths of SVM is its ability to handle both linearly separable data and data that are not linearly separable. In cases where the data is not linearly separable, SVM employs a technique called the \"kernel trick\" to map the original feature space into a higher-dimensional space, where the data might become linearly separable. Popular kernel functions include the linear kernel, polynomial kernel, radial basis function (RBF) kernel, and sigmoid kernel, each suited for different types of data and decision boundaries.\n",
        "\n",
        "SVM has a wide range of applications, including but not limited to:\n",
        "- Text classification and sentiment analysis\n",
        "- Image recognition and object detection\n",
        "- Handwriting recognition\n",
        "- Bioinformatics and protein structure prediction\n",
        "- Finance and stock market forecasting\n",
        "- Medical diagnosis and disease classification\n",
        "\n",
        "Though SVM is a powerful algorithm, it does have some considerations. SVM can become computationally expensive, especially with large datasets, and tuning the hyperparameters and selecting the appropriate kernel function can be crucial for achieving optimal results. Furthermore, SVM is primarily designed for binary classification, and extending it to multi-class problems often involves strategies like one-vs-one or one-vs-rest.\n",
        "\n"
      ],
      "metadata": {
        "id": "AZOBwLVE6ASX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = dataset.drop('Outcome', axis=1)\n",
        "y = dataset['Outcome']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an SVM classifier\n",
        "classifier = SVC(kernel='linear')\n",
        "\n",
        "# Train the classifier on the training data\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Print the confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n"
      ],
      "metadata": {
        "id": "Kk3wJRD00mT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we load the Pima Indian Diabetes dataset using Pandas library. We split the dataset into features (X) and the target variable (y). Then, we split the data into training and testing sets using the `train_test_split` function from Scikit-Learn.\n",
        "\n",
        "Next, we create an SVM classifier using the `SVC` class from Scikit-Learn and specify the `kernel` parameter as 'linear'. We train the classifier on the training data using the `fit` method.\n",
        "\n",
        "After training, we make predictions on the test data using the `predict` method. We evaluate the accuracy of the model by comparing the predicted labels (`y_pred`) with the true labels (`y_test`) using the `accuracy_score` function.\n",
        "\n",
        "Finally, we print the accuracy score and the confusion matrix to evaluate the performance of the SVM classifier on the Pima Indian Diabetes dataset.\n"
      ],
      "metadata": {
        "id": "wC0ky0-R0ryh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes\n",
        "\n"
      ],
      "metadata": {
        "id": "-IMSLIo_0udc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview"
      ],
      "metadata": {
        "id": "P_1ZO2Bz6L8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Naive Bayes is a popular and widely used classification algorithm in the field of machine learning and statistics. It is based on the Bayes theorem, which is a fundamental concept in probability theory. Naive Bayes is particularly well-suited for tasks involving text classification, spam filtering, sentiment analysis, and many other applications where the input features are discrete or categorical in nature.\n",
        "\n",
        "The algorithm gets its name \"naive\" because it makes a strong assumption that the features used for classification are conditionally independent, given the class label. This assumption simplifies the computations and makes the algorithm computationally efficient, even when dealing with a large number of features. Despite its seemingly oversimplified assumption, Naive Bayes often performs remarkably well in practice, especially in scenarios with high-dimensional data and limited training samples.\n",
        "\n",
        "The fundamental idea behind Naive Bayes is to determine the probability of a particular instance belonging to a specific class based on the probabilities of its individual features. It calculates the likelihood of observing each feature in the data for each class, as well as the prior probability of each class (the probability of a class occurring without any knowledge of the features). By combining these probabilities using Bayes theorem, the algorithm can make predictions by selecting the class with the highest posterior probability.\n",
        "\n",
        "One of the key advantages of Naive Bayes is its ability to handle missing data gracefully and its robustness against irrelevant features. Moreover, it is relatively simple to implement and requires minimal hyperparameter tuning, making it an attractive choice for quick prototyping and as a baseline model for more complex classification tasks.\n",
        "\n",
        "However, Naive Bayes may not always perform optimally when the strong independence assumption is violated, or when features are highly correlated. In such cases, more sophisticated models like logistic regression or decision trees may be more suitable.\n",
        "\n"
      ],
      "metadata": {
        "id": "A2BcB2QR6NX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Split the dataset into features and target variable\n",
        "X = dataset.drop('Outcome', axis=1)\n",
        "y = dataset['Outcome']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Naive Bayes classifier\n",
        "nb_classifier = GaussianNB()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Generate a classification report\n",
        "classification_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", classification_report)\n"
      ],
      "metadata": {
        "id": "wgPIq3NZ0yTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first load the Pima Indian Diabetes dataset using Pandas. We then split the dataset into features (X) and the target variable (y). Next, we split the data into training and testing sets using the `train_test_split` function from Scikit-Learn.\n",
        "\n",
        "We create a Gaussian Naive Bayes classifier using the `GaussianNB` class from Scikit-Learn. Then, we train the classifier on the training data using the `fit` method. Afterward, we make predictions on the test data using the `predict` method.\n",
        "\n",
        "We calculate the accuracy of the classifier by comparing the predicted labels (`y_pred`) with the actual labels from the test set (`y_test`) using the `accuracy_score` function.\n",
        "\n",
        "Finally, we generate a classification report using the `classification_report` function, which provides detailed metrics such as precision, recall, F1-score, and support for each class.\n",
        "\n",
        "Note: Make sure you have the Scikit-Learn library installed (`pip install scikit-learn`) to run this example.\n"
      ],
      "metadata": {
        "id": "rtWYO7t002lJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reflection points"
      ],
      "metadata": {
        "id": "oq2JXXwy05uk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Linear Regression**:\n",
        "   - What is linear regression, and how does it differ from other regression algorithms?\n",
        "   - How do you interpret the coefficients in linear regression?\n",
        "   - How can you handle outliers and multicollinearity in linear regression?\n",
        "   - What are some common evaluation metrics used for assessing the performance of linear regression models?\n",
        "   \n",
        "   Answer: Linear regression is a supervised learning algorithm used for predicting continuous numerical values. It assumes a linear relationship between the independent variables and the target variable. The coefficients in linear regression represent the impact of each independent variable on the target variable. Outliers can be handled by removing or transforming them, while multicollinearity can be addressed through techniques like regularization or feature selection. Evaluation metrics for linear regression include mean squared error (MSE), R-squared, and mean absolute error (MAE).\n",
        "\n",
        "2. **Logistic Regression**:\n",
        "   - What is logistic regression, and when is it used instead of linear regression?\n",
        "   - How does logistic regression handle binary classification tasks?\n",
        "   - What is the logistic function (sigmoid function), and what is its significance in logistic regression?\n",
        "   - How can you interpret the coefficients in logistic regression?\n",
        "   \n",
        "   Answer: Logistic regression is used for binary classification problems where the target variable has two classes. Unlike linear regression, logistic regression models the probability of belonging to a particular class. The logistic function, also known as the sigmoid function, maps the output of the linear equation to a probability between 0 and 1. The coefficients in logistic regression represent the log-odds or logit of the probability of the target variable.\n",
        "\n",
        "3. **Decision Trees and Random Forests**:\n",
        "   - What are decision trees, and how do they work?\n",
        "   - What is entropy and how is it used to make decisions in decision trees?\n",
        "   - What are the advantages and limitations of decision trees?\n",
        "   - How does a random forest algorithm improve upon decision trees?\n",
        "   \n",
        "   Answer: Decision trees are hierarchical structures that use a series of binary decisions to make predictions. Entropy is a measure of impurity used in decision trees to determine the best splitting criteria. Decision trees are easy to understand and interpret, but they can be prone to overfitting and lack generalization. Random forests are an ensemble of decision trees that reduce overfitting and increase model performance by averaging the predictions of multiple trees and introducing randomness in the training process.\n",
        "\n",
        "4. **Support Vector Machines (SVM)**:\n",
        "   - What are support vector machines, and how do they work?\n",
        "   - What is the concept of margin in SVMs?\n",
        "   - How does the choice of kernel function impact SVM performance?\n",
        "   - What are some advantages and disadvantages of SVMs?\n",
        "   \n",
        "   Answer: Support Vector Machines (SVM) is a powerful algorithm used for both classification and regression tasks. SVM aims to find a hyperplane that separates data points of different classes with the largest margin. The margin is the distance between the hyperplane and the closest data points. The choice of kernel function determines how the data is transformed to make it separable in higher-dimensional space. SVMs can handle high-dimensional data and are effective in cases with clear separation, but they can be sensitive to noise and require proper scaling of features.\n",
        "\n",
        "5. **Naive Bayes**:\n",
        "   - What is the Naive Bayes algorithm, and how does it work?\n",
        "   - What is the assumption made by Naive Bayes that gives it its name?\n",
        "   - What is the underlying principle of Bayes' theorem?\n",
        "   - What are the advantages and limitations of Naive Bayes?\n",
        "   \n",
        "   Answer: Naive Bayes is a probabilistic algorithm used for classification tasks. It assumes that the features are conditionally independent given the class, which is known as the Naive Bayes assumption. Naive Bayes is based on Bayes' theorem, which calculates the probability of a class given the features. It is computationally efficient and performs well in situations with high-dimensional data. However, Naive Bayes assumes independence between features, which may not hold in some cases, leading to suboptimal performance.\n"
      ],
      "metadata": {
        "id": "7BM2FDSb0__P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A quiz on Supervised Learning Algorithms"
      ],
      "metadata": {
        "id": "sQTbxd3OsVz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure! Below is a quiz on machine learning algorithms: Linear Regression, Logistic Regression, Decision Trees, Random Forests, Support Vector Machines (SVM), and Naive Bayes. There are five questions in this quiz. At the end of the quiz, you'll find the correct answers. Good luck!\n",
        "\n",
        "**Machine Learning Algorithms Quiz**\n",
        "\n",
        "1. **Linear Regression:**\n",
        "Q1. Which type of problem is Linear Regression best suited for?\n",
        "<br>a) Classification problems\n",
        "<br>b) Unsupervised learning problems\n",
        "<br>c) Regression problems\n",
        "<br>d) Clustering problems\n",
        "\n",
        "2. **Logistic Regression:**\n",
        "Q2. What type of output does Logistic Regression produce?\n",
        "<br>a) Continuous numeric values\n",
        "<br>b) Discrete numeric values\n",
        "<br>c) Probability scores\n",
        "<br>d) Binary classes\n",
        "\n",
        "3. **Decision Trees:**\n",
        "Q3. What is the primary goal of a Decision Tree algorithm?\n",
        "<br>a) Minimize the number of tree nodes\n",
        "<br>b) Maximize the number of tree nodes\n",
        "<br>c) Minimize the tree depth\n",
        "<br>d) Maximize the tree depth\n",
        "\n",
        "4. **Random Forests:**\n",
        "Q4. What is the key concept behind the Random Forest algorithm?\n",
        "<br>a) Ensemble learning and combining multiple decision trees\n",
        "<br>b) Using a single decision tree with random features\n",
        "<br>c) Stacking multiple regression models\n",
        "<br>d) Creating random splits in the dataset\n",
        "\n",
        "5. **Support Vector Machines (SVM):**\n",
        "Q5. SVM is mainly used for which type of tasks?\n",
        "<br>a) Regression tasks\n",
        "<br>b) Clustering tasks\n",
        "<br>c) Classification tasks\n",
        "<br>d) Dimensionality reduction tasks\n",
        "\n",
        "6. **Naive Bayes:**\n",
        "Q6. What is the main assumption of the Naive Bayes algorithm?\n",
        "<br>a) Features are dependent on each other\n",
        "<br>b) Features are independent of each other\n",
        "<br>c) Features are not related to the target variable\n",
        "<br>d) Features are correlated to the target variable\n",
        "\n",
        "---\n",
        "**Answers:**\n",
        "\n",
        "1. Q1. c) Regression problems\n",
        "2. Q2. d) Binary classes\n",
        "3. Q3. c) Minimize the tree depth\n",
        "4. Q4. a) Ensemble learning and combining multiple decision trees\n",
        "5. Q5. c) Classification tasks\n",
        "6. Q6. b) Features are independent of each other\n",
        "---"
      ],
      "metadata": {
        "id": "zMFC6811sX_6"
      }
    }
  ]
}