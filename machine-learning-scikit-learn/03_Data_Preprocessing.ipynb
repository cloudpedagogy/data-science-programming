{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMOw+UC31OjD5fmlnS4fg8E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloudpedagogy/machine-learning-scikit-learn/blob/main/03_Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing\n"
      ],
      "metadata": {
        "id": "lpPub52iu7bL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Overview"
      ],
      "metadata": {
        "id": "R0hjOtpcw_c_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Data preprocessing is a critical step in the data analysis and machine learning workflow, playing a pivotal role in transforming raw, unstructured data into a clean, structured format that is suitable for further analysis and modeling. In Python, data preprocessing is carried out using various libraries and techniques to handle missing values, handle outliers, scale features, encode categorical variables, and more. By performing these essential data preparation steps, data scientists and analysts can ensure that the data is of high quality and relevant to the problem at hand, ultimately leading to more accurate and reliable insights.\n",
        "\n",
        "One of the first tasks in data preprocessing is dealing with missing values, which are gaps or null entries in the dataset. Missing values can arise due to various reasons, such as data collection errors or incomplete records. Python offers several powerful libraries, like Pandas and NumPy, that enable data analysts to identify missing values and choose how to handle them. Depending on the data and the analysis goals, missing values can be imputed, removed, or replaced with meaningful values, ensuring that they do not negatively impact the accuracy of the subsequent analyses.\n",
        "\n",
        "Another crucial step in data preprocessing is handling outliers. Outliers are extreme values that deviate significantly from the majority of the data points and can distort statistical analyses or machine learning models. Python provides various techniques, such as the Interquartile Range (IQR) method or the Z-score method, to identify and handle outliers effectively. By treating outliers appropriately, analysts can prevent misleading insights and improve the robustness of their data analysis.\n",
        "\n",
        "Additionally, data preprocessing involves handling categorical variables, which are variables that represent categories rather than numerical values. Many machine learning algorithms require numerical inputs, and as such, categorical variables need to be encoded into a numeric format. Python libraries like Scikit-learn offer tools like one-hot encoding and label encoding to convert categorical variables into a suitable numerical representation, making them usable in machine learning models.\n",
        "\n",
        "Furthermore, feature scaling is a crucial aspect of data preprocessing to ensure that all features contribute equally to the analysis. Features with different scales can dominate the learning process, leading to biased models. Techniques like min-max scaling and standardization are commonly used in Python to bring all features to a similar scale, thus improving the performance of machine learning algorithms and facilitating convergence.\n",
        "\n",
        "In conclusion, data preprocessing is a fundamental and indispensable step in the data analysis and machine learning process. Python provides a wealth of libraries and tools that enable data scientists and analysts to clean, preprocess, and transform data effectively. By addressing missing values, handling outliers, encoding categorical variables, and scaling features, analysts can prepare high-quality data that forms the foundation for accurate and meaningful insights, leading to better-informed decisions and successful machine learning models."
      ],
      "metadata": {
        "id": "GEAXb9pgxBhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Handling missing values\n",
        "\n",
        "In the scikit-learn library, missing values in datasets can be handled using various techniques. Here are some common approaches to handling missing values:\n",
        "\n",
        "1. Dropping Rows or Columns: This approach involves removing rows or columns that contain missing values. However, it should be used with caution as it may lead to loss of important data.\n",
        "\n",
        "2. Imputation: Imputation is the process of filling in missing values with estimated or predicted values. Some commonly used imputation techniques include:\n",
        "   - Mean/Median Imputation: Replace missing values with the mean or median of the non-missing values in the same column.\n",
        "   - Mode Imputation: Replace missing values with the mode (most frequent value) of the non-missing values in the same column.\n",
        "   - Regression Imputation: Predict missing values based on the relationship between the target variable and other variables using regression models.\n",
        "\n",
        "Here's an example of using imputation techniques with the Pima Indian Diabetes dataset:\n"
      ],
      "metadata": {
        "id": "Jd2YWTYHvG0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Replace 0 values with NaN in columns where 0 doesn't make sense\n",
        "columns_to_check = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\n",
        "dataset[columns_to_check] = dataset[columns_to_check].replace(0, pd.np.nan)\n",
        "\n",
        "# Impute missing values using mean imputation\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "imputed_data = imputer.fit_transform(dataset)\n",
        "\n",
        "# Convert the imputed data back to a DataFrame\n",
        "imputed_dataset = pd.DataFrame(imputed_data, columns=column_names)\n",
        "\n",
        "# Print the imputed dataset\n",
        "print(imputed_dataset.head())\n"
      ],
      "metadata": {
        "id": "mcMQnG80vSta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we load the Pima Indian Diabetes dataset using Pandas library. We replace the 0 values in columns such as 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', and 'BMI' with NaN (missing value) using the `replace()` method.\n",
        "\n",
        "Next, we use the `SimpleImputer` class from scikit-learn to perform mean imputation. The `SimpleImputer` replaces missing values with the mean of the non-missing values in each column. We fit the imputer on the dataset using the `fit_transform()` method to impute the missing values.\n",
        "\n",
        "Finally, we convert the imputed data back to a DataFrame and print the imputed dataset using the `head()` method to see the output.\n"
      ],
      "metadata": {
        "id": "8iLWbDGUvcQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature scaling and normalization\n",
        "\n",
        "\n",
        "Feature scaling and normalization are techniques used to transform numeric features in a dataset to a common scale. This process is often necessary in machine learning algorithms to ensure that all features contribute equally to the model's training and avoid biases caused by features with larger magnitudes dominating the learning process. The Scikit-Learn library provides various methods for feature scaling and normalization.\n",
        "\n",
        "Here are two commonly used techniques for feature scaling and normalization in Scikit-Learn:\n",
        "\n",
        "1. Standardization:\n",
        "   Standardization scales the features to have a mean of 0 and a standard deviation of 1. This technique assumes that the feature values follow a Gaussian distribution.\n"
      ],
      "metadata": {
        "id": "_ZETGC40vluJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = dataset.drop('Outcome', axis=1)\n",
        "y = dataset['Outcome']\n",
        "\n",
        "# Perform standardization\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Print the scaled features\n",
        "print(X_scaled)\n"
      ],
      "metadata": {
        "id": "4XX6-3D-vtU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we use the StandardScaler class from Scikit-Learn to perform standardization on the feature matrix X. The fit_transform method calculates the mean and standard deviation of each feature and scales the values accordingly. The resulting X_scaled contains the standardized feature values.\n",
        "\n",
        "2. Min-Max Scaling:\n",
        "   Min-Max Scaling, also known as normalization, scales the features to a specific range, usually between 0 and 1.\n",
        "\n",
        "   Example using the Pima Indian Diabetes dataset:\n",
        "\n"
      ],
      "metadata": {
        "id": "0Qay1JuZv8En"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = dataset.drop('Outcome', axis=1)\n",
        "y = dataset['Outcome']\n",
        "\n",
        "# Perform min-max scaling\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Print the scaled features\n",
        "print(X_scaled)\n"
      ],
      "metadata": {
        "id": "06zPzvumwCbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we use the MinMaxScaler class from Scikit-Learn to perform min-max scaling on the feature matrix X. The fit_transform method determines the minimum and maximum values for each feature and scales the values accordingly to the range [0, 1]. The resulting X_scaled contains the normalized feature values.\n",
        "\n",
        "Both standardization and min-max scaling can be applied to the entire feature matrix or specific columns based on your requirements.\n"
      ],
      "metadata": {
        "id": "4vLi03KZwbcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Handling categorical variables\n",
        "\n",
        "Handling categorical variables in Scikit-Learn library typically involves encoding categorical data into numerical representations so that machine learning algorithms can effectively process them. Scikit-Learn provides various methods for encoding categorical variables, such as Label Encoding and One-Hot Encoding. Here are examples of how to handle categorical variables using the Pima Indian Diabetes dataset:\n",
        "\n",
        "1. Label Encoding:\n",
        "Label Encoding converts categorical variables into integers by assigning a unique numerical value to each category. This method is suitable when the categorical variable has an inherent ordinal relationship.\n"
      ],
      "metadata": {
        "id": "H82iBcWawh_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Perform label encoding on the 'Outcome' column\n",
        "label_encoder = LabelEncoder()\n",
        "dataset['Outcome'] = label_encoder.fit_transform(dataset['Outcome'])\n",
        "\n",
        "# Print the updated dataset\n",
        "print(dataset.head())\n"
      ],
      "metadata": {
        "id": "Bc2bQoWnws1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we use the LabelEncoder class from Scikit-Learn to encode the 'Outcome' column of the Pima Indian Diabetes dataset. The label_encoder.fit_transform() function fits the encoder on the 'Outcome' column and transforms the categorical values into numerical labels. The transformed values replace the original categorical values in the dataset.\n",
        "\n",
        "2. One-Hot Encoding:\n",
        "One-Hot Encoding creates binary columns for each category of a categorical variable. Each column represents a category, and a value of 1 indicates that the observation belongs to that category, while 0 indicates it does not.\n"
      ],
      "metadata": {
        "id": "pqhvupDGwxeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load the Pima Indian Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
        "dataset = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Perform one-hot encoding on the 'Outcome' column\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "encoded_features = onehot_encoder.fit_transform(dataset[['Outcome']])\n",
        "\n",
        "# Create a new DataFrame with the encoded features\n",
        "encoded_df = pd.DataFrame(encoded_features, columns=onehot_encoder.categories_[0])\n",
        "dataset = pd.concat([dataset, encoded_df], axis=1)\n",
        "\n",
        "# Drop the original 'Outcome' column\n",
        "dataset.drop(['Outcome'], axis=1, inplace=True)\n",
        "\n",
        "# Print the updated dataset\n",
        "print(dataset.head())\n"
      ],
      "metadata": {
        "id": "AkZWR94ew15g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we use the OneHotEncoder class from Scikit-Learn to encode the 'Outcome' column of the Pima Indian Diabetes dataset. The onehot_encoder.fit_transform() function fits the encoder on the 'Outcome' column and transforms it into a one-hot encoded representation. We then create a new DataFrame with the encoded features and concatenate it with the original dataset. Finally, we drop the original 'Outcome' column from the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "TqutZAQYw7aW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reflection Points"
      ],
      "metadata": {
        "id": "Oa4vp1GBw8i1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Handling Missing Values**:\n",
        "   - What are some common techniques for identifying missing values in a dataset?\n",
        "     - Answer: Common techniques include checking for null values, using summary statistics, or visualizing missing value patterns.\n",
        "   - What are the potential impacts of missing values on data analysis and modeling?\n",
        "     - Answer: Missing values can lead to biased results, reduced statistical power, or errors in predictive modeling if not handled appropriately.\n",
        "   - What are some strategies for handling missing values in a dataset?\n",
        "     - Answer: Strategies include removing rows or columns with missing values, imputing missing values with statistical measures, or using advanced techniques like regression imputation or multiple imputation.\n",
        "\n",
        "2. **Feature Scaling and Normalization**:\n",
        "   - What is the purpose of feature scaling and normalization in machine learning?\n",
        "     - Answer: Feature scaling ensures that all features have a similar scale, preventing some features from dominating others and affecting model performance.\n",
        "   - What are some common techniques for scaling and normalizing features?\n",
        "     - Answer: Techniques include min-max scaling, z-score standardization, and robust scaling using interquartile range.\n",
        "   - When is it appropriate to apply feature scaling or normalization to a dataset?\n",
        "     - Answer: Feature scaling is typically applied when features have different scales, such as in distance-based algorithms or models that rely on gradient descent optimization.\n",
        "\n",
        "3. **Handling Categorical Variables**:\n",
        "   - How do categorical variables differ from numerical variables in data analysis?\n",
        "     - Answer: Categorical variables represent discrete groups or categories, while numerical variables represent continuous values.\n",
        "   - What are the potential challenges of using categorical variables in machine learning models?\n",
        "     - Answer: Challenges include the need to convert categorical variables into numerical representations, handling high-cardinality categories, and avoiding the introduction of bias during encoding.\n",
        "   - What are some common techniques for handling categorical variables in machine learning?\n",
        "     - Answer: Techniques include one-hot encoding, label encoding, target encoding, or using embedding layers in deep learning models.\n"
      ],
      "metadata": {
        "id": "wJNSDTR1xGhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A quiz on Data Processing"
      ],
      "metadata": {
        "id": "imULrzHNpXfO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Question: The process of filling in or imputing missing values in a dataset is called:\n",
        "   <br>a) Feature scaling\n",
        "   <br>b) Normalization\n",
        "   <br>c) Handling missing values\n",
        "   <br>d) Categorical encoding\n",
        "\n",
        "2. Question: Which Python library can be used to load and manipulate datasets, including handling missing values?\n",
        "   <br>a) NumPy\n",
        "   <br>b) Pandas\n",
        "   <br>c) Scikit-learn\n",
        "   <br>d) Matplotlib\n",
        "\n",
        "3. Question: Which method is commonly used for filling missing numerical values with the mean of the available data?\n",
        "   <br>a) Zero imputation\n",
        "   <br>b) Mode imputation\n",
        "   <br>c) Mean imputation\n",
        "   <br>d) Median imputation\n",
        "\n",
        "4. Question: Feature scaling is important for machine learning algorithms that rely on distance calculations or gradient descent. Which of the following scaling techniques transforms data to a range of [0, 1]?\n",
        "   <br>a) Min-Max scaling\n",
        "   <br>b) Standardization\n",
        "   <br>c) Robust scaling\n",
        "   <br>d) Log transformation\n",
        "\n",
        "5. Question: In feature scaling, the z-score scaling technique transforms data to have a mean of 0 and a standard deviation of 1.\n",
        "   <br>a) True\n",
        "   <br>b) False\n",
        "\n",
        "6. Question: Which of the following is NOT a method to handle categorical variables?\n",
        "   <br>a) One-Hot Encoding\n",
        "   <br>b) Label Encoding\n",
        "   <br>c) Ordinal Encoding\n",
        "   <br>d) Median Encoding\n",
        "\n",
        "7. Question: In One-Hot Encoding, how many new columns are created for a categorical feature with 'n' unique categories?\n",
        "   <br>a) n\n",
        "   <br>b) n+1\n",
        "   <br>c) 2n\n",
        "   <br>d) n/2\n",
        "\n",
        "8. Question: Which Python library provides the 'get_dummies()' function to perform One-Hot Encoding?\n",
        "   <br>a) Pandas\n",
        "   <br>b) NumPy\n",
        "   <br>c) Scikit-learn\n",
        "   <br>d) Statsmodels\n",
        "\n",
        "---\n",
        "Answer Key:\n",
        "1. c) Handling missing values\n",
        "2. b) Pandas\n",
        "3. c) Mean imputation\n",
        "4. a) Min-Max scaling\n",
        "5. b) False\n",
        "6. d) Median Encoding\n",
        "7. b) n+1\n",
        "8. a) Pandas\n",
        "---"
      ],
      "metadata": {
        "id": "k5t7qBcnpa22"
      }
    }
  ]
}